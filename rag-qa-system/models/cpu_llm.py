"""
CPU ìµœì í™” LLM ëª¨ë¸ (llama.cpp ê¸°ë°˜)
VLLM ëŒ€ì‹  CPUì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ì‘ë™
"""
import os
from typing import List, Optional
from llama_cpp import Llama

class CPUOptimizedLLM:
    """CPUì—ì„œ ìµœì í™”ëœ LLM ì‹¤í–‰"""
    
    def __init__(self, 
                 model_path: str = None,
                 n_ctx: int = 2048,      # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
                 n_threads: int = 4,     # CPU ìŠ¤ë ˆë“œ ìˆ˜
                 n_gpu_layers: int = 0): # GPU ë ˆì´ì–´ (0 = CPU only)
        
        # ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ
        if not model_path:
            model_path = os.getenv('LLM_MODEL_PATH', './models/llama-2-7b-chat.Q4_K_M.gguf')
        
        self.model_path = model_path
        self.llm = None
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        try:
            self.llm = Llama(
                model_path=model_path,
                n_ctx=n_ctx,
                n_threads=n_threads,
                n_gpu_layers=n_gpu_layers,
                verbose=False
            )
            print(f"âœ… CPU LLM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {os.path.basename(model_path)}")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ GGUF í˜•ì‹ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:")
            print("   https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF")
    
    def generate(self, prompt: str, max_tokens: int = 512, temperature: float = 0.7) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        if not self.llm:
            return "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        response = self.llm(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            stop=["</s>", "\n\n"],
            echo=False
        )
        
        return response['choices'][0]['text'].strip()
    
    def invoke(self, prompt: str) -> 'LLMResponse':
        """LangChain í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤"""
        text = self.generate(prompt)
        return LLMResponse(text)

class LLMResponse:
    """LangChain í˜¸í™˜ ì‘ë‹µ ê°ì²´"""
    def __init__(self, content: str):
        self.content = content
        self.additional_kwargs = {}
        
    def __str__(self):
        return self.content

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
AVAILABLE_MODELS = {
    "llama2-7b": {
        "name": "Llama 2 7B Chat",
        "url": "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf",
        "size": "3.8GB",
        "description": "Metaì˜ Llama 2 7B ëŒ€í™”í˜• ëª¨ë¸"
    },
    "mistral-7b": {
        "name": "Mistral 7B Instruct",
        "url": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
        "size": "4.1GB",
        "description": "Mistral AIì˜ ê³ ì„±ëŠ¥ ëª¨ë¸"
    },
    "korean-llm": {
        "name": "KULLM v2",
        "url": "https://huggingface.co/nlpai-lab/kullm-polyglot-5.8b-v2-gguf/resolve/main/kullm-polyglot-5.8b-v2.Q4_K_M.gguf",
        "size": "3.5GB",
        "description": "í•œêµ­ì–´ íŠ¹í™” LLM"
    }
}