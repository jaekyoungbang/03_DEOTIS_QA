import chromadb
from chromadb.config import Settings
from langchain_community.vectorstores import Chroma
from langchain.schema import Document
from config import Config
import os

class DualVectorStoreManager:
    """ì´ì¤‘ ë²¡í„°ìŠ¤í† ì–´ ê´€ë¦¬ì - ê¸°ë³¸ ì²­í‚¹ê³¼ ì»¤ìŠ¤í…€ ì²­í‚¹ ë¶„ë¦¬"""
    
    def __init__(self, embedding_function):
        self.embedding_function = embedding_function
        self.basic_vectorstore = None
        self.custom_vectorstore = None
        self.persist_directory = Config.CHROMA_PERSIST_DIRECTORY
        
        # ë³„ë„ ì»¬ë ‰ì…˜ëª… ì„¤ì •
        self.basic_collection_name = "basic_chunks"
        self.custom_collection_name = "custom_chunks"
        
        self.initialize_vectorstores()
    
    def initialize_vectorstores(self):
        """ê¸°ë³¸/ì»¤ìŠ¤í…€ ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        os.makedirs(self.persist_directory, exist_ok=True)
        
        # ê¸°ë³¸ ì²­í‚¹ìš© ë²¡í„°ìŠ¤í† ì–´
        self.basic_vectorstore = Chroma(
            collection_name=self.basic_collection_name,
            embedding_function=self.embedding_function,
            persist_directory=self.persist_directory
        )
        
        # ì»¤ìŠ¤í…€ ì²­í‚¹ìš© ë²¡í„°ìŠ¤í† ì–´  
        self.custom_vectorstore = Chroma(
            collection_name=self.custom_collection_name,
            embedding_function=self.embedding_function,
            persist_directory=self.persist_directory
        )
    
    def add_documents(self, documents, chunking_type="basic", ids=None):
        """ì²­í‚¹ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ë²¡í„°ìŠ¤í† ì–´ì— ë¬¸ì„œ ì¶”ê°€"""
        vectorstore = self._get_vectorstore_by_type(chunking_type)
        
        if ids:
            vectorstore.add_documents(documents=documents, ids=ids)
        else:
            vectorstore.add_documents(documents=documents)
        
        vectorstore.persist()
        return True
    
    def similarity_search(self, query, chunking_type="basic", k=5):
        """ì²­í‚¹ íƒ€ì…ë³„ ìœ ì‚¬ë„ ê²€ìƒ‰"""
        vectorstore = self._get_vectorstore_by_type(chunking_type)
        return vectorstore.similarity_search(query, k=k)
    
    def similarity_search_with_score(self, query, chunking_type="basic", k=5):
        """ì²­í‚¹ íƒ€ì…ë³„ ì ìˆ˜ í¬í•¨ ìœ ì‚¬ë„ ê²€ìƒ‰ - ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜"""
        vectorstore = self._get_vectorstore_by_type(chunking_type)
        
        # ChromaDBì˜ similarity_search_with_score ì‚¬ìš© (ê±°ë¦¬ê°’ ë°˜í™˜)
        results = vectorstore.similarity_search_with_score(query, k=k)
        
        # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (0~1 ì‚¬ì´, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬)
        converted_results = []
        for doc, distance in results:
            # ChromaDBê°€ L2 distanceë¥¼ ë°˜í™˜í•˜ëŠ” ê²½ìš°ë¥¼ ì²˜ë¦¬
            # L2 distanceê°€ í° ê°’(>2)ì´ë©´ L2 ê±°ë¦¬, ì‘ì€ ê°’ì´ë©´ cosine distanceë¡œ ê°€ì •
            if distance > 2:
                # L2 ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜: exp(-distance/scale)ë¡œ ë” ë¶€ë“œëŸ¬ìš´ ë³€í™˜
                import math
                similarity = math.exp(-distance / 1000.0)  # BGE-M3 1024ì°¨ì›ì— ë§ëŠ” ìŠ¤ì¼€ì¼ ì¡°ì •
            else:
                # cosine distanceì¸ ê²½ìš°: similarity = 1 - distance  
                similarity = max(0, 1 - distance)
            converted_results.append((doc, similarity))
        
        return converted_results
    
    def dual_search(self, query, k=5):
        """ê¸°ë³¸/ì»¤ìŠ¤í…€ ë‘ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ë™ì‹œ ê²€ìƒ‰ - ê°•í™”ëœ ì¹´ë“œ ë¶„ì„"""
        try:
            # ì¹´ë“œ ë°œê¸‰ ê´€ë ¨ ì¿¼ë¦¬ì¸ì§€ í™•ì¸
            card_keywords = ["ì¹´ë“œ", "ë°œê¸‰", "íšŒì›", "ì€í–‰", "ì¹´ë“œë°œê¸‰", "ê¹€ëª…ì •"]
            is_card_query = any(keyword in query for keyword in card_keywords)
            
            if is_card_query:
                # ì¹´ë“œ ê´€ë ¨ ì¿¼ë¦¬ì˜ ê²½ìš° ë” ë§ì€ ê²°ê³¼ ê²€ìƒ‰ ë° ìƒì„¸ í‚¤ì›Œë“œ ì¶”ê°€
                extended_keywords = [
                    f"{query} ë°œê¸‰ì ˆì°¨",
                    f"{query} ì‹ ì²­ë°©ë²•",
                    f"{query} ì‹¬ì‚¬ê³¼ì •", 
                    "íšŒì›ì€í–‰ë³„ ì¹´ë“œë°œê¸‰ì•ˆë‚´",
                    "ì¹´ë“œ ë°œê¸‰ ì ˆì°¨",
                    "ì‹ ì²­ ì¤€ë¹„",
                    "ì‹¬ì‚¬ ê³¼ì •",
                    "ì¹´ë“œ ë°œê¸‰"
                ]
                
                all_results = []
                
                # 1. ê¸°ë³¸ ê²€ìƒ‰
                basic_results = self.similarity_search_with_score(query, "basic", k)
                for doc, score in basic_results:
                    doc.metadata['search_source'] = 'basic_chunking'
                    all_results.append((doc, score))
                
                # 2. ì»¤ìŠ¤í…€ ê²€ìƒ‰
                custom_results = self.similarity_search_with_score(query, "custom", k)
                for doc, score in custom_results:
                    doc.metadata['search_source'] = 'custom_chunking' 
                    all_results.append((doc, score))
                
                # 3. í™•ì¥ëœ í‚¤ì›Œë“œë¡œ ì¶”ê°€ ê²€ìƒ‰ (basicì—ì„œ ìƒì„¸ ì •ë³´)
                for keyword in extended_keywords[:3]:  # ìƒìœ„ 3ê°œë§Œ ê²€ìƒ‰
                    try:
                        extended_basic = self.similarity_search_with_score(keyword, "basic", 3)
                        for doc, score in extended_basic:
                            doc.metadata['search_source'] = 'basic_chunking_extended'
                            # ì¤‘ë³µ ë¬¸ì„œ ì œê±°ë¥¼ ìœ„í•´ ë‚´ìš© ë¹„êµ
                            doc_content = doc.page_content[:100]  # ì²« 100ìë¡œ ì¤‘ë³µ ì²´í¬
                            if not any(existing_doc.page_content[:100] == doc_content for existing_doc, _ in all_results):
                                all_results.append((doc, score * 0.9))  # ì•½ê°„ ë‚®ì€ ì ìˆ˜ ë¶€ì—¬
                    except:
                        continue
                
                # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ 15ê°œ ë°˜í™˜ (ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸)
                all_results.sort(key=lambda x: x[1], reverse=True)
                return all_results[:15]
            
            else:
                # ì¼ë°˜ ì¿¼ë¦¬ì˜ ê²½ìš° ê¸°ì¡´ ë°©ì‹
                basic_results = self.similarity_search_with_score(query, "basic", k//2 + 1)
                custom_results = self.similarity_search_with_score(query, "custom", k//2 + 1)
                
                # ê²°ê³¼ í•©ì¹˜ê¸° ë° ì ìˆ˜ìˆœ ì •ë ¬
                all_results = []
                
                # ê¸°ë³¸ ì²­í‚¹ ê²°ê³¼ ì¶”ê°€
                for doc, score in basic_results:
                    doc.metadata['search_source'] = 'basic_chunking'
                    all_results.append((doc, score))
                
                # ì»¤ìŠ¤í…€ ì²­í‚¹ ê²°ê³¼ ì¶”ê°€
                for doc, score in custom_results:
                    doc.metadata['search_source'] = 'custom_chunking' 
                    all_results.append((doc, score))
                
                # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ kê°œ ë°˜í™˜
                all_results.sort(key=lambda x: x[1], reverse=True)
                return all_results[:k]
            
        except Exception as e:
            print(f"âš ï¸ ì´ì¤‘ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            # í´ë°±: ê¸°ë³¸ ê²€ìƒ‰ë§Œ ìˆ˜í–‰
            return self.similarity_search_with_score(query, "basic", k)
    
    def enhanced_card_search(self, query, k=5):
        """ì¹´ë“œ ê´€ë ¨ ì¿¼ë¦¬ì— ìµœì í™”ëœ ê²€ìƒ‰ - ì´ë¯¸ì§€ í¬í•¨ ë¬¸ì„œ ìš°ì„ """
        try:
            # ë‹¤ì–‘í•œ ì¹´ë“œ ë°œê¸‰ ê´€ë ¨ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
            search_terms = [
                query,
                "ì¹´ë“œë°œê¸‰ì ˆì°¨",
                "íšŒì›ì€í–‰ë³„ ì¹´ë“œë°œê¸‰ì•ˆë‚´", 
                "ì‹ ì²­ ì¤€ë¹„ ì„œë¥˜",
                "ì‹¬ì‚¬ ê³¼ì •",
                "ë°œê¸‰ íŒ",
                "ìŠ¹ì¸ë¥  ë†’ì´ëŠ” ë°©ë²•"
            ]
            
            all_results = []
            
            # ê° ê²€ìƒ‰ì–´ë¡œ basic ì»¨ë ‰ì…˜ì—ì„œ ê²€ìƒ‰
            for term in search_terms:
                try:
                    results = self.similarity_search_with_score(term, "basic", 3)
                    for doc, score in results:
                        doc.metadata['search_source'] = 'basic_enhanced'
                        # ì´ë¯¸ì§€ í¬í•¨ ë¬¸ì„œ ìš°ì„  ì²˜ë¦¬
                        if '![' in doc.page_content or '.gif' in doc.page_content or '.png' in doc.page_content:
                            score = score * 1.2  # ì´ë¯¸ì§€ í¬í•¨ ë¬¸ì„œ ê°€ì  ë¶€ì—¬
                            doc.metadata['has_image'] = True
                        all_results.append((doc, score))
                except:
                    continue
            
            # custom ì»¨ë ‰ì…˜ì—ì„œë„ ê²€ìƒ‰
            custom_results = self.similarity_search_with_score(query, "custom", k)
            for doc, score in custom_results:
                doc.metadata['search_source'] = 'custom_chunking'
                all_results.append((doc, score))
            
            # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ìˆœ ì •ë ¬
            unique_results = []
            seen_content = set()
            
            for doc, score in all_results:
                content_hash = hash(doc.page_content[:200])  # ì²« 200ìë¡œ ì¤‘ë³µ ì²´í¬
                if content_hash not in seen_content:
                    seen_content.add(content_hash)
                    unique_results.append((doc, score))
            
            # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ k*2 ë°˜í™˜ (ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸)
            unique_results.sort(key=lambda x: x[1], reverse=True)
            return unique_results[:k*2]
            
        except Exception as e:
            print(f"âš ï¸ ê°•í™”ëœ ì¹´ë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return self.dual_search(query, k)
    
    def _get_vectorstore_by_type(self, chunking_type):
        """ì²­í‚¹ íƒ€ì…ì— ë”°ë¥¸ ë²¡í„°ìŠ¤í† ì–´ ë°˜í™˜"""
        if chunking_type == "custom" or chunking_type == "custom_delimiter":
            return self.custom_vectorstore
        else:
            return self.basic_vectorstore
    
    def get_document_count(self, chunking_type=None):
        """ë¬¸ì„œ ìˆ˜ ì¡°íšŒ"""
        if chunking_type == "basic":
            try:
                return self.basic_vectorstore._collection.count()
            except:
                return 0
        elif chunking_type == "custom":
            try:
                return self.custom_vectorstore._collection.count()
            except:
                return 0
        else:
            # ì „ì²´ ë¬¸ì„œ ìˆ˜
            try:
                basic_count = self.basic_vectorstore._collection.count()
                custom_count = self.custom_vectorstore._collection.count()
                return {"basic": basic_count, "custom": custom_count, "total": basic_count + custom_count}
            except:
                return {"basic": 0, "custom": 0, "total": 0}
    
    def clear_vectorstore(self, chunking_type="all"):
        """ë²¡í„°ìŠ¤í† ì–´ ì‚­ì œ"""
        if chunking_type == "all" or chunking_type == "basic":
            try:
                self.basic_vectorstore.delete_collection()
            except:
                pass
                
        if chunking_type == "all" or chunking_type == "custom":
            try:
                self.custom_vectorstore.delete_collection()
            except:
                pass
        
        # ì¬ì´ˆê¸°í™”
        self.initialize_vectorstores()
        
        # ìºì‹œ ì‚­ì œ
        try:
            from services.hybrid_cache_manager import HybridCacheManager
            cache_manager = HybridCacheManager()
            result = cache_manager.clear_all()
            print(f"ğŸ—‘ï¸ ë²¡í„°DB ì´ˆê¸°í™”ì™€ í•¨ê»˜ ìºì‹œ ì‚­ì œë¨")
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def get_retriever(self, chunking_type="basic", k=5):
        """ì²­í‚¹ íƒ€ì…ë³„ ë¦¬íŠ¸ë¦¬ë²„ ë°˜í™˜"""
        vectorstore = self._get_vectorstore_by_type(chunking_type)
        return vectorstore.as_retriever(search_kwargs={"k": k})

# ì „ì—­ ì´ì¤‘ ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤
_dual_vectorstore_instance = None

def get_dual_vectorstore():
    """ì „ì—­ ì´ì¤‘ ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _dual_vectorstore_instance
    if _dual_vectorstore_instance is None:
        from models.embeddings import EmbeddingManager
        embedding_manager = EmbeddingManager()
        _dual_vectorstore_instance = DualVectorStoreManager(embedding_manager.get_embeddings())
    return _dual_vectorstore_instance

def reset_dual_vectorstore():
    """ì´ì¤‘ ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤ ë¦¬ì…‹"""
    global _dual_vectorstore_instance
    _dual_vectorstore_instance = None