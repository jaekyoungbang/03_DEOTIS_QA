import chromadb
from chromadb.config import Settings
from langchain_community.vectorstores import Chroma
from langchain.schema import Document
from config import Config
import os

class DualVectorStoreManager:
    """ì´ì¤‘ ë²¡í„°ìŠ¤í† ì–´ ê´€ë¦¬ì - ê¸°ë³¸ ì²­í‚¹ê³¼ ì»¤ìŠ¤í…€ ì²­í‚¹ ë¶„ë¦¬"""
    
    def __init__(self, embedding_function):
        self.embedding_function = embedding_function
        self.basic_vectorstore = None
        self.custom_vectorstore = None
        self.persist_directory = Config.CHROMA_PERSIST_DIRECTORY
        
        # ë³„ë„ ì»¬ë ‰ì…˜ëª… ì„¤ì •
        self.basic_collection_name = "basic_chunks"
        self.custom_collection_name = "custom_chunks"
        
        self.initialize_vectorstores()
    
    def initialize_vectorstores(self):
        """ê¸°ë³¸/ì»¤ìŠ¤í…€ ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        os.makedirs(self.persist_directory, exist_ok=True)
        
        # ê¸°ë³¸ ì²­í‚¹ìš© ë²¡í„°ìŠ¤í† ì–´
        self.basic_vectorstore = Chroma(
            collection_name=self.basic_collection_name,
            embedding_function=self.embedding_function,
            persist_directory=self.persist_directory
        )
        
        # ì»¤ìŠ¤í…€ ì²­í‚¹ìš© ë²¡í„°ìŠ¤í† ì–´  
        self.custom_vectorstore = Chroma(
            collection_name=self.custom_collection_name,
            embedding_function=self.embedding_function,
            persist_directory=self.persist_directory
        )
    
    def add_documents(self, documents, chunking_type="basic", ids=None):
        """ì²­í‚¹ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ë²¡í„°ìŠ¤í† ì–´ì— ë¬¸ì„œ ì¶”ê°€"""
        vectorstore = self._get_vectorstore_by_type(chunking_type)
        
        if ids:
            vectorstore.add_documents(documents=documents, ids=ids)
        else:
            vectorstore.add_documents(documents=documents)
        
        vectorstore.persist()
        return True
    
    def similarity_search(self, query, chunking_type="basic", k=5):
        """ì²­í‚¹ íƒ€ì…ë³„ ìœ ì‚¬ë„ ê²€ìƒ‰"""
        vectorstore = self._get_vectorstore_by_type(chunking_type)
        return vectorstore.similarity_search(query, k=k)
    
    def similarity_search_with_score(self, query, chunking_type="basic", k=5):
        """ì²­í‚¹ íƒ€ì…ë³„ ì ìˆ˜ í¬í•¨ ìœ ì‚¬ë„ ê²€ìƒ‰ - BGE-M3 ìµœì í™”ëœ ìœ ì‚¬ë„ ê³„ì‚°"""
        vectorstore = self._get_vectorstore_by_type(chunking_type)
        
        # ChromaDBì˜ similarity_search_with_score ì‚¬ìš© (ê±°ë¦¬ê°’ ë°˜í™˜)
        results = vectorstore.similarity_search_with_score(query, k=k)
        
        # BGE-M3 ì„ë² ë”©ì— ìµœì í™”ëœ ê±°ë¦¬-ìœ ì‚¬ë„ ë³€í™˜
        converted_results = []
        for doc, distance in results:
            # BGE-M3ëŠ” cosine distanceë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ 0~2 ë²”ìœ„ì˜ ê°’ì´ ë‚˜ì˜´
            # ë” ì •êµí•œ ìœ ì‚¬ë„ ê³„ì‚° ì ìš©
            if distance <= 2.0:
                # Cosine distance -> similarity ë³€í™˜
                # cosine similarity = 1 - cosine distance
                base_similarity = max(0, min(1, 1 - distance))
                
                # BGE-M3 íŠ¹ì„±ì„ ê³ ë ¤í•œ ìŠ¤ì¼€ì¼ë§
                # 0.3 ì´í•˜: ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„ (0.85~1.0)
                # 0.3~0.6: ë†’ì€ ìœ ì‚¬ë„ (0.7~0.85)  
                # 0.6~0.9: ì¤‘ê°„ ìœ ì‚¬ë„ (0.5~0.7)
                # 0.9~1.2: ë‚®ì€ ìœ ì‚¬ë„ (0.3~0.5)
                # 1.2+: ë§¤ìš° ë‚®ì€ ìœ ì‚¬ë„ (0.0~0.3)
                
                if distance <= 0.3:
                    # ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„: 85-100%
                    similarity = 0.85 + (0.3 - distance) / 0.3 * 0.15
                elif distance <= 0.6:
                    # ë†’ì€ ìœ ì‚¬ë„: 70-85%
                    similarity = 0.70 + (0.6 - distance) / 0.3 * 0.15
                elif distance <= 0.9:
                    # ì¤‘ê°„ ìœ ì‚¬ë„: 50-70%
                    similarity = 0.50 + (0.9 - distance) / 0.3 * 0.20
                elif distance <= 1.2:
                    # ë‚®ì€ ìœ ì‚¬ë„: 30-50%
                    similarity = 0.30 + (1.2 - distance) / 0.3 * 0.20
                else:
                    # ë§¤ìš° ë‚®ì€ ìœ ì‚¬ë„: 0-30%
                    similarity = max(0, 0.30 - (distance - 1.2) / 0.8 * 0.30)
                    
            else:
                # L2 distanceì¸ ê²½ìš° (BGE-M3ì—ì„œëŠ” ë“œë¬¼ì§€ë§Œ ì˜ˆì™¸ì²˜ë¦¬)
                import math
                similarity = math.exp(-distance / 2048.0)  # 1024ì°¨ì› * 2 ìŠ¤ì¼€ì¼
            
            # ì‹œë§¨í‹± ê´€ë ¨ë„ ë³´ì • (ì„ íƒì )
            try:
                from services.enhanced_query_processor import EnhancedQueryProcessor
                processor = EnhancedQueryProcessor()
                semantic_bonus = processor.calculate_semantic_relevance(query, doc.page_content[:500])
                # ì‹œë§¨í‹± ë³´ë„ˆìŠ¤ë¥¼ ìµœëŒ€ 10% ì¶”ê°€
                similarity = min(1.0, similarity + semantic_bonus * 0.1)
            except:
                pass  # ì—ëŸ¬ ë°œìƒì‹œ ê¸°ë³¸ ìœ ì‚¬ë„ë§Œ ì‚¬ìš©
                
            converted_results.append((doc, similarity))
        
        return converted_results
    
    def dual_search(self, query, k=5):
        """ê¸°ë³¸/ì»¤ìŠ¤í…€ ë‘ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ë™ì‹œ ê²€ìƒ‰ - ê°•í™”ëœ ê°œì¸í™” ë° ì‹œë§¨í‹± ê²€ìƒ‰"""
        try:
            # ì§ˆì˜ í™•ì¥ ì²˜ë¦¬
            from services.enhanced_query_processor import EnhancedQueryProcessor
            processor = EnhancedQueryProcessor()
            
            # ê°œì¸í™” ì¿¼ë¦¬ ë° ì¹´ë“œ ê´€ë ¨ ì¿¼ë¦¬ ê°ì§€
            intents = processor.extract_intent_keywords(query)
            is_personalized = bool(intents["person"])
            is_card_query = bool(intents["card_type"]) or any(keyword in query for keyword in ["ì¹´ë“œ", "ë°œê¸‰", "íšŒì›ì€í–‰"])
            
            print(f"ğŸ” [DualSearch] ê°œì¸í™”: {is_personalized}, ì¹´ë“œê´€ë ¨: {is_card_query}")
            print(f"ğŸ¯ [DualSearch] ì˜ë„ë¶„ì„: {intents}")
            
            all_results = []
            
            if is_personalized and is_card_query:
                # ê°œì¸í™”ëœ ì¹´ë“œ ì¿¼ë¦¬: ê°€ì¥ ì •êµí•œ ê²€ìƒ‰
                print(f"ğŸ’³ [DualSearch] ê°œì¸í™” ì¹´ë“œ ì¿¼ë¦¬ ì²˜ë¦¬")
                
                # 1. ì›ë³¸ ì¿¼ë¦¬ë¡œ ê¸°ë³¸/ì»¤ìŠ¤í…€ ê²€ìƒ‰
                basic_results = self.similarity_search_with_score(query, "basic", k*2)
                custom_results = self.similarity_search_with_score(query, "custom", k*2)
                
                for doc, score in basic_results:
                    doc.metadata['search_source'] = 'basic_personalized'
                    # ê°œì¸ëª…ì´ í¬í•¨ëœ ë¬¸ì„œì— ê°€ì 
                    person_bonus = 0.1 if any(person in doc.page_content for person in intents["person"]) else 0
                    all_results.append((doc, min(1.0, score + person_bonus)))
                
                for doc, score in custom_results:
                    doc.metadata['search_source'] = 'custom_personalized'
                    person_bonus = 0.1 if any(person in doc.page_content for person in intents["person"]) else 0
                    all_results.append((doc, min(1.0, score + person_bonus)))
                
                # 2. í™•ì¥ëœ ê°œì¸í™” ì¿¼ë¦¬ë“¤ë¡œ ì¶”ê°€ ê²€ìƒ‰
                expanded_queries = processor.build_hybrid_search_queries(query)
                for query_info in expanded_queries[1:4]:  # ìƒìœ„ 3ê°œ í™•ì¥ ì¿¼ë¦¬
                    try:
                        exp_basic = self.similarity_search_with_score(query_info["query"], "basic", 2)
                        for doc, score in exp_basic:
                            doc.metadata['search_source'] = f'basic_expanded_{query_info["type"]}'
                            weighted_score = score * query_info["weight"]
                            all_results.append((doc, weighted_score))
                    except:
                        continue
                
                # 3. íŠ¹ì • ì€í–‰/ì¹´ë“œì‚¬ ê´€ë ¨ ë¬¸ì„œ ë¶€ìŠ¤íŒ…
                for bank in intents.get("bank", []):
                    try:
                        bank_query = f"{bank} ì¹´ë“œ ë°œê¸‰ ì•ˆë‚´"
                        bank_results = self.similarity_search_with_score(bank_query, "basic", 3)
                        for doc, score in bank_results:
                            doc.metadata['search_source'] = f'basic_bank_{bank}'
                            all_results.append((doc, score * 0.95))  # ì•½ê°„ì˜ ê°€ì¤‘ì¹˜
                    except:
                        continue
                        
                # ìƒìœ„ 20ê°œ ë°˜í™˜ (ê°œì¸í™”ì—ì„œëŠ” ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ í•„ìš”)
                all_results.sort(key=lambda x: x[1], reverse=True)
                unique_results = self._remove_duplicates(all_results)
                return unique_results[:20]
                
            elif is_card_query:
                # ì¼ë°˜ ì¹´ë“œ ì¿¼ë¦¬: ê¸°ì¡´ ê°•í™” ë¡œì§
                print(f"ğŸ¦ [DualSearch] ì¼ë°˜ ì¹´ë“œ ì¿¼ë¦¬ ì²˜ë¦¬")
                extended_keywords = [
                    f"{query} ë°œê¸‰ì ˆì°¨",
                    f"{query} ì‹ ì²­ë°©ë²•", 
                    "íšŒì›ì€í–‰ë³„ ì¹´ë“œë°œê¸‰ì•ˆë‚´",
                    "ì¹´ë“œ ë°œê¸‰ ì ˆì°¨ ì•ˆë‚´",
                    "ì‹ ì²­ ì¤€ë¹„ì„œë¥˜",
                    "ì¹´ë“œ ì‹¬ì‚¬ ê³¼ì •"
                ]
                
                # ê¸°ë³¸/ì»¤ìŠ¤í…€ ê²€ìƒ‰
                basic_results = self.similarity_search_with_score(query, "basic", k)
                custom_results = self.similarity_search_with_score(query, "custom", k)
                
                for doc, score in basic_results:
                    doc.metadata['search_source'] = 'basic_chunking'
                    all_results.append((doc, score))
                
                for doc, score in custom_results:
                    doc.metadata['search_source'] = 'custom_chunking' 
                    all_results.append((doc, score))
                
                # í™•ì¥ í‚¤ì›Œë“œ ê²€ìƒ‰
                for keyword in extended_keywords[:3]:
                    try:
                        extended_basic = self.similarity_search_with_score(keyword, "basic", 2)
                        for doc, score in extended_basic:
                            doc.metadata['search_source'] = 'basic_chunking_extended'
                            all_results.append((doc, score * 0.9))
                    except:
                        continue
                
                all_results.sort(key=lambda x: x[1], reverse=True)
                unique_results = self._remove_duplicates(all_results)
                return unique_results[:15]
            
            else:
                # ì¼ë°˜ ì¿¼ë¦¬ì˜ ê²½ìš° ê¸°ì¡´ ë°©ì‹
                print(f"ğŸ“„ [DualSearch] ì¼ë°˜ ì¿¼ë¦¬ ì²˜ë¦¬")
                basic_results = self.similarity_search_with_score(query, "basic", k//2 + 1)
                custom_results = self.similarity_search_with_score(query, "custom", k//2 + 1)
                
                # ê¸°ë³¸ ì²­í‚¹ ê²°ê³¼ ì¶”ê°€
                for doc, score in basic_results:
                    doc.metadata['search_source'] = 'basic_chunking'
                    all_results.append((doc, score))
                
                # ì»¤ìŠ¤í…€ ì²­í‚¹ ê²°ê³¼ ì¶”ê°€
                for doc, score in custom_results:
                    doc.metadata['search_source'] = 'custom_chunking' 
                    all_results.append((doc, score))
                
                # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ kê°œ ë°˜í™˜
                all_results.sort(key=lambda x: x[1], reverse=True)
                unique_results = self._remove_duplicates(all_results)
                return unique_results[:k]
            
        except Exception as e:
            print(f"âš ï¸ ì´ì¤‘ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            import traceback
            print(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            # í´ë°±: ê¸°ë³¸ ê²€ìƒ‰ë§Œ ìˆ˜í–‰
            return self.similarity_search_with_score(query, "basic", k)
    
    def _remove_duplicates(self, results):
        """ì¤‘ë³µ ë¬¸ì„œ ì œê±° - ë‚´ìš© ê¸°ë°˜"""
        unique_results = []
        seen_content = set()
        
        for doc, score in results:
            # ì²« 200ìë¡œ ì¤‘ë³µ ì²´í¬ (ë” ì •í™•í•œ ì¤‘ë³µ ê°ì§€)
            content_hash = hash(doc.page_content[:200])
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                unique_results.append((doc, score))
        
        return unique_results
    
    def enhanced_card_search(self, query, k=5):
        """ì¹´ë“œ ê´€ë ¨ ì¿¼ë¦¬ì— ìµœì í™”ëœ ê²€ìƒ‰ - ì´ë¯¸ì§€ í¬í•¨ ë¬¸ì„œ ìš°ì„ """
        try:
            # ë‹¤ì–‘í•œ ì¹´ë“œ ë°œê¸‰ ê´€ë ¨ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
            search_terms = [
                query,
                "ì¹´ë“œë°œê¸‰ì ˆì°¨",
                "íšŒì›ì€í–‰ë³„ ì¹´ë“œë°œê¸‰ì•ˆë‚´", 
                "ì‹ ì²­ ì¤€ë¹„ ì„œë¥˜",
                "ì‹¬ì‚¬ ê³¼ì •",
                "ë°œê¸‰ íŒ",
                "ìŠ¹ì¸ë¥  ë†’ì´ëŠ” ë°©ë²•"
            ]
            
            all_results = []
            
            # ê° ê²€ìƒ‰ì–´ë¡œ basic ì»¨ë ‰ì…˜ì—ì„œ ê²€ìƒ‰
            for term in search_terms:
                try:
                    results = self.similarity_search_with_score(term, "basic", 3)
                    for doc, score in results:
                        doc.metadata['search_source'] = 'basic_enhanced'
                        # ì´ë¯¸ì§€ í¬í•¨ ë¬¸ì„œ ìš°ì„  ì²˜ë¦¬
                        if '![' in doc.page_content or '.gif' in doc.page_content or '.png' in doc.page_content:
                            score = score * 1.2  # ì´ë¯¸ì§€ í¬í•¨ ë¬¸ì„œ ê°€ì  ë¶€ì—¬
                            doc.metadata['has_image'] = True
                        all_results.append((doc, score))
                except:
                    continue
            
            # custom ì»¨ë ‰ì…˜ì—ì„œë„ ê²€ìƒ‰
            custom_results = self.similarity_search_with_score(query, "custom", k)
            for doc, score in custom_results:
                doc.metadata['search_source'] = 'custom_chunking'
                all_results.append((doc, score))
            
            # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ìˆœ ì •ë ¬
            unique_results = []
            seen_content = set()
            
            for doc, score in all_results:
                content_hash = hash(doc.page_content[:200])  # ì²« 200ìë¡œ ì¤‘ë³µ ì²´í¬
                if content_hash not in seen_content:
                    seen_content.add(content_hash)
                    unique_results.append((doc, score))
            
            # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ k*2 ë°˜í™˜ (ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸)
            unique_results.sort(key=lambda x: x[1], reverse=True)
            return unique_results[:k*2]
            
        except Exception as e:
            print(f"âš ï¸ ê°•í™”ëœ ì¹´ë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return self.dual_search(query, k)
    
    def _get_vectorstore_by_type(self, chunking_type):
        """ì²­í‚¹ íƒ€ì…ì— ë”°ë¥¸ ë²¡í„°ìŠ¤í† ì–´ ë°˜í™˜"""
        if chunking_type == "custom" or chunking_type == "custom_delimiter":
            return self.custom_vectorstore
        else:
            return self.basic_vectorstore
    
    def get_document_count(self, chunking_type=None):
        """ë¬¸ì„œ ìˆ˜ ì¡°íšŒ"""
        if chunking_type == "basic":
            try:
                return self.basic_vectorstore._collection.count()
            except:
                return 0
        elif chunking_type == "custom":
            try:
                return self.custom_vectorstore._collection.count()
            except:
                return 0
        else:
            # ì „ì²´ ë¬¸ì„œ ìˆ˜
            try:
                basic_count = self.basic_vectorstore._collection.count()
                custom_count = self.custom_vectorstore._collection.count()
                return {"basic": basic_count, "custom": custom_count, "total": basic_count + custom_count}
            except:
                return {"basic": 0, "custom": 0, "total": 0}
    
    def clear_vectorstore(self, chunking_type="all"):
        """ë²¡í„°ìŠ¤í† ì–´ ì‚­ì œ"""
        if chunking_type == "all" or chunking_type == "basic":
            try:
                self.basic_vectorstore.delete_collection()
            except:
                pass
                
        if chunking_type == "all" or chunking_type == "custom":
            try:
                self.custom_vectorstore.delete_collection()
            except:
                pass
        
        # ì¬ì´ˆê¸°í™”
        self.initialize_vectorstores()
        
        # ìºì‹œ ì‚­ì œ
        try:
            from services.hybrid_cache_manager import HybridCacheManager
            cache_manager = HybridCacheManager()
            result = cache_manager.clear_all()
            print(f"ğŸ—‘ï¸ ë²¡í„°DB ì´ˆê¸°í™”ì™€ í•¨ê»˜ ìºì‹œ ì‚­ì œë¨")
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def get_retriever(self, chunking_type="basic", k=5):
        """ì²­í‚¹ íƒ€ì…ë³„ ë¦¬íŠ¸ë¦¬ë²„ ë°˜í™˜"""
        vectorstore = self._get_vectorstore_by_type(chunking_type)
        return vectorstore.as_retriever(search_kwargs={"k": k})

# ì „ì—­ ì´ì¤‘ ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤
_dual_vectorstore_instance = None

def get_dual_vectorstore():
    """ì „ì—­ ì´ì¤‘ ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _dual_vectorstore_instance
    if _dual_vectorstore_instance is None:
        from models.embeddings import EmbeddingManager
        embedding_manager = EmbeddingManager()
        _dual_vectorstore_instance = DualVectorStoreManager(embedding_manager.get_embeddings())
    return _dual_vectorstore_instance

def reset_dual_vectorstore():
    """ì´ì¤‘ ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤ ë¦¬ì…‹"""
    global _dual_vectorstore_instance
    _dual_vectorstore_instance = None