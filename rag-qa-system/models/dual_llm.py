from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema import StrOutputParser
from typing import Dict, Any
import os
from config import Config

class DualLLMManager:
    """APIÏôÄ Î°úÏª¨ LLMÏùÑ ÎèôÏãúÏóê Í¥ÄÎ¶¨ÌïòÎäî ÌÅ¥ÎûòÏä§"""
    
    def __init__(self):
        self.api_llm = None
        self.local_llm = None
        self.api_chain = None
        self.local_chain = None
        self._initialize_llms()
        self._create_chains()
    
    def _initialize_llms(self):
        """LLM Ï¥àÍ∏∞Ìôî"""
        # API LLM (OpenAI)
        api_config = Config.LLM_MODELS['api']
        if api_config['provider'] == 'openai' and Config.OPENAI_API_KEY:
            self.api_llm = ChatOpenAI(
                model_name=api_config['model_name'],
                temperature=api_config['temperature'],
                max_tokens=api_config['max_tokens'],
                api_key=Config.OPENAI_API_KEY
            )
        
        # Local LLM (vLLM OpenAI Ìò∏Ìôò)
        local_config = Config.LLM_MODELS['local']
        if local_config['provider'] == 'vllm':
            try:
                self.local_llm = ChatOpenAI(
                    model=local_config['model_name'],
                    openai_api_base=local_config['base_url'] + '/v1',
                    openai_api_key='EMPTY',
                    temperature=local_config['temperature'],
                    max_tokens=local_config['max_tokens']
                )
            except Exception as e:
                print(f"Î°úÏª¨ LLM Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
                self.local_llm = None
        else:
            print(f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Î°úÏª¨ LLM provider: {local_config['provider']}")
            self.local_llm = None
    
    def _create_chains(self):
        """RAG Ï≤¥Ïù∏ ÏÉùÏÑ±"""
        # API Chain
        if self.api_llm:
            api_prompt = ChatPromptTemplate.from_messages([
                ("system", Config.LLM_MODELS['api']['system_prompt']),
                ("human", """Îã§Ïùå Î¨∏ÏÑúÎ•º Ï∞∏Í≥†ÌïòÏó¨ ÏßàÎ¨∏Ïóê ÎãµÎ≥ÄÌï¥Ï£ºÏÑ∏Ïöî.

Î¨∏ÏÑú:
{context}

ÏßàÎ¨∏: {question}

ÎãµÎ≥Ä:""")
            ])
            
            self.api_chain = (
                {"context": RunnablePassthrough(), "question": RunnablePassthrough()}
                | api_prompt
                | self.api_llm
                | StrOutputParser()
            )
        
        # Local Chain
        if self.local_llm:
            local_prompt = ChatPromptTemplate.from_template(
                Config.LLM_MODELS['local']['system_prompt'] + """
                
Context:
{context}

Question: {question}

Answer:"""
            )
            
            # vLLMÏö© Ïª§Ïä§ÌÖÄ Ï∂úÎ†• ÌååÏÑú Ï∂îÍ∞Ä
            def parse_vllm_output(output):
                """vLLM Ï∂úÎ†•ÏóêÏÑú content Ï∂îÏ∂ú"""
                if hasattr(output, 'content'):
                    return output.content
                elif isinstance(output, str):
                    return output
                else:
                    return str(output)
            
            self.local_chain = (
                {"context": RunnablePassthrough(), "question": RunnablePassthrough()}
                | local_prompt
                | self.local_llm
                | parse_vllm_output
            )
    
    def get_api_chain(self):
        """API Ï≤¥Ïù∏ Î∞òÌôò"""
        if not self.api_chain:
            raise ValueError("API LLMÏù¥ Ï¥àÍ∏∞ÌôîÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. API ÌÇ§Î•º ÌôïÏù∏ÌïòÏÑ∏Ïöî.")
        return self.api_chain
    
    def get_local_chain(self, model_name=None):
        """Î°úÏª¨ Ï≤¥Ïù∏ Î∞òÌôò (ÎèôÏ†Å Î™®Îç∏ ÏßÄÏõê)"""
        if model_name and model_name != getattr(self.local_llm, 'model', None):
            # ÎèôÏ†ÅÏúºÎ°ú Î™®Îç∏ Î≥ÄÍ≤Ω
            local_config = Config.LLM_MODELS['local']
            if local_config['provider'] == 'vllm':
                try:
                    print(f"üîÑ ÎèôÏ†Å Î°úÏª¨ Î™®Îç∏ Ï†ÑÌôò: {model_name}")
                    dynamic_llm = ChatOpenAI(
                        model=model_name,
                        openai_api_base=local_config['base_url'] + '/v1',
                        openai_api_key='EMPTY',
                        temperature=local_config['temperature'],
                        max_tokens=local_config['max_tokens']
                    )
                    
                    # ÎèôÏ†Å Ï≤¥Ïù∏ ÏÉùÏÑ±
                    local_prompt = ChatPromptTemplate.from_template(
                        Config.LLM_MODELS['local']['system_prompt'] + """
                        
Context:
{context}

Question: {question}

Answer:"""
                    )
                    
                    def parse_vllm_output(output):
                        if hasattr(output, 'content'):
                            return output.content
                        elif isinstance(output, str):
                            return output
                        else:
                            return str(output)
                    
                    dynamic_chain = (
                        {"context": RunnablePassthrough(), "question": RunnablePassthrough()}
                        | local_prompt
                        | dynamic_llm
                        | parse_vllm_output
                    )
                    
                    return dynamic_chain
                    
                except Exception as e:
                    print(f"ÎèôÏ†Å Î°úÏª¨ LLM({model_name}) Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
                    # Í∏∞Î≥∏ Ï≤¥Ïù∏ ÏÇ¨Ïö©
                    pass
        
        if not self.local_chain:
            raise ValueError("ÏÇ¨ÎÇ¥ÏÑúÎ≤Ñ vLLMÏù¥ Ï¥àÍ∏∞ÌôîÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. vLLM ÏÑúÎ≤ÑÍ∞Ä Ïã§Ìñâ Ï§ëÏù∏ÏßÄ ÌôïÏù∏ÌïòÏÑ∏Ïöî.")
        return self.local_chain
    
    def get_available_models(self) -> Dict[str, bool]:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ ÌôïÏù∏ (Ïã§Ï†ú Ïó∞Í≤∞ ÌÖåÏä§Ìä∏)"""
        api_available = False
        local_available = False
        
        # API LLM ÌÖåÏä§Ìä∏
        if self.api_llm:
            try:
                import openai
                # OpenAI API ÌÇ§ ÌÖåÏä§Ìä∏
                if not Config.OPENAI_API_KEY or Config.OPENAI_API_KEY.strip() == "":
                    print("OpenAI API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏùå")
                    api_available = False
                else:
                    # Í∞ÑÎã®Ìïú ÌÖåÏä§Ìä∏ Î©îÏãúÏßÄÎ°ú Ïó∞Í≤∞ ÌôïÏù∏
                    test_response = self.api_llm.invoke("ÌÖåÏä§Ìä∏")
                    api_available = True
                    print("[LLM] ChatGPT API Ïó∞Í≤∞ ÏÑ±Í≥µ")
            except Exception as e:
                print(f"API LLM Ïó∞Í≤∞ Ïã§Ìå®: {e}")
                api_available = False
        else:
            print("API LLM Í∞ùÏ≤¥Í∞Ä ÏóÜÏùå")
        
        # Î°úÏª¨ LLM ÌÖåÏä§Ìä∏ (vLLM ÏÑúÎ≤Ñ)
        if self.local_llm:
            try:
                import requests
                local_config = Config.LLM_MODELS['local']
                
                if local_config['provider'] == 'vllm':
                    # vLLM ÏÑúÎ≤Ñ Ïó∞Í≤∞ ÌÖåÏä§Ìä∏
                    test_url = local_config['base_url'] + '/v1/models'
                    response = requests.get(test_url, timeout=3)
                    if response.status_code == 200:
                        # Ïã§Ï†ú LLM Ìò∏Ï∂ú ÌÖåÏä§Ìä∏
                        test_response = self.local_llm.invoke("ÌÖåÏä§Ìä∏")
                        local_available = True
                        print(f"[LLM] vLLM ÏÑúÎ≤Ñ Ïó∞Í≤∞ ÏÑ±Í≥µ - {local_config['base_url']}")
                    else:
                        print(f"vLLM ÏÑúÎ≤ÑÍ∞Ä ÏùëÎãµÌïòÏßÄ ÏïäÏùå: {response.status_code}")
                        local_available = False
                elif local_config['provider'] == 'ollama':
                    # Ollama ÏÑúÎ≤Ñ Ïó∞Í≤∞ ÌÖåÏä§Ìä∏ (Î∞±ÏóÖ)
                    response = requests.get(local_config['base_url'], timeout=2)
                    if response.status_code == 200:
                        test_response = self.local_llm.invoke("ÌÖåÏä§Ìä∏")
                        local_available = True
                        print("Ollama ÏÑúÎ≤Ñ Ïó∞Í≤∞ ÏÑ±Í≥µ")
                    else:
                        print("Ollama ÏÑúÎ≤ÑÍ∞Ä ÏùëÎãµÌïòÏßÄ ÏïäÏùå")
                        local_available = False
            except requests.exceptions.ConnectionError:
                print(f"Î°úÏª¨ LLM ÏÑúÎ≤ÑÏóê Ïó∞Í≤∞Ìï† Ïàò ÏóÜÏùå ({local_config['base_url']})")
                local_available = False
            except requests.exceptions.Timeout:
                print("Î°úÏª¨ LLM ÏÑúÎ≤Ñ Ïó∞Í≤∞ ÌÉÄÏûÑÏïÑÏõÉ")
                local_available = False
            except Exception as e:
                print(f"Î°úÏª¨ LLM Ïó∞Í≤∞ Ïã§Ìå®: {e}")
                local_available = False
        else:
            print("Î°úÏª¨ LLM Í∞ùÏ≤¥Í∞Ä ÏóÜÏùå")
        
        result = {
            'api': api_available,
            'local': local_available
        }
        print(f"Î™®Îç∏ Í∞ÄÏö©ÏÑ± Í≤∞Í≥º: {result}")
        return result
    
    def get_model_info(self) -> Dict[str, Any]:
        """Î™®Îç∏ Ï†ïÎ≥¥ Î∞òÌôò"""
        info = {}
        
        if self.api_llm:
            info['api'] = {
                'provider': Config.LLM_MODELS['api']['provider'],
                'model': Config.LLM_MODELS['api']['model_name'],
                'display_name': Config.LLM_MODELS['api']['display_name'],
                'temperature': Config.LLM_MODELS['api']['temperature'],
                'status': 'ready'
            }
        else:
            info['api'] = {'status': 'not_initialized'}
        
        if self.local_llm:
            info['local'] = {
                'provider': Config.LLM_MODELS['local']['provider'],
                'model': Config.LLM_MODELS['local']['model_name'],
                'display_name': Config.LLM_MODELS['local']['display_name'],
                'temperature': Config.LLM_MODELS['local']['temperature'],
                'status': 'ready'
            }
        else:
            info['local'] = {'status': 'not_initialized'}
        
        return info