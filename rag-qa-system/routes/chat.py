from flask import Blueprint, request, jsonify, stream_with_context, Response
from services.rag_chain import RAGChain
from models.vectorstore import VectorStoreManager
from config import Config
import json
import time
import uuid

chat_bp = Blueprint('chat', __name__)

# Initialize RAG chain (consider using app context or dependency injection in production)
rag_chain = None

def get_rag_chain():
    global rag_chain
    if rag_chain is None:
        rag_chain = RAGChain()
    return rag_chain

@chat_bp.route('/query', methods=['POST'])
@chat_bp.route('/../rag/chat', methods=['POST'])
def query():
    """Handle chat queries"""
    try:
        data = request.get_json()
        question = data.get('question')
        use_memory = data.get('use_memory', False)
        llm_model = data.get('llm_model', 'gpt-4o-mini')
        search_mode = data.get('search_mode', 'basic')
        
        if not question:
            return jsonify({"error": "Question is required"}), 400
        
        # Get RAG chain instance
        chain = get_rag_chain()
        
        # Query the RAG system with search mode
        response = chain.query(
            question, 
            use_memory=use_memory, 
            llm_model=llm_model,
            search_mode=search_mode
        )
        
        return jsonify(response)
    
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@chat_bp.route('/stream', methods=['POST'])
def stream_query():
    """Handle streaming chat queries with real-time progress"""
    try:
        data = request.get_json()
        question = data.get('question')
        use_memory = data.get('use_memory', False)
        llm_model = data.get('llm_model', 'gpt-4o-mini')
        search_mode = data.get('search_mode', 'basic')
        summarize = data.get('summarize', False)
        
        if not question:
            return jsonify({"error": "Question is required"}), 400
        
        def generate():
            import time
            
            try:
                chain = get_rag_chain()
                
                # Step 4: 4ê°œ ë³„ë„ í”„ë¡œì„¸ìŠ¤ë¡œ ë¹„ë™ê¸° ì²˜ë¦¬
                from models.llm import LLMManager
                from openai import OpenAI
                import os
                import platform
                from queue import Queue, Empty
                from concurrent.futures import ThreadPoolExecutor, as_completed
                import uuid
                
                # í”„ë¡œì„¸ìŠ¤ë³„ í´ë” ê²½ë¡œ ì„¤ì •
                if platform.system() == "Windows":
                    s3_folder = "D:\\99_DEOTIS_QA_SYSTEM\\03_DEOTIS_QA\\s3"
                    s3_chunking_folder = "D:\\99_DEOTIS_QA_SYSTEM\\03_DEOTIS_QA\\s3-chunking"
                else:
                    s3_folder = "/mnt/d/99_DEOTIS_QA_SYSTEM/03_DEOTIS_QA/s3"
                    s3_chunking_folder = "/mnt/d/99_DEOTIS_QA_SYSTEM/03_DEOTIS_QA/s3-chunking"
                
                # 4ê°œ í”„ë¡œì„¸ìŠ¤ ì •ì˜
                processes = [
                    {"llm_type": "openai", "chunking": "basic", "folder": s3_folder, "name": "ChatGPT + s3ê¸°ë³¸", "process_id": 1},
                    {"llm_type": "openai", "chunking": "custom", "folder": s3_chunking_folder, "name": "ChatGPT + s3-chunking", "process_id": 2},
                    {"llm_type": "local", "chunking": "basic", "folder": s3_folder, "name": "ë¡œì»¬LLM + s3ê¸°ë³¸", "process_id": 3},
                    {"llm_type": "local", "chunking": "custom", "folder": s3_chunking_folder, "name": "ë¡œì»¬LLM + s3-chunking", "process_id": 4}
                ]
                
                # Create a queue for streaming results from multiple processes
                result_queue = Queue()
                
                def process_single_search_task(process_info, question, llm_model, chain, session_id, summarize=False):
                    """ë‹¨ì¼ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ ì²˜ë¦¬ - ì™„ì „ ë…ë¦½ì  ì‹¤í–‰"""
                    start_time = time.time()  # ì‹œì‘ ì‹œê°„ ê¸°ë¡
                    try:
                        process_name = process_info["name"]
                        chunking_type = process_info["chunking"]
                        llm_type = process_info["llm_type"]
                        process_id = process_info["process_id"]
                        
                        print(f"[PROCESS] Starting {process_name} (ID: {process_id})")
                        
                        # í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì‹ í˜¸
                        result_queue.put({
                            'type': 'process_start',
                            'process_name': process_name,
                            'process_id': process_id,
                            'session_id': session_id
                        })
                        
                        # ë¡œì»¬ LLM ì‚¬ìš© ë¶ˆê°€ëŠ¥ ì‹œ ì˜¤ë¥˜ ì²˜ë¦¬
                        if llm_type == "local":
                            try:
                                # ë¡œì»¬ LLM ì—°ê²° í…ŒìŠ¤íŠ¸
                                from models.llm import LLMManager
                                llm_manager = LLMManager()
                                llm_manager.get_llm(model_name=llm_model)
                            except Exception as e:
                                result_queue.put({
                                    'type': 'process_error',
                                    'process_name': process_name,
                                    'process_id': process_id,
                                    'session_id': session_id,
                                    'error': 'vllm_not_running',
                                    'message': 'ì‚¬ë‚´ì„œë²„ vLLMì´ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•´ê²°ë°©ë²•: 1. vLLM ì„œë²„ ìƒíƒœ í™•ì¸: 192.168.0.224:8412 2. kanana8b ëª¨ë¸ ë¡œë”© í™•ì¸ 3. ì„œë²„ ì¬ì‹œì‘ í•„ìš”ì‹œ ì—°ë½',
                                    'similarity_info': [],  # ì˜¤ë¥˜ ì‹œ ë¹ˆ ìœ ì‚¬ë„ ì •ë³´
                                    'total_time': 0.0,
                                    'status': 'failed'
                                })
                                return
                        
                        # í•´ë‹¹ ì²­í‚¹ íƒ€ì…ìœ¼ë¡œ ê²€ìƒ‰ ìˆ˜í–‰
                        print(f"[DEBUG] {process_name} ê²€ìƒ‰ ì‹œì‘ - ì²­í‚¹íƒ€ì…: {chunking_type}")
                        print(f"[DEBUG] Chain object: {chain}")
                        print(f"[DEBUG] Has dual_vectorstore_manager: {hasattr(chain, 'dual_vectorstore_manager')}")
                        
                        if hasattr(chain, 'dual_vectorstore_manager') and chain.dual_vectorstore_manager:
                            if chunking_type == "custom":
                                print(f"[DEBUG] custom ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰")
                                search_results = chain.dual_vectorstore_manager.similarity_search_with_score(question, "custom", k=3)
                            else:
                                print(f"[DEBUG] basic ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰")
                                search_results = chain.dual_vectorstore_manager.similarity_search_with_score(question, "basic", k=3)
                        else:
                            # í´ë°±: ê¸°ë³¸ ë²¡í„°ìŠ¤í† ì–´ ì‚¬ìš©
                            print(f"[DEBUG] í´ë°±: ê¸°ë³¸ ë²¡í„°ìŠ¤í† ì–´ ì‚¬ìš©")
                            search_results = chain.vectorstore_manager.similarity_search_with_score(question, k=3)
                        
                        print(f"[DEBUG] {process_name} ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ, ì²«ë²ˆì§¸ ì ìˆ˜: {search_results[0][1]:.2%}" if search_results else "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                        
                        if not search_results:
                            print(f"[WARNING] {process_name} - No search results found")
                            result_queue.put({
                                'type': 'process_error',
                                'process_name': process_name,
                                'process_id': process_id,
                                'session_id': session_id,
                                'error': 'no_results',
                                'message': 'ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.',
                                'total_time': round(time.time() - start_time, 2),
                                'status': 'failed',
                                'similarity_info': []
                            })
                            return
                        
                        # ê° ìˆœìœ„ë³„ë¡œ ê°œë³„ ë‹µë³€ ìƒì„±
                        rank_answers = []
                        
                        # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ìœ ì‚¬ë„ë³„ë¡œ ì²˜ë¦¬ (Top 3)
                        for rank, (doc, score) in enumerate(search_results[:3], 1):
                            try:
                                # ê°œë³„ ë¬¸ì„œì— ëŒ€í•œ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„± (chunking optimization)
                                # Truncate overly long documents to reduce LLM input length
                                max_doc_length = 2000  # Optimal length per document
                                if len(doc.page_content) > max_doc_length:
                                    individual_context = doc.page_content[:max_doc_length-3] + "..."
                                    print(f"[CHUNKING OPT] Document truncated from {len(doc.page_content)} to {len(individual_context)} chars")
                                else:
                                    individual_context = doc.page_content
                                # ìš”ì•½ ëª¨ë“œì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì¡°ì •
                                if summarize:
                                    individual_prompt = f"""ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì—¬ ì œê³µí•˜ì„¸ìš”.
í•µì‹¬ ë‚´ìš©ë§Œ í¬í•¨í•˜ê³  ë¶ˆí•„ìš”í•œ ì„¸ë¶€ì‚¬í•­ì€ ì œì™¸í•˜ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{individual_context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
                                else:
                                    individual_prompt = chain.prompt_template.format(context=individual_context, question=question)
                                
                                answer_text = ""
                                
                                if llm_type == "openai":
                                    # OpenAI API í˜¸ì¶œ (íƒ€ì„ì•„ì›ƒ 30ì´ˆ)
                                    client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), timeout=30.0)
                                    stream = client.chat.completions.create(
                                        model=llm_model,
                                        messages=[{"role": "user", "content": individual_prompt}],
                                        stream=True,
                                        temperature=0.1
                                    )
                                    
                                    # ì „ì²´ ë‹µë³€ì„ ìˆ˜ì§‘
                                    for chunk in stream:
                                        if chunk.choices[0].delta.content is not None:
                                            answer_text += chunk.choices[0].delta.content
                                
                                else:
                                    # ë¡œì»¬ LLM ì²˜ë¦¬
                                    llm_manager = LLMManager()
                                    llm = llm_manager.get_llm(model_name='local')  # ë¡œì»¬ LLM ì‚¬ìš©
                                    full_response = llm.invoke(individual_prompt)
                                    
                                    # vLLM ì‘ë‹µì—ì„œ content ì¶”ì¶œ
                                    if hasattr(full_response, 'content'):
                                        answer_text = full_response.content
                                    elif isinstance(full_response, str):
                                        answer_text = full_response
                                    else:
                                        answer_text = str(full_response)
                                
                                # ì™„ì„±ëœ ë‹µë³€ì„ ì €ì¥
                                rank_answers.append({
                                    'rank': rank,
                                    'score': f'{score:.1%}',
                                    'answer': answer_text,
                                    'source': doc.metadata.get('source_file', 'Unknown')
                                })
                                
                            except Exception as doc_error:
                                rank_answers.append({
                                    'rank': rank,
                                    'score': f'{score:.1%}',
                                    'answer': f'ì˜¤ë¥˜ ë°œìƒ: {str(doc_error)}',
                                    'source': doc.metadata.get('source_file', 'Unknown')
                                })
                        
                        # í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ - ìˆœìœ„ë³„ ë‹µë³€ë“¤ê³¼ ì‹œê°„ ì •ë³´ ì „ì†¡
                        end_time = time.time()
                        total_time = end_time - start_time
                        
                        # ìœ ì‚¬ë„ Top 3 ì •ë³´ ìƒì„±
                        similarity_info = []
                        for i, (doc, score) in enumerate(search_results[:3], 1):
                            similarity_info.append({
                                'rank': i,
                                'score': f'{score:.1%}',
                                'source': doc.metadata.get('source_file', 'Unknown'),
                                'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content
                            })
                        
                        print(f"[SUCCESS] Process {process_name} completed successfully")
                        print(f"[SUCCESS] Generated {len(rank_answers)} answers")
                        
                        result_queue.put({
                            'type': 'process_complete',
                            'process_name': process_name,
                            'process_id': process_id,
                            'session_id': session_id,
                            'rank_answers': rank_answers,
                            'similarity_info': similarity_info,
                            'total_time': round(total_time, 2),
                            'status': 'success',
                            'chunking_type': process_info["chunking"]  # chunking_type ì¶”ê°€
                        })
                        print(f"[SUCCESS] Complete event queued for process {process_id}")
                        
                    except Exception as e:
                        end_time = time.time()
                        total_time = end_time - start_time
                        print(f"[ERROR] Process {process_info['name']} failed: {str(e)}")
                        import traceback
                        print(f"[ERROR] Traceback: {traceback.format_exc()}")
                        
                        result_queue.put({
                            'type': 'process_error',
                            'process_name': process_info["name"],
                            'process_id': process_info["process_id"],
                            'session_id': session_id,
                            'error': str(e),
                            'message': f'í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}',
                            'total_time': round(total_time, 2),
                            'status': 'failed',
                            'similarity_info': []  # ì˜¤ë¥˜ ì‹œ ë¹ˆ ìœ ì‚¬ë„ ì •ë³´
                        })
                        print(f"[ERROR] Error event queued for process {process_info['process_id']}")
                
                # ì„¸ì…˜ ID ìƒì„±
                session_id = str(uuid.uuid4())
                
                # 4ê°œ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì•Œë¦¼
                yield f"data: {json.dumps({'type': 'all_processes_start', 'total_processes': len(processes), 'session_id': session_id})}\n\n"
                
                # 4ê°œ í”„ë¡œì„¸ìŠ¤ë¥¼ ë³‘ë ¬ë¡œ ì‹œì‘
                with ThreadPoolExecutor(max_workers=4) as executor:
                    futures = []
                    
                    for process_info in processes:
                        print(f"[DEBUG] Starting process: {process_info['name']} (ID: {process_info['process_id']})")
                        future = executor.submit(
                            process_single_search_task,
                            process_info, question, llm_model, chain, session_id, summarize
                        )
                        futures.append(future)
                    
                    print(f"[DEBUG] All {len(futures)} processes submitted")
                    
                    # ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° (ê°œë³„ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ì¦‰ì‹œ í‘œì‹œ)
                    completed_processes = 0
                    total_processes = len(processes)
                    
                    while completed_processes < total_processes:
                        try:
                            # íì—ì„œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒì„ ì§§ê²Œ ì„¤ì •í•˜ì—¬ ì‹¤ì‹œê°„ì„± í–¥ìƒ)
                            result = result_queue.get(timeout=0.05)
                            
                            # ê²°ê³¼ ì¦‰ì‹œ ìŠ¤íŠ¸ë¦¬ë° (ê° í”„ë¡œì„¸ìŠ¤ê°€ ì™„ë£Œë˜ëŠ” ìˆœê°„ ë°”ë¡œ í™”ë©´ì— í‘œì‹œ)
                            yield f"data: {json.dumps(result)}\n\n"
                            
                            # ì™„ë£Œëœ í”„ë¡œì„¸ìŠ¤ ì¹´ìš´íŠ¸
                            if result['type'] in ['process_complete', 'process_error']:
                                completed_processes += 1
                                print(f"[STREAM] í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ: {result.get('process_name', 'Unknown')} ({completed_processes}/{total_processes})")
                                
                        except Empty:
                            # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì•„ì£¼ ì§§ì€ ì‹œê°„ ëŒ€ê¸° í›„ ê³„ì† í´ë§
                            time.sleep(0.01)
                            continue
                    
                    # ëª¨ë“  futures ì •ë¦¬
                    for future in as_completed(futures, timeout=30):
                        try:
                            future.result()
                        except Exception as e:
                            print(f"í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ì˜¤ë¥˜: {e}")
                
                # ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€
                yield f"data: {json.dumps({'type': 'all_complete', 'message': 'ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ'})}\n\n"
                
                yield "data: [DONE]\n\n"
                
            except Exception as e:
                yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
                yield "data: [DONE]\n\n"
        
        return Response(
            stream_with_context(generate()),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Headers': 'Content-Type',
                'X-Accel-Buffering': 'no'  # Disable nginx buffering
            }
        )
    
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@chat_bp.route('/chatgpt-basic', methods=['POST'])
def chatgpt_basic():
    """ChatGPT + s3ê¸°ë³¸ ê°œë³„ ì²˜ë¦¬"""
    try:
        data = request.get_json()
        question = data.get('question')
        
        if not question:
            return jsonify({"error": "Question is required"}), 400
        
        chain = get_rag_chain()
        
        # ê¸°ë³¸ ê²€ìƒ‰ ìˆ˜í–‰
        import time
        start_time = time.time()
        
        try:
            if hasattr(chain, 'dual_vectorstore_manager') and chain.dual_vectorstore_manager:
                search_results = chain.dual_vectorstore_manager.similarity_search_with_score(question, "basic", k=5)
            else:
                search_results = chain.vectorstore_manager.similarity_search_with_score(question, k=5)
            
            # s3 í´ë” ë¬¸ì„œë§Œ í•„í„°ë§ (s3-chunking ì œì™¸)
            s3_results = [(doc, score) for doc, score in search_results if doc.metadata.get('source') == 's3']
            
            if not s3_results:
                return jsonify({
                    'success': False,
                    'answer': 's3 í´ë”ì˜ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. s3 í´ë”ì—ì„œ ë¬¸ì„œë¥¼ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.',
                    'similarity_info': [],
                    'total_time': 0.0,
                    'process_name': 'ChatGPT + s3ê¸°ë³¸',
                    'chunking_type': 'basic'
                })
            
            # ì²« ë²ˆì§¸ ê²°ê³¼ë¡œ ìµœì í™”ëœ ë‹µë³€ ìƒì„± (chunking optimization)
            if s3_results:
                doc, score = s3_results[0]
                # Optimize context length for better LLM performance
                max_context_length = 2500  # Optimal length for single document processing
                if len(doc.page_content) > max_context_length:
                    context = doc.page_content[:max_context_length-3] + "..."
                    print(f"[CHUNKING OPT] ChatGPT-basic context truncated from {len(doc.page_content)} to {len(context)} chars")
                else:
                    context = doc.page_content
                prompt = chain.prompt_template.format(context=context, question=question)
                
                # OpenAI API í˜¸ì¶œ
                from openai import OpenAI
                import os
                client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), timeout=30.0)
                stream = client.chat.completions.create(
                    model='gpt-4o-mini',
                    messages=[{"role": "user", "content": prompt}],
                    stream=True,
                    temperature=0.1
                )
                
                answer_text = ""
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        answer_text += chunk.choices[0].delta.content
                
                # ìœ ì‚¬ë„ ì •ë³´ ìƒì„± (s3 ê²°ê³¼ë§Œ ì‚¬ìš©)
                similarity_info = []
                for i, (doc, score) in enumerate(s3_results[:3], 1):
                    similarity_info.append({
                        'rank': i,
                        'score': f'{score:.1%}',
                        'source': doc.metadata.get('source', 's3'),
                        'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content
                    })
                
                end_time = time.time()
                total_time = end_time - start_time
                
                return jsonify({
                    'success': True,
                    'answer': answer_text,
                    'similarity_info': similarity_info,
                    'total_time': round(total_time, 2),
                    'process_name': 'ChatGPT + s3ê¸°ë³¸',
                    'chunking_type': 'basic'
                })
            else:
                return jsonify({
                    'success': False,
                    'answer': 'ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                    'similarity_info': [],
                    'total_time': 0.0,
                    'process_name': 'ChatGPT + s3ê¸°ë³¸',
                    'chunking_type': 'basic'
                })
                
        except Exception as e:
            end_time = time.time()
            total_time = end_time - start_time
            return jsonify({
                'success': False,
                'answer': f'ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
                'similarity_info': [],
                'total_time': round(total_time, 2),
                'process_name': 'ChatGPT + s3ê¸°ë³¸',
                'chunking_type': 'basic'
            })
            
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@chat_bp.route('/chatgpt-custom', methods=['POST'])
def chatgpt_custom():
    """ChatGPT + s3-chunking ê°œë³„ ì²˜ë¦¬"""
    try:
        data = request.get_json()
        question = data.get('question')
        
        if not question:
            return jsonify({"error": "Question is required"}), 400
        
        chain = get_rag_chain()
        
        # ì»¤ìŠ¤í…€ ê²€ìƒ‰ ìˆ˜í–‰
        import time
        start_time = time.time()
        
        try:
            if hasattr(chain, 'dual_vectorstore_manager') and chain.dual_vectorstore_manager:
                search_results = chain.dual_vectorstore_manager.similarity_search_with_score(question, "custom", k=3)
            else:
                search_results = chain.vectorstore_manager.similarity_search_with_score(question, k=3)
            
            # ì²« ë²ˆì§¸ ê²°ê³¼ë¡œ ìµœì í™”ëœ ë‹µë³€ ìƒì„± (chunking optimization)
            if search_results:
                doc, score = search_results[0]
                # Optimize context length for better LLM performance  
                max_context_length = 2500  # Optimal length for single document processing
                if len(doc.page_content) > max_context_length:
                    context = doc.page_content[:max_context_length-3] + "..."
                    print(f"[CHUNKING OPT] ChatGPT-custom context truncated from {len(doc.page_content)} to {len(context)} chars")
                else:
                    context = doc.page_content
                prompt = chain.prompt_template.format(context=context, question=question)
                
                # OpenAI API í˜¸ì¶œ
                from openai import OpenAI
                import os
                client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), timeout=30.0)
                stream = client.chat.completions.create(
                    model='gpt-4o-mini',
                    messages=[{"role": "user", "content": prompt}],
                    stream=True,
                    temperature=0.1
                )
                
                answer_text = ""
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        answer_text += chunk.choices[0].delta.content
                
                # ìœ ì‚¬ë„ ì •ë³´ ìƒì„±
                similarity_info = []
                for i, (doc, score) in enumerate(search_results[:3], 1):
                    similarity_info.append({
                        'rank': i,
                        'score': f'{score:.1%}',
                        'source': doc.metadata.get('source_file', 'Unknown'),
                        'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content
                    })
                
                end_time = time.time()
                total_time = end_time - start_time
                
                return jsonify({
                    'success': True,
                    'answer': answer_text,
                    'similarity_info': similarity_info,
                    'total_time': round(total_time, 2),
                    'process_name': 'ChatGPT + s3-chunking',
                    'chunking_type': 'custom'
                })
            else:
                return jsonify({
                    'success': False,
                    'answer': 'ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                    'similarity_info': [],
                    'total_time': 0.0,
                    'process_name': 'ChatGPT + s3-chunking',
                    'chunking_type': 'custom'
                })
                
        except Exception as e:
            end_time = time.time()
            total_time = end_time - start_time
            return jsonify({
                'success': False,
                'answer': f'ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
                'similarity_info': [],
                'total_time': round(total_time, 2),
                'process_name': 'ChatGPT + s3-chunking',
                'chunking_type': 'custom'
            })
            
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@chat_bp.route('/local-basic', methods=['POST'])
def local_basic():
    """ë¡œì»¬LLM + s3ê¸°ë³¸ ê°œë³„ ì²˜ë¦¬"""
    try:
        data = request.get_json()
        question = data.get('question')
        session_id = data.get('session_id', str(uuid.uuid4()))
        
        if not question:
            return jsonify({'error': 'ì§ˆë¬¸ì´ í•„ìš”í•©ë‹ˆë‹¤'}), 400
        
        def generate_response():
            try:
                start_time = time.time()
                
                # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰ (basic ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰ - s3 ë¬¸ì„œë§Œ)
                from models.dual_vectorstore import get_dual_vectorstore
                dual_vectorstore = get_dual_vectorstore()
                search_results = dual_vectorstore.similarity_search_with_score(question, "basic", k=5)
                
                if not search_results:
                    yield f"data: {json.dumps({'type': 'process_error', 'process_name': 'ë¡œì»¬LLM + s3ê¸°ë³¸', 'process_id': 3, 'session_id': session_id, 'error': 'no_documents', 'message': 'ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. s3 í´ë”ì—ì„œ ë¬¸ì„œë¥¼ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.', 'similarity_info': [], 'total_time': 0.0, 'status': 'failed'}, ensure_ascii=False)}\n\n"
                    return

                # basic ì»¬ë ‰ì…˜ì—ëŠ” ì´ë¯¸ s3 ë¬¸ì„œë§Œ ìˆìœ¼ë¯€ë¡œ í•„í„°ë§ ë¶ˆí•„ìš”
                s3_results = search_results
                
                if not s3_results:
                    yield f"data: {json.dumps({'type': 'process_error', 'process_name': 'ë¡œì»¬LLM + s3ê¸°ë³¸', 'process_id': 3, 'session_id': session_id, 'error': 'no_s3_documents', 'message': 's3 í´ë”ì˜ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. s3 í´ë”ì—ì„œ ë¬¸ì„œë¥¼ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.', 'similarity_info': [], 'total_time': 0.0, 'status': 'failed'}, ensure_ascii=False)}\n\n"
                    return

                # ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ë§¤ìš° ì§§ê²Œ, ë¡œì»¬ LLM í† í° ì œí•œ)
                s3_context = ""
                max_context_length = 200  # ë” ì§§ê²Œ ì œí•œ
                for doc, score in s3_results[:2]:  # 2ê°œë§Œ ì‚¬ìš©
                    content = doc.page_content[:max_context_length]
                    s3_context += f"{content}\n\n"
                    if len(s3_context) > 400:  # ë§¤ìš° ì§§ì€ ì „ì²´ ê¸¸ì´
                        break

                # ë¡œì»¬ LLM í˜¸ì¶œ (ë§¤ìš° ì§§ì€ í”„ë¡¬í”„íŠ¸)
                from models.llm import LLMManager
                llm_manager = LLMManager()
                local_llm = llm_manager.get_llm(model_name='local')
                
                prompt = f"""ë‹¤ìŒ ì •ë³´ë¡œ ë‹µë³€: {s3_context[:300]}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

                try:
                    response = local_llm.invoke(prompt)
                    answer = str(response) if response else 'ë¡œì»¬ LLM ì‘ë‹µ ì—†ìŒ'
                except Exception as e:
                    # ë¡œì»¬ LLM ì—ëŸ¬ ì‹œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°„ë‹¨ ë‹µë³€
                    answer = f"ê²€ìƒ‰ëœ ì •ë³´: {s3_context[:200]}... (ë¡œì»¬ LLM ì„œë²„ ì—ëŸ¬: {str(e)[:50]})"
                
                end_time = time.time()
                total_time = end_time - start_time
                
                # ìœ ì‚¬ë„ ì •ë³´ ìƒì„± (s3 ê²°ê³¼ë§Œ ì‚¬ìš©)
                similarity_info = []
                for i, (doc, score) in enumerate(s3_results[:3], 1):
                    similarity_info.append({
                        'rank': i,
                        'score': f'{score:.1%}',
                        'source': doc.metadata.get('source', 's3'),
                        'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content
                    })
                
                yield f"data: {json.dumps({'type': 'process_complete', 'process_name': 'ë¡œì»¬LLM + s3ê¸°ë³¸', 'process_id': 3, 'session_id': session_id, 'answer': answer, 'similarity_info': similarity_info, 'total_time': total_time, 'status': 'success', 'chunking_type': 'basic'}, ensure_ascii=False)}\n\n"
                
            except Exception as e:
                print(f"ë¡œì»¬LLM ê¸°ë³¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                yield f"data: {json.dumps({'type': 'process_error', 'process_name': 'ë¡œì»¬LLM + s3ê¸°ë³¸', 'process_id': 3, 'session_id': session_id, 'error': 'connection_error', 'message': f'ë¡œì»¬ LLM ì—°ê²° ì˜¤ë¥˜: {str(e)}. ë¡œì»¬ LLM ì„œë²„ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”.', 'similarity_info': [], 'total_time': 0.0, 'status': 'failed'}, ensure_ascii=False)}\n\n"

        return Response(
            generate_response(),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'X-Accel-Buffering': 'no'
            }
        )
        
    except Exception as e:
        return jsonify({
            'error': f'ë¡œì»¬LLM + s3ê¸°ë³¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}',
            'status': 'failed'
        }), 500

@chat_bp.route('/local-custom', methods=['POST'])
def local_custom():
    """ë¡œì»¬LLM + s3-chunking ê°œë³„ ì²˜ë¦¬"""
    try:
        data = request.get_json()
        question = data.get('question')
        session_id = data.get('session_id', str(uuid.uuid4()))
        
        if not question:
            return jsonify({'error': 'ì§ˆë¬¸ì´ í•„ìš”í•©ë‹ˆë‹¤'}), 400
        
        def generate_response():
            try:
                start_time = time.time()
                
                # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
                vectorstore_manager = VectorStoreManager()
                search_results = vectorstore_manager.similarity_search(question, k=5)
                
                if not search_results:
                    yield f"data: {json.dumps({'type': 'process_error', 'process_name': 'ë¡œì»¬LLM + s3-chunking', 'process_id': 4, 'session_id': session_id, 'error': 'no_documents', 'message': 'ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. s3-chunking í´ë”ì—ì„œ ë¬¸ì„œë¥¼ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.', 'similarity_info': [], 'total_time': 0.0, 'status': 'failed'}, ensure_ascii=False)}\n\n"
                    return

                # s3-chunking ë¬¸ì„œ í•„í„°ë§
                s3_chunking_results = [(doc, score) for doc, score in search_results if doc.metadata.get('source') == 's3-chunking']
                
                if not s3_chunking_results:
                    yield f"data: {json.dumps({'type': 'process_error', 'process_name': 'ë¡œì»¬LLM + s3-chunking', 'process_id': 4, 'session_id': session_id, 'error': 'no_chunking_documents', 'message': 's3-chunking í´ë”ì˜ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.', 'similarity_info': [], 'total_time': 0.0, 'status': 'failed'}, ensure_ascii=False)}\n\n"
                    return

                # ì»¨í…ìŠ¤íŠ¸ ìƒì„± (s3-chunking ë¬¸ì„œë§Œ ì‚¬ìš©, ê¸¸ì´ ì œí•œ)
                context = ""
                max_context_length = 500  # ë¡œì»¬ LLMì˜ í† í° ì œí•œ ê³ ë ¤
                for doc, score in s3_chunking_results[:3]:
                    content = doc.page_content
                    if len(content) > max_context_length:
                        content = content[:max_context_length] + "..."
                    context += f"[ìœ ì‚¬ë„: {score:.1%}] {content}\n\n"
                    if len(context) > 1000:  # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
                        break

                # ë¡œì»¬ LLM í˜¸ì¶œ (LocalLLM í´ë˜ìŠ¤ ì‚¬ìš©)
                from models.llm import LLMManager
                llm_manager = LLMManager()
                local_llm = llm_manager.get_llm(model_name='local')
                
                prompt = f"""ë‹¤ìŒ ì •ë³´ë¡œ ë‹µë³€: {context[:300]}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

                try:
                    response = local_llm.invoke(prompt)
                    answer = str(response) if response else 'ë¡œì»¬ LLM ì‘ë‹µ ì—†ìŒ'
                except Exception as e:
                    # ë¡œì»¬ LLM ì—ëŸ¬ ì‹œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°„ë‹¨ ë‹µë³€
                    answer = f"ê²€ìƒ‰ëœ ì •ë³´: {context[:200]}... (ë¡œì»¬ LLM ì„œë²„ ì—ëŸ¬: {str(e)[:50]})"
                
                end_time = time.time()
                total_time = end_time - start_time
                
                # ìœ ì‚¬ë„ ì •ë³´ ìƒì„±
                similarity_info = []
                for i, (doc, score) in enumerate(s3_chunking_results[:3], 1):
                    similarity_info.append({
                        'rank': i,
                        'score': f'{score:.1%}',
                        'source': doc.metadata.get('source', 's3-chunking'),
                        'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content
                    })
                
                yield f"data: {json.dumps({'type': 'process_complete', 'process_name': 'ë¡œì»¬LLM + s3-chunking', 'process_id': 4, 'session_id': session_id, 'answer': answer, 'similarity_info': similarity_info, 'total_time': total_time, 'status': 'success', 'chunking_type': 'custom'}, ensure_ascii=False)}\n\n"
                
            except Exception as e:
                print(f"ë¡œì»¬LLM ì»¤ìŠ¤í…€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                yield f"data: {json.dumps({'type': 'process_error', 'process_name': 'ë¡œì»¬LLM + s3-chunking', 'process_id': 4, 'session_id': session_id, 'error': 'connection_error', 'message': f'ë¡œì»¬ LLM ì—°ê²° ì˜¤ë¥˜: {str(e)}. ë¡œì»¬ LLM ì„œë²„ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”.', 'similarity_info': [], 'total_time': 0.0, 'status': 'failed'}, ensure_ascii=False)}\n\n"

        return Response(
            generate_response(),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'X-Accel-Buffering': 'no'
            }
        )
        
    except Exception as e:
        return jsonify({
            'error': f'ë¡œì»¬LLM + s3-chunking ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}',
            'status': 'failed'
        }), 500

@chat_bp.route('/vllm-dual-stream', methods=['POST'])
def vllm_dual_stream():
    """vLLM ë‘ ì˜µì…˜ ë™ì‹œ ìŠ¤íŠ¸ë¦¬ë° (ì‚¬ë‚´ì„œë²„ vLLM + s3ê¸°ë³¸, ì‚¬ë‚´ì„œë²„ vLLM + s3-chunking)"""
    try:
        print(f"ğŸŒ [REQUEST] vLLM ë“€ì–¼ ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ ë°›ìŒ")
        print(f"ğŸ“‹ [REQUEST] Content-Type: {request.content_type}")
        print(f"ğŸ“‹ [REQUEST] Method: {request.method}")
        
        data = request.get_json()
        print(f"ğŸ“Š [REQUEST] ë°›ì€ ë°ì´í„°: {data}")
        
        if not data:
            print("âŒ [REQUEST] JSON ë°ì´í„°ê°€ ì—†ìŒ")
            return jsonify({"error": "JSON data is required"}), 400
        
        question = data.get('question') or data.get('query')
        selected_model = data.get('local_model', 'kanana8b')  # í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì„ íƒí•œ ëª¨ë¸
        print(f"â“ [REQUEST] ì§ˆë¬¸: {question}")
        print(f"ğŸ¤– [REQUEST] ì„ íƒëœ ëª¨ë¸: {selected_model}")
        
        if not question:
            print("âŒ [REQUEST] ì§ˆë¬¸ì´ ì—†ìŒ")
            return jsonify({"error": "Question or query is required"}), 400
        
        def generate():
            import time
            import uuid
            from concurrent.futures import ThreadPoolExecutor, as_completed
            from queue import Queue, Empty
            
            # ì„¸ì…˜ ID ìƒì„±
            session_id = str(uuid.uuid4())
            
            # 2ê°œ vLLM í”„ë¡œì„¸ìŠ¤ ì •ì˜
            processes = [
                {"llm_type": "local", "chunking": "basic", "name": "ì‚¬ë‚´ì„œë²„ vLLM + s3ê¸°ë³¸", "process_id": 3},
                {"llm_type": "local", "chunking": "custom", "name": "ì‚¬ë‚´ì„œë²„ vLLM + s3-chunking", "process_id": 4}
            ]
            
            result_queue = Queue()
            
            def vllm_process_task(process_info, question, session_id, selected_model):
                """vLLM ì²˜ë¦¬ íƒœìŠ¤í¬ - ë¡œê¹… ì¶”ê°€"""
                start_time = time.time()
                process_name = process_info["name"]
                process_id = process_info["process_id"]
                chunking_type = process_info["chunking"]
                
                print(f"ğŸš€ [vLLM {process_id}] {process_name} ì‹œì‘")
                print(f"ğŸ“Š [vLLM {process_id}] ì§ˆë¬¸: {question[:100]}...")
                print(f"ğŸ”§ [vLLM {process_id}] ì²­í‚¹ íƒ€ì…: {chunking_type}")
                
                try:
                    # í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì•Œë¦¼
                    result_queue.put({
                        'type': 'process_start',
                        'process_name': process_name,
                        'process_id': process_id,
                        'session_id': session_id
                    })
                    
                    # vLLM ì—°ê²° í™•ì¸
                    from models.llm import LLMManager
                    print(f"ğŸ”Œ [vLLM {process_id}] LLM ë§¤ë‹ˆì € ìƒì„± ì¤‘...")
                    llm_manager = LLMManager()
                    
                    print(f"ğŸ¤– [vLLM {process_id}] vLLM ì—°ê²° ì‹œë„: 192.168.0.224:8412")
                    vllm_llm = llm_manager.get_vllm_llm()
                    print(f"âœ… [vLLM {process_id}] vLLM ì—°ê²° ì„±ê³µ")
                    
                    # ë¬¸ì„œ ê²€ìƒ‰
                    print(f"ğŸ” [vLLM {process_id}] ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘...")
                    chain = get_rag_chain()
                    
                    if chunking_type == "basic":
                        # s3ê¸°ë³¸: DualVectorStoreì˜ basic ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰
                        if hasattr(chain, 'dual_vectorstore_manager') and chain.dual_vectorstore_manager:
                            print(f"ğŸ“š [vLLM {process_id}] basic ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰")
                            search_results = chain.dual_vectorstore_manager.similarity_search_with_score(question, "basic", k=5)
                        else:
                            print(f"âš ï¸ [vLLM {process_id}] í´ë°±: ê¸°ë³¸ ë²¡í„°ìŠ¤í† ì–´ ì‚¬ìš©")
                            search_results = chain.vectorstore_manager.similarity_search_with_score(question, k=5)
                    else:
                        # s3-chunking: DualVectorStoreì˜ custom ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰
                        if hasattr(chain, 'dual_vectorstore_manager') and chain.dual_vectorstore_manager:
                            print(f"ğŸ“š [vLLM {process_id}] custom ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰")
                            search_results = chain.dual_vectorstore_manager.similarity_search_with_score(question, "custom", k=5)
                        else:
                            print(f"âš ï¸ [vLLM {process_id}] í´ë°±: ê¸°ë³¸ ë²¡í„°ìŠ¤í† ì–´ ì‚¬ìš©")
                            search_results = chain.vectorstore_manager.similarity_search_with_score(question, k=5)
                    
                    print(f"ğŸ“Š [vLLM {process_id}] ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ")
                    if search_results:
                        print(f"ğŸ¯ [vLLM {process_id}] ìµœê³  ìœ ì‚¬ë„: {search_results[0][1]:.2%}")
                    
                    if not search_results:
                        result_queue.put({
                            'type': 'process_error',
                            'process_name': process_name,
                            'process_id': process_id,
                            'session_id': session_id,
                            'error': 'no_results',
                            'message': 'ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.',
                            'similarity_info': [],
                            'total_time': round(time.time() - start_time, 2),
                            'status': 'failed'
                        })
                        return
                    
                    # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (vLLM í† í° ì œí•œ ê³ ë ¤)
                    context = ""
                    max_doc_length = 800  # vLLMìš© ìµœì í™”ëœ ê¸¸ì´
                    for doc, score in search_results[:2]:  # 2ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©
                        doc_content = doc.page_content
                        if len(doc_content) > max_doc_length:
                            doc_content = doc_content[:max_doc_length] + "..."
                        context += f"{doc_content}\n\n"
                        if len(context) > 1200:  # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ì œí•œ
                            break
                    
                    print(f"ğŸ“ [vLLM {process_id}] ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)}ì")
                    
                    # vLLM í˜¸ì¶œì„ ìœ„í•œ ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸
                    prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:

{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
                    
                    print(f"ğŸ¤– [vLLM {process_id}] vLLM í˜¸ì¶œ ì‹œì‘...")
                    print(f"ğŸŒ [vLLM {process_id}] ì„œë²„: 192.168.0.224:8412")
                    print(f"ğŸ”§ [vLLM {process_id}] ëª¨ë¸: {selected_model}")
                    
                    # ì„ íƒëœ ëª¨ë¸ë¡œ vLLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                    from langchain_openai import ChatOpenAI
                    config = Config.LLM_MODELS['local']
                    custom_vllm = ChatOpenAI(
                        model=selected_model,  # ì„ íƒëœ ëª¨ë¸ ì‚¬ìš©
                        openai_api_base=config['base_url'] + '/v1',
                        openai_api_key='EMPTY',
                        temperature=config['temperature'],
                        max_tokens=config['max_tokens']
                    )
                    
                    # ì»¤ìŠ¤í…€ vLLM í˜¸ì¶œ
                    response = custom_vllm.invoke(prompt)
                    print(f"ğŸ“¨ [vLLM {process_id}] vLLM ì‘ë‹µ ë°›ìŒ")
                    
                    # ì‘ë‹µ ì²˜ë¦¬
                    if hasattr(response, 'content'):
                        answer = response.content
                    elif isinstance(response, str):
                        answer = response
                    else:
                        answer = str(response)
                    
                    print(f"âœ… [vLLM {process_id}] ë‹µë³€ ìƒì„± ì™„ë£Œ: {len(answer)}ì")
                    
                    # ìœ ì‚¬ë„ ì •ë³´ ìƒì„±
                    similarity_info = []
                    for i, (doc, score) in enumerate(search_results[:3], 1):
                        similarity_info.append({
                            'rank': i,
                            'score': f'{score:.1%}',
                            'source': doc.metadata.get('source_file', doc.metadata.get('source', 'Unknown')),
                            'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content
                        })
                    
                    end_time = time.time()
                    total_time = end_time - start_time
                    print(f"â±ï¸ [vLLM {process_id}] ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
                    
                    result_queue.put({
                        'type': 'process_complete',
                        'process_name': process_name,
                        'process_id': process_id,
                        'session_id': session_id,
                        'answer': answer,
                        'similarity_info': similarity_info,
                        'total_time': round(total_time, 2),
                        'status': 'success',
                        'chunking_type': chunking_type
                    })
                    
                    print(f"ğŸ‰ [vLLM {process_id}] í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
                    
                except Exception as e:
                    end_time = time.time()
                    total_time = end_time - start_time
                    error_msg = str(e)
                    print(f"âŒ [vLLM {process_id}] ì˜¤ë¥˜ ë°œìƒ: {error_msg}")
                    import traceback
                    print(f"ğŸ” [vLLM {process_id}] ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
                    
                    # vLLM íŠ¹í™” ì—ëŸ¬ ë©”ì‹œì§€
                    if "connection" in error_msg.lower() or "connect" in error_msg.lower():
                        user_message = f"vLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨ (192.168.0.224:8412). ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
                    elif "timeout" in error_msg.lower():
                        user_message = f"vLLM ì‘ë‹µ ì‹œê°„ ì´ˆê³¼. ì„œë²„ ë¶€í•˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
                    else:
                        user_message = f"vLLM ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {error_msg[:100]}"
                    
                    result_queue.put({
                        'type': 'process_error',
                        'process_name': process_name,
                        'process_id': process_id,
                        'session_id': session_id,
                        'error': 'vllm_error',
                        'message': user_message,
                        'similarity_info': [],
                        'total_time': round(total_time, 2),
                        'status': 'failed'
                    })
            
            # ì‹œì‘ ì•Œë¦¼
            yield f"data: {json.dumps({'type': 'all_processes_start', 'total_processes': 2, 'session_id': session_id}, ensure_ascii=False)}\n\n"
            print(f"ğŸš€ [MAIN] vLLM ë“€ì–¼ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ - ì„¸ì…˜: {session_id}")
            
            # 2ê°œ í”„ë¡œì„¸ìŠ¤ ë³‘ë ¬ ì‹¤í–‰
            with ThreadPoolExecutor(max_workers=2) as executor:
                futures = []
                for process_info in processes:
                    future = executor.submit(vllm_process_task, process_info, question, session_id, selected_model)
                    futures.append(future)
                    print(f"ğŸ”„ [MAIN] {process_info['name']} ìŠ¤ë ˆë“œ ì‹œì‘")
                
                # ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°
                completed_processes = 0
                total_processes = 2
                
                while completed_processes < total_processes:
                    try:
                        result = result_queue.get(timeout=0.1)
                        yield f"data: {json.dumps(result, ensure_ascii=False)}\n\n"
                        
                        if result['type'] in ['process_complete', 'process_error']:
                            completed_processes += 1
                            print(f"âœ… [MAIN] í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ: {result.get('process_name', 'Unknown')} ({completed_processes}/{total_processes})")
                            
                    except Empty:
                        time.sleep(0.01)
                        continue
                
                # futures ì •ë¦¬
                for future in as_completed(futures, timeout=60):
                    try:
                        future.result()
                    except Exception as e:
                        print(f"âŒ [MAIN] ìŠ¤ë ˆë“œ ì™„ë£Œ ì˜¤ë¥˜: {e}")
            
            yield f"data: {json.dumps({'type': 'all_complete', 'message': 'vLLM ë“€ì–¼ ì²˜ë¦¬ ì™„ë£Œ'}, ensure_ascii=False)}\n\n"
            yield "data: [DONE]\n\n"
            print(f"ğŸ [MAIN] ëª¨ë“  vLLM í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ")
        
        return Response(
            stream_with_context(generate()),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Headers': 'Content-Type',
                'X-Accel-Buffering': 'no'
            }
        )
    
    except Exception as e:
        print(f"âŒ [MAIN] vLLM ë“€ì–¼ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
        return jsonify({"error": str(e)}), 500

@chat_bp.route('/clear-memory', methods=['POST'])
def clear_memory():
    """Clear conversation memory"""
    try:
        chain = get_rag_chain()
        chain.clear_memory()
        return jsonify({"message": "Memory cleared successfully"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500