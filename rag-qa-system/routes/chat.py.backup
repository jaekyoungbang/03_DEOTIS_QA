from flask import Blueprint, request, jsonify, stream_with_context, Response
from services.rag_chain import RAGChain
from models.vectorstore import VectorStoreManager
from config import Config
from services.card_manager import process_user_card_query
import json
import time
import uuid
import os
import re

chat_bp = Blueprint('chat', __name__)

def extract_images_from_context(context, answer):
    """RAG 컨텍스트와 답변에서 이미지 경로 추출 (강화된 카드 이미지 인식)"""
    images = []
    
    # MD 파일에서 이미지 패턴 찾기: ![alt](image.gif) 또는 ![alt](image.png)
    image_pattern = r'!\[([^\]]*)\]\(([^)]+\.(?:gif|png|jpg|jpeg))\)'
    
    # 컨텍스트에서 이미지 검색
    context_matches = re.findall(image_pattern, context, re.IGNORECASE)
    for alt_text, image_path in context_matches:
        # 카드 이미지 우선 처리
        priority = 1
        if any(card_name in alt_text.lower() for card_name in ['카드', '로고', '발급']):
            priority = 5  # 카드 이미지 고우선순위
        elif '절차' in alt_text:
            priority = 4  # 절차 이미지
        
        images.append({
            'alt': alt_text,
            'path': image_path,
            'url': f'/images/{image_path}',  # 웹 서빙 URL
            'source': 'context',
            'priority': priority
        })
    
    # 답변에서 이미진 검색
    answer_matches = re.findall(image_pattern, answer, re.IGNORECASE)
    for alt_text, image_path in answer_matches:
        # 답변의 이미지를 최고 우선순위로
        images.append({
            'alt': alt_text,
            'path': image_path,
            'url': f'/images/{image_path}',
            'source': 'answer',
            'priority': 10
        })
    
    # 중복 제거 및 우선순위 정렬
    unique_images = []
    seen_paths = set()
    
    # 우선순위순 정렬
    images.sort(key=lambda x: x.get('priority', 1), reverse=True)
    
    for img in images:
        if img['path'] not in seen_paths:
            seen_paths.add(img['path'])
            unique_images.append(img)
    
    print(f"🖼️ 추출된 이미지: {len(unique_images)}개 - {[img['path'] for img in unique_images]}")
    return unique_images

# Initialize RAG chain (consider using app context or dependency injection in production)
rag_chain = None

def get_rag_chain():
    global rag_chain
    if rag_chain is None:
        rag_chain = RAGChain()
    return rag_chain

@chat_bp.route('/query', methods=['POST'])
@chat_bp.route('/../rag/chat', methods=['POST'])
def query():
    """Handle chat queries"""
    try:
        data = request.get_json()
        question = data.get('question')
        use_memory = data.get('use_memory', False)
        llm_model = data.get('llm_model', 'gpt-4o-mini')
        search_mode = data.get('search_mode', 'basic')
        
        if not question:
            return jsonify({"error": "Question is required"}), 400
        
        # 카드 분석 질문 감지
        import re
        card_analysis_pattern = r'([가-힣]{2,4})\s*(?:회원|고객|님|씨)?\s*(?:카드|발급)'
        match = re.search(card_analysis_pattern, question)
        
        if match:
            try:
                customer_name = match.group(1)
                from services.card_analysis_service import CardAnalysisService
                card_service = CardAnalysisService()
                analysis = card_service.analyze_customer_cards(customer_name)
                formatted_response = card_service.format_analysis_response(analysis)
                
                return jsonify({
                    "answer": formatted_response,
                    "sources": [f"{customer_name}_회원은행별_카드발급안내.md"],
                    "query": question,
                    "search_mode": "card_analysis",
                    "llm_model": llm_model,
                    "processing_time": 0.5,
                    "source_documents": [],
                    "card_analysis": {
                        "customer_name": customer_name,
                        "owned_count": len(analysis.owned_cards),
                        "recommended_count": len(analysis.recommended_cards),
                        "available_count": len(analysis.available_cards),
                        "total_options": analysis.total_summary['총옵션']
                    }
                })
            except Exception as card_error:
                print(f"카드 분석 오류: {card_error}")
                # 일반 RAG로 fallback
        
        # Get RAG chain instance
        chain = get_rag_chain()
        
        # Query the RAG system with search mode
        response = chain.query(
            question, 
            use_memory=use_memory, 
            llm_model=llm_model,
            search_mode=search_mode
        )
        
        return jsonify(response)
    
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@chat_bp.route('/stream', methods=['POST'])
def stream_query():
    """Handle streaming chat queries with real-time progress"""
    try:
        data = request.get_json()
        question = data.get('question')
        use_memory = data.get('use_memory', False)
        llm_model = data.get('llm_model', 'gpt-4o-mini')
        search_mode = data.get('search_mode', 'basic')
        summarize = data.get('summarize', False)
        
        if not question:
            return jsonify({"error": "Question is required"}), 400
        
        def generate():
            import time
            
            try:
                chain = get_rag_chain()
                
                # Step 4: 4개 별도 프로세스로 비동기 처리
                from models.llm import LLMManager
                from openai import OpenAI
                import os
                import platform
                from queue import Queue, Empty
                from concurrent.futures import ThreadPoolExecutor, as_completed
                import uuid
                
                # 프로세스별 폴더 경로 설정
                if platform.system() == "Windows":
                    s3_folder = "D:\\99_DEOTIS_QA_SYSTEM\\03_DEOTIS_QA\\rag-qa-system\\s3"
                    s3_chunking_folder = "D:\\99_DEOTIS_QA_SYSTEM\\03_DEOTIS_QA\\rag-qa-system\\s3-chunking"
                else:
                    s3_folder = "/mnt/d/99_DEOTIS_QA_SYSTEM/03_DEOTIS_QA/rag-qa-system/s3"
                    s3_chunking_folder = "/mnt/d/99_DEOTIS_QA_SYSTEM/03_DEOTIS_QA/rag-qa-system/s3-chunking"
                
                # 4개 프로세스 정의
                processes = [
                    {"llm_type": "openai", "chunking": "basic", "folder": s3_folder, "name": "ChatGPT + s3기본", "process_id": 1},
                    {"llm_type": "openai", "chunking": "custom", "folder": s3_chunking_folder, "name": "ChatGPT + s3-chunking", "process_id": 2},
                    {"llm_type": "local", "chunking": "basic", "folder": s3_folder, "name": "로컬LLM + s3기본", "process_id": 3},
                    {"llm_type": "local", "chunking": "custom", "folder": s3_chunking_folder, "name": "로컬LLM + s3-chunking", "process_id": 4}
                ]
                
                # Create a queue for streaming results from multiple processes
                result_queue = Queue()
                
                def process_single_search_task(process_info, question, llm_model, chain, session_id, summarize=False):
                    """단일 검색 프로세스 처리 - 완전 독립적 실행"""
                    start_time = time.time()  # 시작 시간 기록
                    try:
                        process_name = process_info["name"]
                        chunking_type = process_info["chunking"]
                        llm_type = process_info["llm_type"]
                        process_id = process_info["process_id"]
                        
                        print(f"[PROCESS] Starting {process_name} (ID: {process_id})")
                        
                        # 프로세스 시작 신호
                        result_queue.put({
                            'type': 'process_start',
                            'process_name': process_name,
                            'process_id': process_id,
                            'session_id': session_id
                        })
                        
                        # 로컬 LLM 사용 불가능 시 오류 처리
                        if llm_type == "local":
                            try:
                                # 로컬 LLM 연결 테스트
                                from models.llm import LLMManager
                                llm_manager = LLMManager()
                                llm_manager.get_llm(model_name=llm_model)
                            except Exception as e:
                                result_queue.put({
                                    'type': 'process_error',
                                    'process_name': process_name,
                                    'process_id': process_id,
                                    'session_id': session_id,
                                    'error': 'vllm_not_running',
                                    'message': '사내서버 vLLM이 실행되지 않았습니다. 해결방법: 1. vLLM 서버 상태 확인: 192.168.0.224:8412 2. kanana8b 모델 로딩 확인 3. 서버 재시작 필요시 연락',
                                    'similarity_info': [],  # 오류 시 빈 유사도 정보
                                    'total_time': 0.0,
                                    'status': 'failed'
                                })
                                return
                        
                        # 해당 청킹 타입으로 검색 수행
                        print(f"[DEBUG] {process_name} 검색 시작 - 청킹타입: {chunking_type}")
                        print(f"[DEBUG] Chain object: {chain}")
                        print(f"[DEBUG] Has dual_vectorstore_manager: {hasattr(chain, 'dual_vectorstore_manager')}")
                        
                        if hasattr(chain, 'dual_vectorstore_manager') and chain.dual_vectorstore_manager:
                            if chunking_type == "custom":
                                print(f"[DEBUG] custom 벡터스토어에서 검색")
                                search_results = chain.dual_vectorstore_manager.similarity_search_with_score(question, "custom", k=3)
                            else:
                                print(f"[DEBUG] basic 벡터스토어에서 검색")
                                search_results = chain.dual_vectorstore_manager.similarity_search_with_score(question, "basic", k=3)
                        else:
                            # 폴백: 기본 벡터스토어 사용
                            print(f"[DEBUG] 폴백: 기본 벡터스토어 사용")
                            search_results = chain.vectorstore_manager.similarity_search_with_score(question, k=3)
                        
                        print(f"[DEBUG] {process_name} 검색 결과: {len(search_results)}개, 첫번째 점수: {search_results[0][1]:.2%}" if search_results else "검색 결과 없음")
                        
                        if not search_results:
                            print(f"[WARNING] {process_name} - No search results found")
                            result_queue.put({
                                'type': 'process_error',
                                'process_name': process_name,
                                'process_id': process_id,
                                'session_id': session_id,
                                'error': 'no_results',
                                'message': '검색 결과가 없습니다.',
                                'total_time': round(time.time() - start_time, 2),
                                'status': 'failed',
                                'similarity_info': []
                            })
                            return
                        
                        # 각 순위별로 개별 답변 생성
                        rank_answers = []
                        
                        # 검색된 문서들을 유사도별로 처리 (Top 3)
                        for rank, (doc, score) in enumerate(search_results[:3], 1):
                            try:
                                # 개별 문서에 대한 최적화된 프롬프트 생성 (chunking optimization)
                                # Truncate overly long documents to reduce LLM input length
                                max_doc_length = 2000  # Optimal length per document
                                if len(doc.page_content) > max_doc_length:
                                    individual_context = doc.page_content[:max_doc_length-3] + "..."
                                    print(f"[CHUNKING OPT] Document truncated from {len(doc.page_content)} to {len(individual_context)} chars")
                                else:
                                    individual_context = doc.page_content
                                # 요약 모드에 따른 프롬프트 조정
                                if summarize:
                                    individual_prompt = f"""주어진 문서를 기반으로 질문에 대한 답변을 간결하게 요약하여 제공하세요.
핵심 내용만 포함하고 불필요한 세부사항은 제외하세요.

문서 내용:
{individual_context}

질문: {question}

답변:"""
                                else:
                                    individual_prompt = chain.prompt_template.format(context=individual_context, question=question)
                                
                                answer_text = ""
                                
                                if llm_type == "openai":
                                    # OpenAI API 호출 (타임아웃 30초)
                                    client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), timeout=30.0)
                                    stream = client.chat.completions.create(
                                        model=llm_model,
                                        messages=[{"role": "user", "content": individual_prompt}],
                                        stream=True,
                                        temperature=0.1
                                    )
                                    
                                    # 전체 답변을 수집
                                    for chunk in stream:
                                        if chunk.choices[0].delta.content is not None:
                                            answer_text += chunk.choices[0].delta.content
                                
                                else:
                                    # 로컬 LLM 처리
                                    llm_manager = LLMManager()
                                    llm = llm_manager.get_llm(model_name='local')  # 로컬 LLM 사용
                                    full_response = llm.invoke(individual_prompt)
                                    
                                    # vLLM 응답에서 content 추출
                                    if hasattr(full_response, 'content'):
                                        answer_text = full_response.content
                                    elif isinstance(full_response, str):
                                        answer_text = full_response
                                    else:
                                        answer_text = str(full_response)
                                
                                # 완성된 답변을 저장
                                rank_answers.append({
                                    'rank': rank,
                                    'score': f'{score:.1%}',
                                    'answer': answer_text,
                                    'source': doc.metadata.get('source_file', 'Unknown')
                                })
                                
                            except Exception as doc_error:
                                rank_answers.append({
                                    'rank': rank,
                                    'score': f'{score:.1%}',
                                    'answer': f'오류 발생: {str(doc_error)}',
                                    'source': doc.metadata.get('source_file', 'Unknown')
                                })
                        
                        # 프로세스 완료 - 순위별 답변들과 시간 정보 전송
                        end_time = time.time()
                        total_time = end_time - start_time
                        
                        # 유사도 Top 3 정보 생성
                        similarity_info = []
                        for i, (doc, score) in enumerate(search_results[:3], 1):
                            similarity_info.append({
                                'rank': i,
                                'score': f'{score:.1%}',
                                'source': doc.metadata.get('source_file', 'Unknown'),
                                'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content
                            })
                        
                        print(f"[SUCCESS] Process {process_name} completed successfully")
                        print(f"[SUCCESS] Generated {len(rank_answers)} answers")
                        
                        result_queue.put({
                            'type': 'process_complete',
                            'process_name': process_name,
                            'process_id': process_id,
                            'session_id': session_id,
                            'rank_answers': rank_answers,
                            'similarity_info': similarity_info,
                            'total_time': round(total_time, 2),
                            'status': 'success',
                            'chunking_type': process_info["chunking"]  # chunking_type 추가
                        })
                        print(f"[SUCCESS] Complete event queued for process {process_id}")
                        
                    except Exception as e:
                        end_time = time.time()
                        total_time = end_time - start_time
                        print(f"[ERROR] Process {process_info['name']} failed: {str(e)}")
                        import traceback
                        print(f"[ERROR] Traceback: {traceback.format_exc()}")
                        
                        result_queue.put({
                            'type': 'process_error',
                            'process_name': process_info["name"],
                            'process_id': process_info["process_id"],
                            'session_id': session_id,
                            'error': str(e),
                            'message': f'프로세스 실행 중 오류: {str(e)}',
                            'total_time': round(total_time, 2),
                            'status': 'failed',
                            'similarity_info': []  # 오류 시 빈 유사도 정보
                        })
                        print(f"[ERROR] Error event queued for process {process_info['process_id']}")
                
                # 세션 ID 생성
                session_id = str(uuid.uuid4())
                
                # 4개 프로세스 시작 알림
                yield f"data: {json.dumps({'type': 'all_processes_start', 'total_processes': len(processes), 'session_id': session_id})}\n\n"
                
                # 4개 프로세스를 병렬로 시작
                with ThreadPoolExecutor(max_workers=4) as executor:
                    futures = []
                    
                    for process_info in processes:
                        print(f"[DEBUG] Starting process: {process_info['name']} (ID: {process_info['process_id']})")
                        future = executor.submit(
                            process_single_search_task,
                            process_info, question, llm_model, chain, session_id, summarize
                        )
                        futures.append(future)
                    
                    print(f"[DEBUG] All {len(futures)} processes submitted")
                    
                    # 결과를 실시간으로 스트리밍 (개별 프로세스 완료 즉시 표시)
                    completed_processes = 0
                    total_processes = len(processes)
                    
                    while completed_processes < total_processes:
                        try:
                            # 큐에서 결과 가져오기 (타임아웃을 짧게 설정하여 실시간성 향상)
                            result = result_queue.get(timeout=0.05)
                            
                            # 결과 즉시 스트리밍 (각 프로세스가 완료되는 순간 바로 화면에 표시)
                            yield f"data: {json.dumps(result)}\n\n"
                            
                            # 완료된 프로세스 카운트
                            if result['type'] in ['process_complete', 'process_error']:
                                completed_processes += 1
                                print(f"[STREAM] 프로세스 완료: {result.get('process_name', 'Unknown')} ({completed_processes}/{total_processes})")
                                
                        except Empty:
                            # 결과가 없으면 아주 짧은 시간 대기 후 계속 폴링
                            time.sleep(0.01)
                            continue
                    
                    # 모든 futures 정리
                    for future in as_completed(futures, timeout=30):
                        try:
                            future.result()
                        except Exception as e:
                            print(f"프로세스 완료 오류: {e}")
                
                # 최종 완료 메시지
                yield f"data: {json.dumps({'type': 'all_complete', 'message': '모든 프로세스 완료'})}\n\n"
                
                yield "data: [DONE]\n\n"
                
            except Exception as e:
                yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
                yield "data: [DONE]\n\n"
        
        return Response(
            stream_with_context(generate()),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Headers': 'Content-Type',
                'X-Accel-Buffering': 'no'  # Disable nginx buffering
            }
        )
    
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@chat_bp.route('/chatgpt-basic', methods=['POST'])
def chatgpt_basic():
    """ChatGPT + s3기본 개별 처리"""
    try:
        data = request.get_json()
        question = data.get('question')
        
        if not question:
            return jsonify({"error": "Question is required"}), 400
        
        chain = get_rag_chain()
        
        # 벡터스토어 초기화 확인
        chain._initialize_vectorstore()
        
        # 기본 검색 수행
        import time
        start_time = time.time()
        
        try:
            if hasattr(chain, 'dual_vectorstore_manager') and chain.dual_vectorstore_manager:
                search_results = chain.dual_vectorstore_manager.similarity_search_with_score(question, "basic", k=5)
            else:
                search_results = chain.vectorstore_manager.similarity_search_with_score(question, k=5)
            
            # s3 폴더 문서만 필터링 (s3-chunking 제외)
            s3_results = [(doc, score) for doc, score in search_results if doc.metadata.get('source') == 's3']
            
            if not s3_results:
                return jsonify({
                    'success': False,
                    'answer': 's3 폴더의 문서가 없습니다. s3 폴더에서 문서를 먼저 로드해주세요.',
                    'similarity_info': [],
                    'total_time': 0.0,
                    'process_name': 'ChatGPT + s3기본',
                    'chunking_type': 'basic'
                })
            
            # 첫 번째 결과로 최적화된 답변 생성 (chunking optimization)
            if s3_results:
                doc, score = s3_results[0]
                # Optimize context length for better LLM performance
                max_context_length = 2500  # Optimal length for single document processing
                if len(doc.page_content) > max_context_length:
                    context = doc.page_content[:max_context_length-3] + "..."
                    print(f"[CHUNKING OPT] ChatGPT-basic context truncated from {len(doc.page_content)} to {len(context)} chars")
                else:
                    context = doc.page_content
                prompt = chain.prompt_template.format(context=context, question=question)
                
                # OpenAI API 호출
                from openai import OpenAI
                import os
                client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), timeout=30.0)
                stream = client.chat.completions.create(
                    model='gpt-4o-mini',
                    messages=[{"role": "user", "content": prompt}],
                    stream=True,
                    temperature=0.1
                )
                
                answer_text = ""
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        answer_text += chunk.choices[0].delta.content
                
                # 유사도 정보 생성 (s3 결과만 사용)
                similarity_info = []
                for i, (doc, score) in enumerate(s3_results[:3], 1):
                    similarity_info.append({
                        'rank': i,
                        'score': f'{score:.1%}',
                        'source': doc.metadata.get('source', 's3'),
                        'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content
                    })
                
                end_time = time.time()
                total_time = end_time - start_time
                
                return jsonify({
                    'success': True,
                    'answer': answer_text,
                    'similarity_info': similarity_info,
                    'total_time': round(total_time, 2),
                    'process_name': 'ChatGPT + s3기본',
                    'chunking_type': 'basic'
                })
            else:
                return jsonify({
                    'success': False,
                    'answer': '관련 문서를 찾을 수 없습니다.',
                    'similarity_info': [],
                    'total_time': 0.0,
                    'process_name': 'ChatGPT + s3기본',
                    'chunking_type': 'basic'
                })
                
        except Exception as e:
            end_time = time.time()
            total_time = end_time - start_time
            return jsonify({
                'success': False,
                'answer': f'오류 발생: {str(e)}',
                'similarity_info': [],
                'total_time': round(total_time, 2),
                'process_name': 'ChatGPT + s3기본',
                'chunking_type': 'basic'
            })
            
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@chat_bp.route('/chatgpt-custom', methods=['POST'])
def chatgpt_custom():
    """ChatGPT + s3-chunking 개별 처리"""
    try:
        data = request.get_json()
        question = data.get('question')
        
        if not question:
            return jsonify({"error": "Question is required"}), 400
        
        chain = get_rag_chain()
        
        # 벡터스토어 초기화 확인
        chain._initialize_vectorstore()
        
        # 커스텀 검색 수행
        import time
        start_time = time.time()
        
        try:
            if hasattr(chain, 'dual_vectorstore_manager') and chain.dual_vectorstore_manager:
                search_results = chain.dual_vectorstore_manager.similarity_search_with_score(question, "custom", k=3)
            else:
                search_results = chain.vectorstore_manager.similarity_search_with_score(question, k=3)
            
            # 첫 번째 결과로 최적화된 답변 생성 (chunking optimization)
            if search_results:
                doc, score = search_results[0]
                # Optimize context length for better LLM performance  
                max_context_length = 2500  # Optimal length for single document processing
                if len(doc.page_content) > max_context_length:
                    context = doc.page_content[:max_context_length-3] + "..."
                    print(f"[CHUNKING OPT] ChatGPT-custom context truncated from {len(doc.page_content)} to {len(context)} chars")
                else:
                    context = doc.page_content
                prompt = chain.prompt_template.format(context=context, question=question)
                
                # OpenAI API 호출
                from openai import OpenAI
                import os
                client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), timeout=30.0)
                stream = client.chat.completions.create(
                    model='gpt-4o-mini',
                    messages=[{"role": "user", "content": prompt}],
                    stream=True,
                    temperature=0.1
                )
                
                answer_text = ""
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        answer_text += chunk.choices[0].delta.content
                
                # 유사도 정보 생성
                similarity_info = []
                for i, (doc, score) in enumerate(search_results[:3], 1):
                    similarity_info.append({
                        'rank': i,
                        'score': f'{score:.1%}',
                        'source': doc.metadata.get('source_file', 'Unknown'),
                        'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content
                    })
                
                end_time = time.time()
                total_time = end_time - start_time
                
                return jsonify({
                    'success': True,
                    'answer': answer_text,
                    'similarity_info': similarity_info,
                    'total_time': round(total_time, 2),
                    'process_name': 'ChatGPT + s3-chunking',
                    'chunking_type': 'custom'
                })
            else:
                return jsonify({
                    'success': False,
                    'answer': '관련 문서를 찾을 수 없습니다.',
                    'similarity_info': [],
                    'total_time': 0.0,
                    'process_name': 'ChatGPT + s3-chunking',
                    'chunking_type': 'custom'
                })
                
        except Exception as e:
            end_time = time.time()
            total_time = end_time - start_time
            return jsonify({
                'success': False,
                'answer': f'오류 발생: {str(e)}',
                'similarity_info': [],
                'total_time': round(total_time, 2),
                'process_name': 'ChatGPT + s3-chunking',
                'chunking_type': 'custom'
            })
            
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@chat_bp.route('/local-basic', methods=['POST'])
def local_basic():
    """로컬LLM + s3기본 개별 처리"""
    try:
        data = request.get_json()
        question = data.get('question')
        session_id = data.get('session_id', str(uuid.uuid4()))
        
        if not question:
            return jsonify({'error': '질문이 필요합니다'}), 400
        
        def generate_response():
            try:
                start_time = time.time()
                
                # 벡터 검색 수행 (basic 컬렉션에서 검색 - s3 문서만)
                from models.dual_vectorstore import get_dual_vectorstore
                dual_vectorstore = get_dual_vectorstore()
                search_results = dual_vectorstore.similarity_search_with_score(question, "basic", k=5)
                
                if not search_results:
                    yield f"data: {json.dumps({'type': 'process_error', 'process_name': '로컬LLM + s3기본', 'process_id': 3, 'session_id': session_id, 'error': 'no_documents', 'message': '검색된 문서가 없습니다. s3 폴더에서 문서를 먼저 로드해주세요.', 'similarity_info': [], 'total_time': 0.0, 'status': 'failed'}, ensure_ascii=False)}\n\n"
                    return

                # basic 컬렉션에는 이미 s3 문서만 있으므로 필터링 불필요
                s3_results = search_results
                
                if not s3_results:
                    yield f"data: {json.dumps({'type': 'process_error', 'process_name': '로컬LLM + s3기본', 'process_id': 3, 'session_id': session_id, 'error': 'no_s3_documents', 'message': 's3 폴더의 문서가 없습니다. s3 폴더에서 문서를 먼저 로드해주세요.', 'similarity_info': [], 'total_time': 0.0, 'status': 'failed'}, ensure_ascii=False)}\n\n"
                    return

                # 컨텍스트 생성 (매우 짧게, 로컬 LLM 토큰 제한)
                s3_context = ""
                max_context_length = 200  # 더 짧게 제한
                for doc, score in s3_results[:2]:  # 2개만 사용
                    content = doc.page_content[:max_context_length]
                    s3_context += f"{content}\n\n"
                    if len(s3_context) > 400:  # 매우 짧은 전체 길이
                        break

                # 로컬 LLM 호출 (매우 짧은 프롬프트)
                from models.llm import LLMManager
                llm_manager = LLMManager()
                local_llm = llm_manager.get_llm(model_name='local')
                
                prompt = f"""다음 정보로 답변: {s3_context[:300]}

질문: {question}

답변:"""

                try:
                    response = local_llm.invoke(prompt)
                    answer = str(response) if response else '로컬 LLM 응답 없음'
                except Exception as e:
                    # 로컬 LLM 에러 시 컨텍스트 기반 간단 답변
                    answer = f"검색된 정보: {s3_context[:200]}... (로컬 LLM 서버 에러: {str(e)[:50]})"
                
                end_time = time.time()
                total_time = end_time - start_time
                
                # 유사도 정보 생성 (s3 결과만 사용)
                similarity_info = []
                for i, (doc, score) in enumerate(s3_results[:3], 1):
                    similarity_info.append({
                        'rank': i,
                        'score': f'{score:.1%}',
                        'source': doc.metadata.get('source', 's3'),
                        'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content
                    })
                
                yield f"data: {json.dumps({'type': 'process_complete', 'process_name': '로컬LLM + s3기본', 'process_id': 3, 'session_id': session_id, 'answer': answer, 'similarity_info': similarity_info, 'total_time': total_time, 'status': 'success', 'chunking_type': 'basic'}, ensure_ascii=False)}\n\n"
                
            except Exception as e:
                print(f"로컬LLM 기본 처리 오류: {e}")
                yield f"data: {json.dumps({'type': 'process_error', 'process_name': '로컬LLM + s3기본', 'process_id': 3, 'session_id': session_id, 'error': 'connection_error', 'message': f'로컬 LLM 연결 오류: {str(e)}. 로컬 LLM 서버를 시작해주세요.', 'similarity_info': [], 'total_time': 0.0, 'status': 'failed'}, ensure_ascii=False)}\n\n"

        return Response(
            generate_response(),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'X-Accel-Buffering': 'no'
            }
        )
        
    except Exception as e:
        return jsonify({
            'error': f'로컬LLM + s3기본 처리 실패: {str(e)}',
            'status': 'failed'
        }), 500

@chat_bp.route('/local-custom', methods=['POST'])
def local_custom():
    """로컬LLM + s3-chunking 개별 처리"""
    try:
        data = request.get_json()
        question = data.get('question')
        session_id = data.get('session_id', str(uuid.uuid4()))
        
        if not question:
            return jsonify({'error': '질문이 필요합니다'}), 400
        
        def generate_response():
            try:
                start_time = time.time()
                
                # 벡터 검색 수행
                vectorstore_manager = VectorStoreManager()
                search_results = vectorstore_manager.similarity_search(question, k=5)
                
                if not search_results:
                    yield f"data: {json.dumps({'type': 'process_error', 'process_name': '로컬LLM + s3-chunking', 'process_id': 4, 'session_id': session_id, 'error': 'no_documents', 'message': '검색된 문서가 없습니다. s3-chunking 폴더에서 문서를 먼저 로드해주세요.', 'similarity_info': [], 'total_time': 0.0, 'status': 'failed'}, ensure_ascii=False)}\n\n"
                    return

                # s3-chunking 문서 필터링
                s3_chunking_results = [(doc, score) for doc, score in search_results if doc.metadata.get('source') == 's3-chunking']
                
                if not s3_chunking_results:
                    yield f"data: {json.dumps({'type': 'process_error', 'process_name': '로컬LLM + s3-chunking', 'process_id': 4, 'session_id': session_id, 'error': 'no_chunking_documents', 'message': 's3-chunking 폴더의 문서가 없습니다.', 'similarity_info': [], 'total_time': 0.0, 'status': 'failed'}, ensure_ascii=False)}\n\n"
                    return

                # 컨텍스트 생성 (s3-chunking 문서만 사용, 길이 제한)
                context = ""
                max_context_length = 500  # 로컬 LLM의 토큰 제한 고려
                for doc, score in s3_chunking_results[:3]:
                    content = doc.page_content
                    if len(content) > max_context_length:
                        content = content[:max_context_length] + "..."
                    context += f"[유사도: {score:.1%}] {content}\n\n"
                    if len(context) > 1000:  # 전체 컨텍스트 길이 제한
                        break

                # 로컬 LLM 호출 (LocalLLM 클래스 사용)
                from models.llm import LLMManager
                llm_manager = LLMManager()
                local_llm = llm_manager.get_llm(model_name='local')
                
                prompt = f"""다음 정보로 답변: {context[:300]}

질문: {question}

답변:"""

                try:
                    response = local_llm.invoke(prompt)
                    answer = str(response) if response else '로컬 LLM 응답 없음'
                except Exception as e:
                    # 로컬 LLM 에러 시 컨텍스트 기반 간단 답변
                    answer = f"검색된 정보: {context[:200]}... (로컬 LLM 서버 에러: {str(e)[:50]})"
                
                end_time = time.time()
                total_time = end_time - start_time
                
                # 유사도 정보 생성
                similarity_info = []
                for i, (doc, score) in enumerate(s3_chunking_results[:3], 1):
                    similarity_info.append({
                        'rank': i,
                        'score': f'{score:.1%}',
                        'source': doc.metadata.get('source', 's3-chunking'),
                        'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content
                    })
                
                yield f"data: {json.dumps({'type': 'process_complete', 'process_name': '로컬LLM + s3-chunking', 'process_id': 4, 'session_id': session_id, 'answer': answer, 'similarity_info': similarity_info, 'total_time': total_time, 'status': 'success', 'chunking_type': 'custom'}, ensure_ascii=False)}\n\n"
                
            except Exception as e:
                print(f"로컬LLM 커스텀 처리 오류: {e}")
                yield f"data: {json.dumps({'type': 'process_error', 'process_name': '로컬LLM + s3-chunking', 'process_id': 4, 'session_id': session_id, 'error': 'connection_error', 'message': f'로컬 LLM 연결 오류: {str(e)}. 로컬 LLM 서버를 시작해주세요.', 'similarity_info': [], 'total_time': 0.0, 'status': 'failed'}, ensure_ascii=False)}\n\n"

        return Response(
            generate_response(),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'X-Accel-Buffering': 'no'
            }
        )
        
    except Exception as e:
        return jsonify({
            'error': f'로컬LLM + s3-chunking 처리 실패: {str(e)}',
            'status': 'failed'
        }), 500

@chat_bp.route('/vllm-dual-stream', methods=['POST'])
def vllm_dual_stream():
    """vLLM 두 옵션 동시 스트리밍 (사내서버 vLLM + s3기본, 사내서버 vLLM + s3-chunking)"""
    try:
        print(f"🌐 [REQUEST] vLLM 듀얼 스트리밍 요청 받음")
        print(f"📋 [REQUEST] Content-Type: {request.content_type}")
        print(f"📋 [REQUEST] Method: {request.method}")
        
        data = request.get_json()
        print(f"📊 [REQUEST] 받은 데이터: {data}")
        
        if not data:
            print("❌ [REQUEST] JSON 데이터가 없음")
            return jsonify({"error": "JSON data is required"}), 400
        
        question = data.get('question') or data.get('query')
        selected_model = data.get('local_model', 'kanana8b')  # 클라이언트에서 선택한 모델
        print(f"❓ [REQUEST] 질문: {question}")
        print(f"🤖 [REQUEST] 선택된 모델: {selected_model}")
        
        if not question:
            print("❌ [REQUEST] 질문이 없음")
            return jsonify({"error": "Question or query is required"}), 400
        
        def generate():
            import time
            import uuid
            from concurrent.futures import ThreadPoolExecutor, as_completed
            from queue import Queue, Empty
            
            # 세션 ID 생성
            session_id = str(uuid.uuid4())
            
            # 2개 vLLM 프로세스 정의
            processes = [
                {"llm_type": "local", "chunking": "basic", "name": "사내서버 vLLM + s3기본", "process_id": 3},
                {"llm_type": "local", "chunking": "custom", "name": "사내서버 vLLM + s3-chunking", "process_id": 4}
            ]
            
            result_queue = Queue()
            
            def vllm_process_task(process_info, question, session_id, selected_model):
                """vLLM 처리 태스크 - 로깅 추가"""
                start_time = time.time()
                process_name = process_info["name"]
                process_id = process_info["process_id"]
                chunking_type = process_info["chunking"]
                
                print(f"🚀 [vLLM {process_id}] {process_name} 시작")
                print(f"📊 [vLLM {process_id}] 질문: {question[:100]}...")
                print(f"🔧 [vLLM {process_id}] 청킹 타입: {chunking_type}")
                
                # 김명정 등 사용자 감지 및 카드 정보 처리
                enhanced_question = question
                card_summary = None
                is_personalized = False
                
                user_patterns = ['김명정', '이영희', '박철수', '최영수']
                detected_user = None
                
                for user in user_patterns:
                    if user in question:
                        detected_user = user
                        break
                
                if detected_user:
                    print(f"🔍 [vLLM {process_id}] {detected_user} 고객 감지")
                    card_keywords = ['카드', '발급', '회원은행', '은행별']
                    
                    if any(keyword in question for keyword in card_keywords):
                        print(f"💳 [vLLM {process_id}] 카드 관련 질의 감지 - 동적 처리")
                        # 개인화 플래그 설정 (중요: MCP 성공 전에 미리 설정)
                        is_personalized = True
                        print(f"✅ [vLLM {process_id}] 개인화 모드 활성화: {detected_user} 고객")
                        
                        # MCP 카드 분석 시스템 강화 (모든 청킹 타입에서 동작)
                        try:
                            print(f"🔍 [vLLM {process_id}] MCP 카드 분석 시스템 시작")
                            
                            # CardAnalysisService 사용
                            from services.card_analysis_service import CardAnalysisService
                            card_service = CardAnalysisService()
                            
                            # 김명정 카드 정보 분석
                            analysis = card_service.analyze_customer_cards(detected_user)
                            card_summary = card_service.format_analysis_response(analysis)
                            
                            print(f"📊 [vLLM {process_id}] 카드 분석 결과:")
                            print(f"   - 보유 카드: {len(analysis.owned_cards)}장")
                            print(f"   - 추천 카드: {len(analysis.recommended_cards)}장") 
                            print(f"   - 발급 가능: {len(analysis.available_cards)}장")
                            
                            # 카드 이미지 정보 추가
                            owned_card_images = []
                            for card in analysis.owned_cards:
                                if '우리카드' in card.name:
                                    owned_card_images.append('![\uc6b0\ub9ac\uce74\ub4dc \ub85c\uace0](/images/Aspose.Words.4c2a2064-0c7c-48d5-aca6-c4d7a6eade2b.014.gif)')
                                elif '하나카드' in card.name:
                                    owned_card_images.append('![\ud558\ub098\uce74\ub4dc \ub85c\uace0](/images/Aspose.Words.4c2a2064-0c7c-48d5-aca6-c4d7a6eade2b.016.gif)')
                                elif 'NH카드' in card.name or '농협' in card.name:
                                    owned_card_images.append('![\ub18d\ud611\uce74\ub4dc \ub85c\uace0](/images/Aspose.Words.4c2a2064-0c7c-48d5-aca6-c4d7a6eade2b.017.gif)')
                            
                            # 발급 가능 카드 이미지
                            available_card_images = []
                            for card in analysis.available_cards + analysis.recommended_cards:
                                if 'BC카드' in card.name:
                                    available_card_images.append('![BC\uce74\ub4dc \ub85c\uace0](/images/Aspose.Words.4c2a2064-0c7c-48d5-aca6-c4d7a6eade2b.004.gif)')
                                elif '신한카드' in card.name:
                                    available_card_images.append('![\uc2e0\ud55c\uce74\ub4dc \ub85c\uace0](/images/Aspose.Words.4c2a2064-0c7c-48d5-aca6-c4d7a6eade2b.005.gif)')
                                elif '국민카드' in card.name or 'KB' in card.name:
                                    available_card_images.append('![\uad6d\ubbfc\uce74\ub4dc \ub85c\uace0](/images/Aspose.Words.4c2a2064-0c7c-48d5-aca6-c4d7a6eade2b.006.jpeg)')
                                elif '롯데카드' in card.name:
                                    available_card_images.append('![\ub86f\ub370\uce74\ub4dc \ub85c\uace0](/images/Aspose.Words.4c2a2064-0c7c-48d5-aca6-c4d7a6eade2b.007.jpeg)')
                                elif '삼성카드' in card.name:
                                    available_card_images.append('![\uc0bc\uc131\uce74\ub4dc \ub85c\uace0](/images/Aspose.Words.4c2a2064-0c7c-48d5-aca6-c4d7a6eade2b.008.jpeg)')
                            
                            # 카드 발급 절차 이미지
                            process_image = '![\uce74\ub4dc\ubc1c\uae09 \uc808\ucc28](/images/Aspose.Words.4c2a2064-0c7c-48d5-aca6-c4d7a6eade2b.013.gif)'
                            
                            enhanced_question = f"""
질문: {question}

=== {detected_user} 고객 카드 분석 결과 ===
{card_summary}

현재 보유하신 카드 이미지:
{chr(10).join(owned_card_images) if owned_card_images else '보유한 카드가 없습니다.'}

발급 가능한 카드 이미지:
{chr(10).join(available_card_images[:3])}

카드 발급 절차:
{process_image}

위 분석 결과와 이미지를 바탕으로 {detected_user} 고객에게 맞춤형 카드 발급 안내를 제공해주세요.
현재 보유하신 카드와 새로 발급 가능한 카드를 명확히 구분하여 안내해주세요.
답변에 이미지를 포함하여 작성해주세요.
"""
                            print(f"✅ [vLLM {process_id}] MCP 개인화 질의 생성 완료: {len(enhanced_question)}자")
                            print(f"🎯 [vLLM {process_id}] 개인화 전용 프롬프트 사용 예정")
                            
                        except Exception as e:
                            print(f"❌ [vLLM {process_id}] MCP 카드 분석 오류: {e}")
                            import traceback
                            print(f"🔍 [vLLM {process_id}] 상세 오류:\n{traceback.format_exc()}")
                            enhanced_question = question
                
                try:
                    # 프로세스 시작 알림
                    result_queue.put({
                        'type': 'process_start',
                        'process_name': process_name,
                        'process_id': process_id,
                        'session_id': session_id
                    })
                    
                    # vLLM 연결 확인
                    from models.llm import LLMManager
                    print(f"🔌 [vLLM {process_id}] LLM 매니저 생성 중...")
                    llm_manager = LLMManager()
                    
                    print(f"🤖 [vLLM {process_id}] vLLM 연결 시도: 192.168.0.224:8412")
                    vllm_llm = llm_manager.get_vllm_llm()
                    print(f"✅ [vLLM {process_id}] vLLM 연결 성공")
                    
                    # 문서 검색
                    print(f"🔍 [vLLM {process_id}] 문서 검색 시작...")
                    chain = get_rag_chain()
                    
                    # 벡터스토어 초기화 확인
                    chain._initialize_vectorstore()
                    
                    if chunking_type == "basic":
                        # s3기본: DualVectorStore의 basic 컬렉션에서 검색
                        if hasattr(chain, 'dual_vectorstore_manager') and chain.dual_vectorstore_manager:
                            print(f"📚 [vLLM {process_id}] basic 컬렉션에서 검색")
                            search_results = chain.dual_vectorstore_manager.similarity_search_with_score(question, "basic", k=10)  # 더 많은 결과
                        else:
                            print(f"⚠️ [vLLM {process_id}] 폴백: 기본 벡터스토어 사용")
                            search_query = enhanced_question if is_personalized else question
                            search_results = chain.vectorstore_manager.similarity_search_with_score(search_query, k=10)
                    else:
                        # s3-chunking: 다양한 키워드로 검색하여 카드 상세 정보 포함
                        print(f"📚 [vLLM {process_id}] custom 컬렉션에서 확장 검색 시작")
                        
                        # 1차: 기본 검색 (개인화된 경우 enhanced_question 사용)
                        search_query = enhanced_question if is_personalized else question
                        basic_search = chain.dual_vectorstore_manager.similarity_search_with_score(search_query, "custom", k=5)
                        
                        # 2차: 카드 발급 상세 정보를 위한 키워드 검색
                        detailed_keywords = [
                            "카드발급 절차",
                            "발급대상",
                            "신청방법 구비서류",
                            "회원은행 영업점",
                            "카드 심사",
                            "결제능력 심사",
                            "BC카드발급안내",
                            f"{detected_user} 고객 현황",
                            "보유 카드 미보유 카드",
                            "카드 이미지"
                        ]
                        
                        additional_results = []
                        for keyword in detailed_keywords:
                            try:
                                keyword_results = chain.dual_vectorstore_manager.similarity_search_with_score(keyword, "custom", k=3)
                                additional_results.extend(keyword_results)
                                print(f"🔑 [vLLM {process_id}] '{keyword}' 검색: {len(keyword_results)}개 결과")
                            except Exception as e:
                                print(f"⚠️ [vLLM {process_id}] '{keyword}' 검색 실패: {e}")
                                continue
                        
                        # 3차: s3기본 컬렉션에서도 카드 발급 상세 정보 가져오기 (크로스 검색)
                        s3_basic_results = []
                        try:
                            s3_basic_results = chain.dual_vectorstore_manager.similarity_search_with_score("카드발급 절차 신청방법", "basic", k=5)
                            print(f"🔄 [vLLM {process_id}] s3기본에서 크로스 검색: {len(s3_basic_results)}개 결과")
                            additional_results.extend(s3_basic_results)
                        except Exception as e:
                            print(f"⚠️ [vLLM {process_id}] s3기본 크로스 검색 실패: {e}")
                        
                        # 결과 합치기 및 중복 제거
                        all_results = basic_search + additional_results
                        seen_content = set()
                        search_results = []
                        
                        for doc, score in all_results:
                            content_hash = hash(doc.page_content[:100])  # 첫 100자로 중복 체크
                            if content_hash not in seen_content:
                                search_results.append((doc, score))
                                seen_content.add(content_hash)
                        
                        # 점수순 정렬
                        search_results.sort(key=lambda x: x[1], reverse=True)
                        search_results = search_results[:15]  # 상위 15개로 확대
                        
                        print(f"🔍 [vLLM {process_id}] 확장 검색 완료: 기본 {len(basic_search)}개 + 키워드 {len(additional_results)-len(s3_basic_results)}개 + s3기본 {len(s3_basic_results)}개 → 최종 {len(search_results)}개")
                    
                    print(f"📊 [vLLM {process_id}] 검색 결과: {len(search_results)}개")
                    if search_results:
                        print(f"🎯 [vLLM {process_id}] 최고 유사도: {search_results[0][1]:.2%}")
                    
                    if not search_results:
                        result_queue.put({
                            'type': 'process_error',
                            'process_name': process_name,
                            'process_id': process_id,
                            'session_id': session_id,
                            'error': 'no_results',
                            'message': '검색 결과가 없습니다.',
                            'similarity_info': [],
                            'total_time': round(time.time() - start_time, 2),
                            'status': 'failed'
                        })
                        return
                    
                    # 컨텍스트 준비 (카드 상세 정보 우선 포함)
                    context = ""
                    max_doc_length = 3000  # 카드 이미지와 상세 정보를 위해 더 확대
                    included_docs = 0
                    
                    # 우선순위 1: 이미지가 포함된 문서 먼저 포함
                    image_docs = []
                    other_docs = []
                    
                    for doc, score in search_results:
                        if '![' in doc.page_content and '](' in doc.page_content:  # 이미지 문법이 포함된 문서
                            image_docs.append((doc, score))
                        else:
                            other_docs.append((doc, score))
                    
                    # 이미지가 있는 문서 먼저 포함
                    for doc, score in (image_docs + other_docs):
                        if included_docs >= 8:  # 최대 8개 문서까지
                            break
                            
                        doc_content = doc.page_content
                        if len(doc_content) > max_doc_length:
                            doc_content = doc_content[:max_doc_length] + "..."
                        
                        context += f"[유사도: {score:.1%}] {doc_content}\n\n"
                        included_docs += 1
                        
                        if len(context) > 10000:  # 전체 컨텍스트 더 확대 (이미지 정보 포함)
                            break
                    
                    print(f"📝 [vLLM {process_id}] 컨텍스트 구성: 이미지 문서 {len(image_docs)}개, 일반 문서 {len(other_docs)}개, 총 {included_docs}개 포함")
                    
                    print(f"📝 [vLLM {process_id}] 컨텍스트 길이: {len(context)}자")
                    
                    # vLLM 호출을 위한 프롬프트 (개인화 대응)
                    print(f"🤖 [vLLM {process_id}] 프롬프트 선택: is_personalized={is_personalized}, detected_user={detected_user}")
                    
                    if is_personalized and detected_user:
                        # 개인화된 카드 발급 전용 프롬프트
                        print(f"🎆 [vLLM {process_id}] {detected_user} 고객 전용 개인화 프롬프트 사용")
                        prompt = f"""당신은 BC카드 전문 상담사입니다. {detected_user} 고객에게 맞춤형 카드 발급 안내를 제공하세요.

=== 제공된 자료 ===
{context}

=== {detected_user} 고객 개인화 정보 ===
{enhanced_question}

**답변 작성 가이드:**
1. **제목 표시**: "**1. 현재 보유 중인 카드**", "**2. 발급 추천 카드**" 형식으로 굵게 표시 (### 마크다운 헤더 절대 사용 금지)
2. 보유 카드는 "✅ 현재 보유중" 으로 표시
3. 추천 카드는 "⭐ 발급 추천" 으로 표시  
4. 발급 가능 카드는 "🆕 발급가능" 으로 표시
5. 이미지가 있는 경우 ![\uce74\ub4dc\uc774\ub984](\uc774\ubbf8\uc9c0\ud30c\uc77c) 형식으로 포함
6. **실제 데이터만 사용**: 제공된 자료에 있는 내용만 사용하고, 없는 내용은 절대 임의로 생성하지 말 것
7. BC카드 발급 절차는 제공된 자료에 있는 경우만 포함
8. **가독성 개선**: 각 섹션 사이에 적절한 공백 포함하여 읽기 편하게 구성
9. **중요**: 마크다운 ### 헤더 대신 **굵은 글씨**로 제목을 표시하여 웹에서 깔끔하게 보이도록 할 것
10. **허위정보 금지**: VIP 혜택, 특별 우대 등 제공된 자료에 없는 내용은 절대 언급하지 말 것

답변:"""
                    else:
                        # 일반 프롬프트
                        print(f"🗏️ [vLLM {process_id}] 기본 프롬프트 사용")
                        prompt = f"""다음 정보를 바탕으로 질문에 답하세요:

{context}

질문: {question}

답변:"""
                    
                    print(f"🤖 [vLLM {process_id}] vLLM 호출 시작...")
                    print(f"🌐 [vLLM {process_id}] 서버: 192.168.0.224:8412")
                    print(f"🔧 [vLLM {process_id}] 모델: {selected_model}")
                    
                    # 선택된 모델로 vLLM 인스턴스 생성
                    from langchain_openai import ChatOpenAI
                    config = Config.LLM_MODELS['local']
                    custom_vllm = ChatOpenAI(
                        model=selected_model,  # 선택된 모델 사용
                        openai_api_base=config['base_url'] + '/v1',
                        openai_api_key='EMPTY',
                        temperature=config['temperature'],
                        max_tokens=config['max_tokens']
                    )
                    
                    # 커스텀 vLLM 호출
                    response = custom_vllm.invoke(prompt)
                    print(f"📨 [vLLM {process_id}] vLLM 응답 받음")
                    
                    # 응답 처리
                    if hasattr(response, 'content'):
                        answer = response.content
                    elif isinstance(response, str):
                        answer = response
                    else:
                        answer = str(response)
                    
                    print(f"✅ [vLLM {process_id}] 답변 생성 완료: {len(answer)}자")
                    
                    # 유사도 정보 생성
                    similarity_info = []
                    for i, (doc, score) in enumerate(search_results[:3], 1):
                        # 벡터스토어에서 이미 0-1 범위의 cosine similarity를 반환하므로 그대로 사용
                        normalized_score = max(0.0, min(1.0, score))  # 0-1 범위로 클램핑만 수행
                        
                        similarity_info.append({
                            'rank': i,
                            'score': f'{normalized_score:.2%}',  # 소수점 2자리로 표시
                            'source': doc.metadata.get('source_file', doc.metadata.get('source', 'Unknown')),
                            'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content
                        })
                    
                    end_time = time.time()
                    total_time = end_time - start_time
                    print(f"⏱️ [vLLM {process_id}] 총 처리 시간: {total_time:.2f}초")
                    
                    # RAG 응답에서 이미지 경로 자동 추출
                    extracted_images = extract_images_from_context(context, answer)
                    print(f"🖼️ [vLLM {process_id}] 추출된 이미지: {len(extracted_images)}개")
                    
                    result_queue.put({
                        'type': 'process_complete',
                        'process_name': process_name,
                        'process_id': process_id,
                        'session_id': session_id,
                        'answer': answer,
                        'similarity_info': similarity_info,
                        'total_time': round(total_time, 2),
                        'status': 'success',
                        'chunking_type': chunking_type,
                        'images': extracted_images  # 이미지 정보 추가
                    })
                    
                    print(f"🎉 [vLLM {process_id}] 프로세스 완료!")
                    
                except Exception as e:
                    end_time = time.time()
                    total_time = end_time - start_time
                    error_msg = str(e)
                    print(f"❌ [vLLM {process_id}] 오류 발생: {error_msg}")
                    import traceback
                    print(f"🔍 [vLLM {process_id}] 상세 오류:\n{traceback.format_exc()}")
                    
                    # vLLM 특화 에러 메시지
                    if "connection" in error_msg.lower() or "connect" in error_msg.lower():
                        user_message = f"vLLM 서버 연결 실패 (192.168.0.224:8412). 서버 상태를 확인해주세요."
                    elif "timeout" in error_msg.lower():
                        user_message = f"vLLM 응답 시간 초과. 서버 부하를 확인해주세요."
                    else:
                        user_message = f"vLLM 처리 중 오류: {error_msg[:100]}"
                    
                    result_queue.put({
                        'type': 'process_error',
                        'process_name': process_name,
                        'process_id': process_id,
                        'session_id': session_id,
                        'error': 'vllm_error',
                        'message': user_message,
                        'similarity_info': [],
                        'total_time': round(total_time, 2),
                        'status': 'failed'
                    })
            
            # 시작 알림
            yield f"data: {json.dumps({'type': 'all_processes_start', 'total_processes': 2, 'session_id': session_id}, ensure_ascii=False)}\n\n"
            print(f"🚀 [MAIN] vLLM 듀얼 스트리밍 시작 - 세션: {session_id}")
            
            # 2개 프로세스 병렬 실행
            with ThreadPoolExecutor(max_workers=2) as executor:
                futures = []
                for process_info in processes:
                    future = executor.submit(vllm_process_task, process_info, question, session_id, selected_model)
                    futures.append(future)
                    print(f"🔄 [MAIN] {process_info['name']} 스레드 시작")
                
                # 결과 스트리밍
                completed_processes = 0
                total_processes = 2
                
                while completed_processes < total_processes:
                    try:
                        result = result_queue.get(timeout=0.1)
                        yield f"data: {json.dumps(result, ensure_ascii=False)}\n\n"
                        
                        if result['type'] in ['process_complete', 'process_error']:
                            completed_processes += 1
                            print(f"✅ [MAIN] 프로세스 완료: {result.get('process_name', 'Unknown')} ({completed_processes}/{total_processes})")
                            
                    except Empty:
                        time.sleep(0.01)
                        continue
                
                # futures 정리
                for future in as_completed(futures, timeout=60):
                    try:
                        future.result()
                    except Exception as e:
                        print(f"❌ [MAIN] 스레드 완료 오류: {e}")
            
            yield f"data: {json.dumps({'type': 'all_complete', 'message': 'vLLM 듀얼 처리 완료'}, ensure_ascii=False)}\n\n"
            yield "data: [DONE]\n\n"
            print(f"🏁 [MAIN] 모든 vLLM 프로세스 완료")
        
        return Response(
            stream_with_context(generate()),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Headers': 'Content-Type',
                'X-Accel-Buffering': 'no'
            }
        )
    
    except Exception as e:
        print(f"❌ [MAIN] vLLM 듀얼 스트리밍 오류: {e}")
        return jsonify({"error": str(e)}), 500

@chat_bp.route('/clear-memory', methods=['POST'])
def clear_memory():
    """Clear conversation memory"""
    try:
        chain = get_rag_chain()
        chain._initialize_vectorstore()
        chain.clear_memory()
        return jsonify({"message": "Memory cleared successfully"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500