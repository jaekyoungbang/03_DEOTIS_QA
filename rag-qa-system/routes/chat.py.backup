from flask import Blueprint, request, jsonify, stream_with_context, Response
from services.rag_chain import RAGChain
from models.vectorstore import VectorStoreManager
from config import Config
from services.card_manager import process_user_card_query
import json
import time
import uuid
import os
import re

chat_bp = Blueprint('chat', __name__)

def extract_images_from_context(context, answer):
    """RAG ì»¨í…ìŠ¤íŠ¸ì™€ ë‹µë³€ì—ì„œ ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ (ê°•í™”ëœ ì¹´ë“œ ì´ë¯¸ì§€ ì¸ì‹)"""
    images = []
    
    # MD íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ íŒ¨í„´ ì°¾ê¸°: ![alt](image.gif) ë˜ëŠ” ![alt](image.png)
    image_pattern = r'!\[([^\]]*)\]\(([^)]+\.(?:gif|png|jpg|jpeg))\)'
    
    # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ì§€ ê²€ìƒ‰
    context_matches = re.findall(image_pattern, context, re.IGNORECASE)
    for alt_text, image_path in context_matches:
        # ì¹´ë“œ ì´ë¯¸ì§€ ìš°ì„  ì²˜ë¦¬
        priority = 1
        if any(card_name in alt_text.lower() for card_name in ['ì¹´ë“œ', 'ë¡œê³ ', 'ë°œê¸‰']):
            priority = 5  # ì¹´ë“œ ì´ë¯¸ì§€ ê³ ìš°ì„ ìˆœìœ„
        elif 'ì ˆì°¨' in alt_text:
            priority = 4  # ì ˆì°¨ ì´ë¯¸ì§€
        
        images.append({
            'alt': alt_text,
            'path': image_path,
            'url': f'/images/{image_path}',  # ì›¹ ì„œë¹™ URL
            'source': 'context',
            'priority': priority
        })
    
    # ë‹µë³€ì—ì„œ ì´ë¯¸ì§„ ê²€ìƒ‰
    answer_matches = re.findall(image_pattern, answer, re.IGNORECASE)
    for alt_text, image_path in answer_matches:
        # ë‹µë³€ì˜ ì´ë¯¸ì§€ë¥¼ ìµœê³  ìš°ì„ ìˆœìœ„ë¡œ
        images.append({
            'alt': alt_text,
            'path': image_path,
            'url': f'/images/{image_path}',
            'source': 'answer',
            'priority': 10
        })
    
    # ì¤‘ë³µ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì •ë ¬
    unique_images = []
    seen_paths = set()
    
    # ìš°ì„ ìˆœìœ„ìˆœ ì •ë ¬
    images.sort(key=lambda x: x.get('priority', 1), reverse=True)
    
    for img in images:
        if img['path'] not in seen_paths:
            seen_paths.add(img['path'])
            unique_images.append(img)
    
    print(f"ğŸ–¼ï¸ ì¶”ì¶œëœ ì´ë¯¸ì§€: {len(unique_images)}ê°œ - {[img['path'] for img in unique_images]}")
    return unique_images

# Initialize RAG chain (consider using app context or dependency injection in production)
rag_chain = None

def get_rag_chain():
    global rag_chain
    if rag_chain is None:
        rag_chain = RAGChain()
    return rag_chain

@chat_bp.route('/query', methods=['POST'])
@chat_bp.route('/../rag/chat', methods=['POST'])
def query():
    """Handle chat queries"""
    try:
        data = request.get_json()
        question = data.get('question')
        use_memory = data.get('use_memory', False)
        llm_model = data.get('llm_model', 'gpt-4o-mini')
        search_mode = data.get('search_mode', 'basic')
        
        if not question:
            return jsonify({"error": "Question is required"}), 400
        
        # ì¹´ë“œ ë¶„ì„ ì§ˆë¬¸ ê°ì§€
        import re
        card_analysis_pattern = r'([ê°€-í£]{2,4})\s*(?:íšŒì›|ê³ ê°|ë‹˜|ì”¨)?\s*(?:ì¹´ë“œ|ë°œê¸‰)'
        match = re.search(card_analysis_pattern, question)
        
        if match:
            try:
                customer_name = match.group(1)
                from services.card_analysis_service import CardAnalysisService
                card_service = CardAnalysisService()
                analysis = card_service.analyze_customer_cards(customer_name)
                formatted_response = card_service.format_analysis_response(analysis)
                
                return jsonify({
                    "answer": formatted_response,
                    "sources": [f"{customer_name}_íšŒì›ì€í–‰ë³„_ì¹´ë“œë°œê¸‰ì•ˆë‚´.md"],
                    "query": question,
                    "search_mode": "card_analysis",
                    "llm_model": llm_model,
                    "processing_time": 0.5,
                    "source_documents": [],
                    "card_analysis": {
                        "customer_name": customer_name,
                        "owned_count": len(analysis.owned_cards),
                        "recommended_count": len(analysis.recommended_cards),
                        "available_count": len(analysis.available_cards),
                        "total_options": analysis.total_summary['ì´ì˜µì…˜']
                    }
                })
            except Exception as card_error:
                print(f"ì¹´ë“œ ë¶„ì„ ì˜¤ë¥˜: {card_error}")
                # ì¼ë°˜ RAGë¡œ fallback
        
        # Get RAG chain instance
        chain = get_rag_chain()
        
        # Query the RAG system with search mode
        response = chain.query(
            question, 
            use_memory=use_memory, 
            llm_model=llm_model,
            search_mode=search_mode
        )
        
        return jsonify(response)
    
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@chat_bp.route('/stream', methods=['POST'])
def stream_query():
    """Handle streaming chat queries with real-time progress"""
    try:
        data = request.get_json()
        question = data.get('question')
        use_memory = data.get('use_memory', False)
        llm_model = data.get('llm_model', 'gpt-4o-mini')
        search_mode = data.get('search_mode', 'basic')
        summarize = data.get('summarize', False)
        
        if not question:
            return jsonify({"error": "Question is required"}), 400
        
        def generate():
            import time
            
            try:
                chain = get_rag_chain()
                
                # Step 4: 4ê°œ ë³„ë„ í”„ë¡œì„¸ìŠ¤ë¡œ ë¹„ë™ê¸° ì²˜ë¦¬
                from models.llm import LLMManager
                from openai import OpenAI
                import os
                import platform
                from queue import Queue, Empty
                from concurrent.futures import ThreadPoolExecutor, as_completed
                import uuid
                
                # í”„ë¡œì„¸ìŠ¤ë³„ í´ë” ê²½ë¡œ ì„¤ì •
                if platform.system() == "Windows":
                    s3_folder = "D:\\99_DEOTIS_QA_SYSTEM\\03_DEOTIS_QA\\rag-qa-system\\s3"
                    s3_chunking_folder = "D:\\99_DEOTIS_QA_SYSTEM\\03_DEOTIS_QA\\rag-qa-system\\s3-chunking"
                else:
                    s3_folder = "/mnt/d/99_DEOTIS_QA_SYSTEM/03_DEOTIS_QA/rag-qa-system/s3"
                    s3_chunking_folder = "/mnt/d/99_DEOTIS_QA_SYSTEM/03_DEOTIS_QA/rag-qa-system/s3-chunking"
                
                # 4ê°œ í”„ë¡œì„¸ìŠ¤ ì •ì˜
                processes = [
                    {"llm_type": "openai", "chunking": "basic", "folder": s3_folder, "name": "ChatGPT + s3ê¸°ë³¸", "process_id": 1},
                    {"llm_type": "openai", "chunking": "custom", "folder": s3_chunking_folder, "name": "ChatGPT + s3-chunking", "process_id": 2},
                    {"llm_type": "local", "chunking": "basic", "folder": s3_folder, "name": "ë¡œì»¬LLM + s3ê¸°ë³¸", "process_id": 3},
                    {"llm_type": "local", "chunking": "custom", "folder": s3_chunking_folder, "name": "ë¡œì»¬LLM + s3-chunking", "process_id": 4}
                ]
                
                # Create a queue for streaming results from multiple processes
                result_queue = Queue()
                
                def process_single_search_task(process_info, question, llm_model, chain, session_id, summarize=False):
                    """ë‹¨ì¼ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ ì²˜ë¦¬ - ì™„ì „ ë…ë¦½ì  ì‹¤í–‰"""
                    start_time = time.time()  # ì‹œì‘ ì‹œê°„ ê¸°ë¡
                    try:
                        process_name = process_info["name"]
                        chunking_type = process_info["chunking"]
                        llm_type = process_info["llm_type"]
                        process_id = process_info["process_id"]
                        
                        print(f"[PROCESS] Starting {process_name} (ID: {process_id})")
                        
                        # í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì‹ í˜¸
                        result_queue.put({
                            'type': 'process_start',
                            'process_name': process_name,
                            'process_id': process_id,
                            'session_id': session_id
                        })
                        
                        # ë¡œì»¬ LLM ì‚¬ìš© ë¶ˆê°€ëŠ¥ ì‹œ ì˜¤ë¥˜ ì²˜ë¦¬
                        if llm_type == "local":
                            try:
                                # ë¡œì»¬ LLM ì—°ê²° í…ŒìŠ¤íŠ¸
                                from models.llm import LLMManager
                                llm_manager = LLMManager()
                                llm_manager.get_llm(model_name=llm_model)
                            except Exception as e:
                                result_queue.put({
                                    'type': 'process_error',
                                    'process_name': process_name,
                                    'process_id': process_id,
                                    'session_id': session_id,
                                    'error': 'vllm_not_running',
                                    'message': 'ì‚¬ë‚´ì„œë²„ vLLMì´ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•´ê²°ë°©ë²•: 1. vLLM ì„œë²„ ìƒíƒœ í™•ì¸: 192.168.0.224:8412 2. kanana8b ëª¨ë¸ ë¡œë”© í™•ì¸ 3. ì„œë²„ ì¬ì‹œì‘ í•„ìš”ì‹œ ì—°ë½',
                                    'similarity_info': [],  # ì˜¤ë¥˜ ì‹œ ë¹ˆ ìœ ì‚¬ë„ ì •ë³´
                                    'total_time': 0.0,
                                    'status': 'failed'
                                })
                                return
                        
                        # í•´ë‹¹ ì²­í‚¹ íƒ€ì…ìœ¼ë¡œ ê²€ìƒ‰ ìˆ˜í–‰
                        print(f"[DEBUG] {process_name} ê²€ìƒ‰ ì‹œì‘ - ì²­í‚¹íƒ€ì…: {chunking_type}")
                        print(f"[DEBUG] Chain object: {chain}")
                        print(f"[DEBUG] Has dual_vectorstore_manager: {hasattr(chain, 'dual_vectorstore_manager')}")
                        
                        if hasattr(chain, 'dual_vectorstore_manager') and chain.dual_vectorstore_manager:
                            if chunking_type == "custom":
                                print(f"[DEBUG] custom ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰")
                                search_results = chain.dual_vectorstore_manager.similarity_search_with_score(question, "custom", k=3)
                            else:
                                print(f"[DEBUG] basic ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰")
                                search_results = chain.dual_vectorstore_manager.similarity_search_with_score(question, "basic", k=3)
                        else:
                            # í´ë°±: ê¸°ë³¸ ë²¡í„°ìŠ¤í† ì–´ ì‚¬ìš©
                            print(f"[DEBUG] í´ë°±: ê¸°ë³¸ ë²¡í„°ìŠ¤í† ì–´ ì‚¬ìš©")
                            search_results = chain.vectorstore_manager.similarity_search_with_score(question, k=3)
                        
                        print(f"[DEBUG] {process_name} ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ, ì²«ë²ˆì§¸ ì ìˆ˜: {search_results[0][1]:.2%}" if search_results else "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                        
                        if not search_results:
                            print(f"[WARNING] {process_name} - No search results found")
                            result_queue.put({
                                'type': 'process_error',
                                'process_name': process_name,
                                'process_id': process_id,
                                'session_id': session_id,
                                'error': 'no_results',
                                'message': 'ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.',
                                'total_time': round(time.time() - start_time, 2),
                                'status': 'failed',
                                'similarity_info': []
                            })
                            return
                        
                        # ê° ìˆœìœ„ë³„ë¡œ ê°œë³„ ë‹µë³€ ìƒì„±
                        rank_answers = []
                        
                        # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ìœ ì‚¬ë„ë³„ë¡œ ì²˜ë¦¬ (Top 3)
                        for rank, (doc, score) in enumerate(search_results[:3], 1):
                            try:
                                # ê°œë³„ ë¬¸ì„œì— ëŒ€í•œ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„± (chunking optimization)
                                # Truncate overly long documents to reduce LLM input length
                                max_doc_length = 2000  # Optimal length per document
                                if len(doc.page_content) > max_doc_length:
                                    individual_context = doc.page_content[:max_doc_length-3] + "..."
                                    print(f"[CHUNKING OPT] Document truncated from {len(doc.page_content)} to {len(individual_context)} chars")
                                else:
                                    individual_context = doc.page_content
                                # ìš”ì•½ ëª¨ë“œì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì¡°ì •
                                if summarize:
                                    individual_prompt = f"""ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì—¬ ì œê³µí•˜ì„¸ìš”.
í•µì‹¬ ë‚´ìš©ë§Œ í¬í•¨í•˜ê³  ë¶ˆí•„ìš”í•œ ì„¸ë¶€ì‚¬í•­ì€ ì œì™¸í•˜ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{individual_context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
                                else:
                                    individual_prompt = chain.prompt_template.format(context=individual_context, question=question)
                                
                                answer_text = ""
                                
                                if llm_type == "openai":
                                    # OpenAI API í˜¸ì¶œ (íƒ€ì„ì•„ì›ƒ 30ì´ˆ)
                                    client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), timeout=30.0)
                                    stream = client.chat.completions.create(
                                        model=llm_model,
                                        messages=[{"role": "user", "content": individual_prompt}],
                                        stream=True,
                                        temperature=0.1
                                    )
                                    
                                    # ì „ì²´ ë‹µë³€ì„ ìˆ˜ì§‘
                                    for chunk in stream:
                                        if chunk.choices[0].delta.content is not None:
                                            answer_text += chunk.choices[0].delta.content
                                
                                else:
                                    # ë¡œì»¬ LLM ì²˜ë¦¬
                                    llm_manager = LLMManager()
                                    llm = llm_manager.get_llm(model_name='local')  # ë¡œì»¬ LLM ì‚¬ìš©
                                    full_response = llm.invoke(individual_prompt)
                                    
                                    # vLLM ì‘ë‹µì—ì„œ content ì¶”ì¶œ
                                    if hasattr(full_response, 'content'):
                                        answer_text = full_response.content
                                    elif isinstance(full_response, str):
                                        answer_text = full_response
                                    else:
                                        answer_text = str(full_response)
                                
                                # ì™„ì„±ëœ ë‹µë³€ì„ ì €ì¥
                                rank_answers.append({
                                    'rank': rank,
                                    'score': f'{score:.1%}',
                                    'answer': answer_text,
                                    'source': doc.metadata.get('source_file', 'Unknown')
                                })
                                
                            except Exception as doc_error:
                                rank_answers.append({
                                    'rank': rank,
                                    'score': f'{score:.1%}',
                                    'answer': f'ì˜¤ë¥˜ ë°œìƒ: {str(doc_error)}',
                                    'source': doc.metadata.get('source_file', 'Unknown')
                                })
                        
                        # í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ - ìˆœìœ„ë³„ ë‹µë³€ë“¤ê³¼ ì‹œê°„ ì •ë³´ ì „ì†¡
                        end_time = time.time()
                        total_time = end_time - start_time
                        
                        # ìœ ì‚¬ë„ Top 3 ì •ë³´ ìƒì„±
                        similarity_info = []
                        for i, (doc, score) in enumerate(search_results[:3], 1):
                            similarity_info.append({
                                'rank': i,
                                'score': f'{score:.1%}',
                                'source': doc.metadata.get('source_file', 'Unknown'),
                                'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content
                            })
                        
                        print(f"[SUCCESS] Process {process_name} completed successfully")
                        print(f"[SUCCESS] Generated {len(rank_answers)} answers")
                        
                        result_queue.put({
                            'type': 'process_complete',
                            'process_name': process_name,
                            'process_id': process_id,
                            'session_id': session_id,
                            'rank_answers': rank_answers,
                            'similarity_info': similarity_info,
                            'total_time': round(total_time, 2),
                            'status': 'success',
                            'chunking_type': process_info["chunking"]  # chunking_type ì¶”ê°€
                        })
                        print(f"[SUCCESS] Complete event queued for process {process_id}")
                        
                    except Exception as e:
                        end_time = time.time()
                        total_time = end_time - start_time
                        print(f"[ERROR] Process {process_info['name']} failed: {str(e)}")
                        import traceback
                        print(f"[ERROR] Traceback: {traceback.format_exc()}")
                        
                        result_queue.put({
                            'type': 'process_error',
                            'process_name': process_info["name"],
                            'process_id': process_info["process_id"],
                            'session_id': session_id,
                            'error': str(e),
                            'message': f'í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}',
                            'total_time': round(total_time, 2),
                            'status': 'failed',
                            'similarity_info': []  # ì˜¤ë¥˜ ì‹œ ë¹ˆ ìœ ì‚¬ë„ ì •ë³´
                        })
                        print(f"[ERROR] Error event queued for process {process_info['process_id']}")
                
                # ì„¸ì…˜ ID ìƒì„±
                session_id = str(uuid.uuid4())
                
                # 4ê°œ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì•Œë¦¼
                yield f"data: {json.dumps({'type': 'all_processes_start', 'total_processes': len(processes), 'session_id': session_id})}\n\n"
                
                # 4ê°œ í”„ë¡œì„¸ìŠ¤ë¥¼ ë³‘ë ¬ë¡œ ì‹œì‘
                with ThreadPoolExecutor(max_workers=4) as executor:
                    futures = []
                    
                    for process_info in processes:
                        print(f"[DEBUG] Starting process: {process_info['name']} (ID: {process_info['process_id']})")
                        future = executor.submit(
                            process_single_search_task,
                            process_info, question, llm_model, chain, session_id, summarize
                        )
                        futures.append(future)
                    
                    print(f"[DEBUG] All {len(futures)} processes submitted")
                    
                    # ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° (ê°œë³„ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ì¦‰ì‹œ í‘œì‹œ)
                    completed_processes = 0
                    total_processes = len(processes)
                    
                    while completed_processes < total_processes:
                        try:
                            # íì—ì„œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒì„ ì§§ê²Œ ì„¤ì •í•˜ì—¬ ì‹¤ì‹œê°„ì„± í–¥ìƒ)
                            result = result_queue.get(timeout=0.05)
                            
                            # ê²°ê³¼ ì¦‰ì‹œ ìŠ¤íŠ¸ë¦¬ë° (ê° í”„ë¡œì„¸ìŠ¤ê°€ ì™„ë£Œë˜ëŠ” ìˆœê°„ ë°”ë¡œ í™”ë©´ì— í‘œì‹œ)
                            yield f"data: {json.dumps(result)}\n\n"
                            
                            # ì™„ë£Œëœ í”„ë¡œì„¸ìŠ¤ ì¹´ìš´íŠ¸
                            if result['type'] in ['process_complete', 'process_error']:
                                completed_processes += 1
                                print(f"[STREAM] í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ: {result.get('process_name', 'Unknown')} ({completed_processes}/{total_processes})")
                                
                        except Empty:
                            # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì•„ì£¼ ì§§ì€ ì‹œê°„ ëŒ€ê¸° í›„ ê³„ì† í´ë§
                            time.sleep(0.01)
                            continue
                    
                    # ëª¨ë“  futures ì •ë¦¬
                    for future in as_completed(futures, timeout=30):
                        try:
                            future.result()
                        except Exception as e:
                            print(f"í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ì˜¤ë¥˜: {e}")
                
                # ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€
                yield f"data: {json.dumps({'type': 'all_complete', 'message': 'ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ'})}\n\n"
                
                yield "data: [DONE]\n\n"
                
            except Exception as e:
                yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
                yield "data: [DONE]\n\n"
        
        return Response(
            stream_with_context(generate()),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Headers': 'Content-Type',
                'X-Accel-Buffering': 'no'  # Disable nginx buffering
            }
        )
    
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@chat_bp.route('/chatgpt-basic', methods=['POST'])
def chatgpt_basic():
    """ChatGPT + s3ê¸°ë³¸ ê°œë³„ ì²˜ë¦¬"""
    try:
        data = request.get_json()
        question = data.get('question')
        
        if not question:
            return jsonify({"error": "Question is required"}), 400
        
        chain = get_rag_chain()
        
        # ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” í™•ì¸
        chain._initialize_vectorstore()
        
        # ê¸°ë³¸ ê²€ìƒ‰ ìˆ˜í–‰
        import time
        start_time = time.time()
        
        try:
            if hasattr(chain, 'dual_vectorstore_manager') and chain.dual_vectorstore_manager:
                search_results = chain.dual_vectorstore_manager.similarity_search_with_score(question, "basic", k=5)
            else:
                search_results = chain.vectorstore_manager.similarity_search_with_score(question, k=5)
            
            # s3 í´ë” ë¬¸ì„œë§Œ í•„í„°ë§ (s3-chunking ì œì™¸)
            s3_results = [(doc, score) for doc, score in search_results if doc.metadata.get('source') == 's3']
            
            if not s3_results:
                return jsonify({
                    'success': False,
                    'answer': 's3 í´ë”ì˜ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. s3 í´ë”ì—ì„œ ë¬¸ì„œë¥¼ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.',
                    'similarity_info': [],
                    'total_time': 0.0,
                    'process_name': 'ChatGPT + s3ê¸°ë³¸',
                    'chunking_type': 'basic'
                })
            
            # ì²« ë²ˆì§¸ ê²°ê³¼ë¡œ ìµœì í™”ëœ ë‹µë³€ ìƒì„± (chunking optimization)
            if s3_results:
                doc, score = s3_results[0]
                # Optimize context length for better LLM performance
                max_context_length = 2500  # Optimal length for single document processing
                if len(doc.page_content) > max_context_length:
                    context = doc.page_content[:max_context_length-3] + "..."
                    print(f"[CHUNKING OPT] ChatGPT-basic context truncated from {len(doc.page_content)} to {len(context)} chars")
                else:
                    context = doc.page_content
                prompt = chain.prompt_template.format(context=context, question=question)
                
                # OpenAI API í˜¸ì¶œ
                from openai import OpenAI
                import os
                client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), timeout=30.0)
                stream = client.chat.completions.create(
                    model='gpt-4o-mini',
                    messages=[{"role": "user", "content": prompt}],
                    stream=True,
                    temperature=0.1
                )
                
                answer_text = ""
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        answer_text += chunk.choices[0].delta.content
                
                # ìœ ì‚¬ë„ ì •ë³´ ìƒì„± (s3 ê²°ê³¼ë§Œ ì‚¬ìš©)
                similarity_info = []
                for i, (doc, score) in enumerate(s3_results[:3], 1):
                    similarity_info.append({
                        'rank': i,
                        'score': f'{score:.1%}',
                        'source': doc.metadata.get('source', 's3'),
                        'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content
                    })
                
                end_time = time.time()
                total_time = end_time - start_time
                
                return jsonify({
                    'success': True,
                    'answer': answer_text,
                    'similarity_info': similarity_info,
                    'total_time': round(total_time, 2),
                    'process_name': 'ChatGPT + s3ê¸°ë³¸',
                    'chunking_type': 'basic'
                })
            else:
                return jsonify({
                    'success': False,
                    'answer': 'ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                    'similarity_info': [],
                    'total_time': 0.0,
                    'process_name': 'ChatGPT + s3ê¸°ë³¸',
                    'chunking_type': 'basic'
                })
                
        except Exception as e:
            end_time = time.time()
            total_time = end_time - start_time
            return jsonify({
                'success': False,
                'answer': f'ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
                'similarity_info': [],
                'total_time': round(total_time, 2),
                'process_name': 'ChatGPT + s3ê¸°ë³¸',
                'chunking_type': 'basic'
            })
            
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@chat_bp.route('/chatgpt-custom', methods=['POST'])
def chatgpt_custom():
    """ChatGPT + s3-chunking ê°œë³„ ì²˜ë¦¬"""
    try:
        data = request.get_json()
        question = data.get('question')
        
        if not question:
            return jsonify({"error": "Question is required"}), 400
        
        chain = get_rag_chain()
        
        # ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” í™•ì¸
        chain._initialize_vectorstore()
        
        # ì»¤ìŠ¤í…€ ê²€ìƒ‰ ìˆ˜í–‰
        import time
        start_time = time.time()
        
        try:
            if hasattr(chain, 'dual_vectorstore_manager') and chain.dual_vectorstore_manager:
                search_results = chain.dual_vectorstore_manager.similarity_search_with_score(question, "custom", k=3)
            else:
                search_results = chain.vectorstore_manager.similarity_search_with_score(question, k=3)
            
            # ì²« ë²ˆì§¸ ê²°ê³¼ë¡œ ìµœì í™”ëœ ë‹µë³€ ìƒì„± (chunking optimization)
            if search_results:
                doc, score = search_results[0]
                # Optimize context length for better LLM performance  
                max_context_length = 2500  # Optimal length for single document processing
                if len(doc.page_content) > max_context_length:
                    context = doc.page_content[:max_context_length-3] + "..."
                    print(f"[CHUNKING OPT] ChatGPT-custom context truncated from {len(doc.page_content)} to {len(context)} chars")
                else:
                    context = doc.page_content
                prompt = chain.prompt_template.format(context=context, question=question)
                
                # OpenAI API í˜¸ì¶œ
                from openai import OpenAI
                import os
                client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), timeout=30.0)
                stream = client.chat.completions.create(
                    model='gpt-4o-mini',
                    messages=[{"role": "user", "content": prompt}],
                    stream=True,
                    temperature=0.1
                )
                
                answer_text = ""
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        answer_text += chunk.choices[0].delta.content
                
                # ìœ ì‚¬ë„ ì •ë³´ ìƒì„±
                similarity_info = []
                for i, (doc, score) in enumerate(search_results[:3], 1):
                    similarity_info.append({
                        'rank': i,
                        'score': f'{score:.1%}',
                        'source': doc.metadata.get('source_file', 'Unknown'),
                        'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content
                    })
                
                end_time = time.time()
                total_time = end_time - start_time
                
                return jsonify({
                    'success': True,
                    'answer': answer_text,
                    'similarity_info': similarity_info,
                    'total_time': round(total_time, 2),
                    'process_name': 'ChatGPT + s3-chunking',
                    'chunking_type': 'custom'
                })
            else:
                return jsonify({
                    'success': False,
                    'answer': 'ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                    'similarity_info': [],
                    'total_time': 0.0,
                    'process_name': 'ChatGPT + s3-chunking',
                    'chunking_type': 'custom'
                })
                
        except Exception as e:
            end_time = time.time()
            total_time = end_time - start_time
            return jsonify({
                'success': False,
                'answer': f'ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
                'similarity_info': [],
                'total_time': round(total_time, 2),
                'process_name': 'ChatGPT + s3-chunking',
                'chunking_type': 'custom'
            })
            
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@chat_bp.route('/local-basic', methods=['POST'])
def local_basic():
    """ë¡œì»¬LLM + s3ê¸°ë³¸ ê°œë³„ ì²˜ë¦¬"""
    try:
        data = request.get_json()
        question = data.get('question')
        session_id = data.get('session_id', str(uuid.uuid4()))
        
        if not question:
            return jsonify({'error': 'ì§ˆë¬¸ì´ í•„ìš”í•©ë‹ˆë‹¤'}), 400
        
        def generate_response():
            try:
                start_time = time.time()
                
                # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰ (basic ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰ - s3 ë¬¸ì„œë§Œ)
                from models.dual_vectorstore import get_dual_vectorstore
                dual_vectorstore = get_dual_vectorstore()
                search_results = dual_vectorstore.similarity_search_with_score(question, "basic", k=5)
                
                if not search_results:
                    yield f"data: {json.dumps({'type': 'process_error', 'process_name': 'ë¡œì»¬LLM + s3ê¸°ë³¸', 'process_id': 3, 'session_id': session_id, 'error': 'no_documents', 'message': 'ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. s3 í´ë”ì—ì„œ ë¬¸ì„œë¥¼ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.', 'similarity_info': [], 'total_time': 0.0, 'status': 'failed'}, ensure_ascii=False)}\n\n"
                    return

                # basic ì»¬ë ‰ì…˜ì—ëŠ” ì´ë¯¸ s3 ë¬¸ì„œë§Œ ìˆìœ¼ë¯€ë¡œ í•„í„°ë§ ë¶ˆí•„ìš”
                s3_results = search_results
                
                if not s3_results:
                    yield f"data: {json.dumps({'type': 'process_error', 'process_name': 'ë¡œì»¬LLM + s3ê¸°ë³¸', 'process_id': 3, 'session_id': session_id, 'error': 'no_s3_documents', 'message': 's3 í´ë”ì˜ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. s3 í´ë”ì—ì„œ ë¬¸ì„œë¥¼ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.', 'similarity_info': [], 'total_time': 0.0, 'status': 'failed'}, ensure_ascii=False)}\n\n"
                    return

                # ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ë§¤ìš° ì§§ê²Œ, ë¡œì»¬ LLM í† í° ì œí•œ)
                s3_context = ""
                max_context_length = 200  # ë” ì§§ê²Œ ì œí•œ
                for doc, score in s3_results[:2]:  # 2ê°œë§Œ ì‚¬ìš©
                    content = doc.page_content[:max_context_length]
                    s3_context += f"{content}\n\n"
                    if len(s3_context) > 400:  # ë§¤ìš° ì§§ì€ ì „ì²´ ê¸¸ì´
                        break

                # ë¡œì»¬ LLM í˜¸ì¶œ (ë§¤ìš° ì§§ì€ í”„ë¡¬í”„íŠ¸)
                from models.llm import LLMManager
                llm_manager = LLMManager()
                local_llm = llm_manager.get_llm(model_name='local')
                
                prompt = f"""ë‹¤ìŒ ì •ë³´ë¡œ ë‹µë³€: {s3_context[:300]}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

                try:
                    response = local_llm.invoke(prompt)
                    answer = str(response) if response else 'ë¡œì»¬ LLM ì‘ë‹µ ì—†ìŒ'
                except Exception as e:
                    # ë¡œì»¬ LLM ì—ëŸ¬ ì‹œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°„ë‹¨ ë‹µë³€
                    answer = f"ê²€ìƒ‰ëœ ì •ë³´: {s3_context[:200]}... (ë¡œì»¬ LLM ì„œë²„ ì—ëŸ¬: {str(e)[:50]})"
                
                end_time = time.time()
                total_time = end_time - start_time
                
                # ìœ ì‚¬ë„ ì •ë³´ ìƒì„± (s3 ê²°ê³¼ë§Œ ì‚¬ìš©)
                similarity_info = []
                for i, (doc, score) in enumerate(s3_results[:3], 1):
                    similarity_info.append({
                        'rank': i,
                        'score': f'{score:.1%}',
                        'source': doc.metadata.get('source', 's3'),
                        'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content
                    })
                
                yield f"data: {json.dumps({'type': 'process_complete', 'process_name': 'ë¡œì»¬LLM + s3ê¸°ë³¸', 'process_id': 3, 'session_id': session_id, 'answer': answer, 'similarity_info': similarity_info, 'total_time': total_time, 'status': 'success', 'chunking_type': 'basic'}, ensure_ascii=False)}\n\n"
                
            except Exception as e:
                print(f"ë¡œì»¬LLM ê¸°ë³¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                yield f"data: {json.dumps({'type': 'process_error', 'process_name': 'ë¡œì»¬LLM + s3ê¸°ë³¸', 'process_id': 3, 'session_id': session_id, 'error': 'connection_error', 'message': f'ë¡œì»¬ LLM ì—°ê²° ì˜¤ë¥˜: {str(e)}. ë¡œì»¬ LLM ì„œë²„ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”.', 'similarity_info': [], 'total_time': 0.0, 'status': 'failed'}, ensure_ascii=False)}\n\n"

        return Response(
            generate_response(),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'X-Accel-Buffering': 'no'
            }
        )
        
    except Exception as e:
        return jsonify({
            'error': f'ë¡œì»¬LLM + s3ê¸°ë³¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}',
            'status': 'failed'
        }), 500

@chat_bp.route('/local-custom', methods=['POST'])
def local_custom():
    """ë¡œì»¬LLM + s3-chunking ê°œë³„ ì²˜ë¦¬"""
    try:
        data = request.get_json()
        question = data.get('question')
        session_id = data.get('session_id', str(uuid.uuid4()))
        
        if not question:
            return jsonify({'error': 'ì§ˆë¬¸ì´ í•„ìš”í•©ë‹ˆë‹¤'}), 400
        
        def generate_response():
            try:
                start_time = time.time()
                
                # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
                vectorstore_manager = VectorStoreManager()
                search_results = vectorstore_manager.similarity_search(question, k=5)
                
                if not search_results:
                    yield f"data: {json.dumps({'type': 'process_error', 'process_name': 'ë¡œì»¬LLM + s3-chunking', 'process_id': 4, 'session_id': session_id, 'error': 'no_documents', 'message': 'ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. s3-chunking í´ë”ì—ì„œ ë¬¸ì„œë¥¼ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.', 'similarity_info': [], 'total_time': 0.0, 'status': 'failed'}, ensure_ascii=False)}\n\n"
                    return

                # s3-chunking ë¬¸ì„œ í•„í„°ë§
                s3_chunking_results = [(doc, score) for doc, score in search_results if doc.metadata.get('source') == 's3-chunking']
                
                if not s3_chunking_results:
                    yield f"data: {json.dumps({'type': 'process_error', 'process_name': 'ë¡œì»¬LLM + s3-chunking', 'process_id': 4, 'session_id': session_id, 'error': 'no_chunking_documents', 'message': 's3-chunking í´ë”ì˜ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.', 'similarity_info': [], 'total_time': 0.0, 'status': 'failed'}, ensure_ascii=False)}\n\n"
                    return

                # ì»¨í…ìŠ¤íŠ¸ ìƒì„± (s3-chunking ë¬¸ì„œë§Œ ì‚¬ìš©, ê¸¸ì´ ì œí•œ)
                context = ""
                max_context_length = 500  # ë¡œì»¬ LLMì˜ í† í° ì œí•œ ê³ ë ¤
                for doc, score in s3_chunking_results[:3]:
                    content = doc.page_content
                    if len(content) > max_context_length:
                        content = content[:max_context_length] + "..."
                    context += f"[ìœ ì‚¬ë„: {score:.1%}] {content}\n\n"
                    if len(context) > 1000:  # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
                        break

                # ë¡œì»¬ LLM í˜¸ì¶œ (LocalLLM í´ë˜ìŠ¤ ì‚¬ìš©)
                from models.llm import LLMManager
                llm_manager = LLMManager()
                local_llm = llm_manager.get_llm(model_name='local')
                
                prompt = f"""ë‹¤ìŒ ì •ë³´ë¡œ ë‹µë³€: {context[:300]}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

                try:
                    response = local_llm.invoke(prompt)
                    answer = str(response) if response else 'ë¡œì»¬ LLM ì‘ë‹µ ì—†ìŒ'
                except Exception as e:
                    # ë¡œì»¬ LLM ì—ëŸ¬ ì‹œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°„ë‹¨ ë‹µë³€
                    answer = f"ê²€ìƒ‰ëœ ì •ë³´: {context[:200]}... (ë¡œì»¬ LLM ì„œë²„ ì—ëŸ¬: {str(e)[:50]})"
                
                end_time = time.time()
                total_time = end_time - start_time
                
                # ìœ ì‚¬ë„ ì •ë³´ ìƒì„±
                similarity_info = []
                for i, (doc, score) in enumerate(s3_chunking_results[:3], 1):
                    similarity_info.append({
                        'rank': i,
                        'score': f'{score:.1%}',
                        'source': doc.metadata.get('source', 's3-chunking'),
                        'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content
                    })
                
                yield f"data: {json.dumps({'type': 'process_complete', 'process_name': 'ë¡œì»¬LLM + s3-chunking', 'process_id': 4, 'session_id': session_id, 'answer': answer, 'similarity_info': similarity_info, 'total_time': total_time, 'status': 'success', 'chunking_type': 'custom'}, ensure_ascii=False)}\n\n"
                
            except Exception as e:
                print(f"ë¡œì»¬LLM ì»¤ìŠ¤í…€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                yield f"data: {json.dumps({'type': 'process_error', 'process_name': 'ë¡œì»¬LLM + s3-chunking', 'process_id': 4, 'session_id': session_id, 'error': 'connection_error', 'message': f'ë¡œì»¬ LLM ì—°ê²° ì˜¤ë¥˜: {str(e)}. ë¡œì»¬ LLM ì„œë²„ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”.', 'similarity_info': [], 'total_time': 0.0, 'status': 'failed'}, ensure_ascii=False)}\n\n"

        return Response(
            generate_response(),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'X-Accel-Buffering': 'no'
            }
        )
        
    except Exception as e:
        return jsonify({
            'error': f'ë¡œì»¬LLM + s3-chunking ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}',
            'status': 'failed'
        }), 500

@chat_bp.route('/vllm-dual-stream', methods=['POST'])
def vllm_dual_stream():
    """vLLM ë‘ ì˜µì…˜ ë™ì‹œ ìŠ¤íŠ¸ë¦¬ë° (ì‚¬ë‚´ì„œë²„ vLLM + s3ê¸°ë³¸, ì‚¬ë‚´ì„œë²„ vLLM + s3-chunking)"""
    try:
        print(f"ğŸŒ [REQUEST] vLLM ë“€ì–¼ ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ ë°›ìŒ")
        print(f"ğŸ“‹ [REQUEST] Content-Type: {request.content_type}")
        print(f"ğŸ“‹ [REQUEST] Method: {request.method}")
        
        data = request.get_json()
        print(f"ğŸ“Š [REQUEST] ë°›ì€ ë°ì´í„°: {data}")
        
        if not data:
            print("âŒ [REQUEST] JSON ë°ì´í„°ê°€ ì—†ìŒ")
            return jsonify({"error": "JSON data is required"}), 400
        
        question = data.get('question') or data.get('query')
        selected_model = data.get('local_model', 'kanana8b')  # í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì„ íƒí•œ ëª¨ë¸
        print(f"â“ [REQUEST] ì§ˆë¬¸: {question}")
        print(f"ğŸ¤– [REQUEST] ì„ íƒëœ ëª¨ë¸: {selected_model}")
        
        if not question:
            print("âŒ [REQUEST] ì§ˆë¬¸ì´ ì—†ìŒ")
            return jsonify({"error": "Question or query is required"}), 400
        
        def generate():
            import time
            import uuid
            from concurrent.futures import ThreadPoolExecutor, as_completed
            from queue import Queue, Empty
            
            # ì„¸ì…˜ ID ìƒì„±
            session_id = str(uuid.uuid4())
            
            # 2ê°œ vLLM í”„ë¡œì„¸ìŠ¤ ì •ì˜
            processes = [
                {"llm_type": "local", "chunking": "basic", "name": "ì‚¬ë‚´ì„œë²„ vLLM + s3ê¸°ë³¸", "process_id": 3},
                {"llm_type": "local", "chunking": "custom", "name": "ì‚¬ë‚´ì„œë²„ vLLM + s3-chunking", "process_id": 4}
            ]
            
            result_queue = Queue()
            
            def vllm_process_task(process_info, question, session_id, selected_model):
                """vLLM ì²˜ë¦¬ íƒœìŠ¤í¬ - ë¡œê¹… ì¶”ê°€"""
                start_time = time.time()
                process_name = process_info["name"]
                process_id = process_info["process_id"]
                chunking_type = process_info["chunking"]
                
                print(f"ğŸš€ [vLLM {process_id}] {process_name} ì‹œì‘")
                print(f"ğŸ“Š [vLLM {process_id}] ì§ˆë¬¸: {question[:100]}...")
                print(f"ğŸ”§ [vLLM {process_id}] ì²­í‚¹ íƒ€ì…: {chunking_type}")
                
                # ê¹€ëª…ì • ë“± ì‚¬ìš©ì ê°ì§€ ë° ì¹´ë“œ ì •ë³´ ì²˜ë¦¬
                enhanced_question = question
                card_summary = None
                is_personalized = False
                
                user_patterns = ['ê¹€ëª…ì •', 'ì´ì˜í¬', 'ë°•ì² ìˆ˜', 'ìµœì˜ìˆ˜']
                detected_user = None
                
                for user in user_patterns:
                    if user in question:
                        detected_user = user
                        break
                
                if detected_user:
                    print(f"ğŸ” [vLLM {process_id}] {detected_user} ê³ ê° ê°ì§€")
                    card_keywords = ['ì¹´ë“œ', 'ë°œê¸‰', 'íšŒì›ì€í–‰', 'ì€í–‰ë³„']
                    
                    if any(keyword in question for keyword in card_keywords):
                        print(f"ğŸ’³ [vLLM {process_id}] ì¹´ë“œ ê´€ë ¨ ì§ˆì˜ ê°ì§€ - ë™ì  ì²˜ë¦¬")
                        # ê°œì¸í™” í”Œë˜ê·¸ ì„¤ì • (ì¤‘ìš”: MCP ì„±ê³µ ì „ì— ë¯¸ë¦¬ ì„¤ì •)
                        is_personalized = True
                        print(f"âœ… [vLLM {process_id}] ê°œì¸í™” ëª¨ë“œ í™œì„±í™”: {detected_user} ê³ ê°")
                        
                        # MCP ì¹´ë“œ ë¶„ì„ ì‹œìŠ¤í…œ ê°•í™” (ëª¨ë“  ì²­í‚¹ íƒ€ì…ì—ì„œ ë™ì‘)
                        try:
                            print(f"ğŸ” [vLLM {process_id}] MCP ì¹´ë“œ ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘")
                            
                            # CardAnalysisService ì‚¬ìš©
                            from services.card_analysis_service import CardAnalysisService
                            card_service = CardAnalysisService()
                            
                            # ê¹€ëª…ì • ì¹´ë“œ ì •ë³´ ë¶„ì„
                            analysis = card_service.analyze_customer_cards(detected_user)
                            card_summary = card_service.format_analysis_response(analysis)
                            
                            print(f"ğŸ“Š [vLLM {process_id}] ì¹´ë“œ ë¶„ì„ ê²°ê³¼:")
                            print(f"   - ë³´ìœ  ì¹´ë“œ: {len(analysis.owned_cards)}ì¥")
                            print(f"   - ì¶”ì²œ ì¹´ë“œ: {len(analysis.recommended_cards)}ì¥") 
                            print(f"   - ë°œê¸‰ ê°€ëŠ¥: {len(analysis.available_cards)}ì¥")
                            
                            # ì¹´ë“œ ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
                            owned_card_images = []
                            for card in analysis.owned_cards:
                                if 'ìš°ë¦¬ì¹´ë“œ' in card.name:
                                    owned_card_images.append('![\uc6b0\ub9ac\uce74\ub4dc \ub85c\uace0](/images/Aspose.Words.4c2a2064-0c7c-48d5-aca6-c4d7a6eade2b.014.gif)')
                                elif 'í•˜ë‚˜ì¹´ë“œ' in card.name:
                                    owned_card_images.append('![\ud558\ub098\uce74\ub4dc \ub85c\uace0](/images/Aspose.Words.4c2a2064-0c7c-48d5-aca6-c4d7a6eade2b.016.gif)')
                                elif 'NHì¹´ë“œ' in card.name or 'ë†í˜‘' in card.name:
                                    owned_card_images.append('![\ub18d\ud611\uce74\ub4dc \ub85c\uace0](/images/Aspose.Words.4c2a2064-0c7c-48d5-aca6-c4d7a6eade2b.017.gif)')
                            
                            # ë°œê¸‰ ê°€ëŠ¥ ì¹´ë“œ ì´ë¯¸ì§€
                            available_card_images = []
                            for card in analysis.available_cards + analysis.recommended_cards:
                                if 'BCì¹´ë“œ' in card.name:
                                    available_card_images.append('![BC\uce74\ub4dc \ub85c\uace0](/images/Aspose.Words.4c2a2064-0c7c-48d5-aca6-c4d7a6eade2b.004.gif)')
                                elif 'ì‹ í•œì¹´ë“œ' in card.name:
                                    available_card_images.append('![\uc2e0\ud55c\uce74\ub4dc \ub85c\uace0](/images/Aspose.Words.4c2a2064-0c7c-48d5-aca6-c4d7a6eade2b.005.gif)')
                                elif 'êµ­ë¯¼ì¹´ë“œ' in card.name or 'KB' in card.name:
                                    available_card_images.append('![\uad6d\ubbfc\uce74\ub4dc \ub85c\uace0](/images/Aspose.Words.4c2a2064-0c7c-48d5-aca6-c4d7a6eade2b.006.jpeg)')
                                elif 'ë¡¯ë°ì¹´ë“œ' in card.name:
                                    available_card_images.append('![\ub86f\ub370\uce74\ub4dc \ub85c\uace0](/images/Aspose.Words.4c2a2064-0c7c-48d5-aca6-c4d7a6eade2b.007.jpeg)')
                                elif 'ì‚¼ì„±ì¹´ë“œ' in card.name:
                                    available_card_images.append('![\uc0bc\uc131\uce74\ub4dc \ub85c\uace0](/images/Aspose.Words.4c2a2064-0c7c-48d5-aca6-c4d7a6eade2b.008.jpeg)')
                            
                            # ì¹´ë“œ ë°œê¸‰ ì ˆì°¨ ì´ë¯¸ì§€
                            process_image = '![\uce74\ub4dc\ubc1c\uae09 \uc808\ucc28](/images/Aspose.Words.4c2a2064-0c7c-48d5-aca6-c4d7a6eade2b.013.gif)'
                            
                            enhanced_question = f"""
ì§ˆë¬¸: {question}

=== {detected_user} ê³ ê° ì¹´ë“œ ë¶„ì„ ê²°ê³¼ ===
{card_summary}

í˜„ì¬ ë³´ìœ í•˜ì‹  ì¹´ë“œ ì´ë¯¸ì§€:
{chr(10).join(owned_card_images) if owned_card_images else 'ë³´ìœ í•œ ì¹´ë“œê°€ ì—†ìŠµë‹ˆë‹¤.'}

ë°œê¸‰ ê°€ëŠ¥í•œ ì¹´ë“œ ì´ë¯¸ì§€:
{chr(10).join(available_card_images[:3])}

ì¹´ë“œ ë°œê¸‰ ì ˆì°¨:
{process_image}

ìœ„ ë¶„ì„ ê²°ê³¼ì™€ ì´ë¯¸ì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ {detected_user} ê³ ê°ì—ê²Œ ë§ì¶¤í˜• ì¹´ë“œ ë°œê¸‰ ì•ˆë‚´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
í˜„ì¬ ë³´ìœ í•˜ì‹  ì¹´ë“œì™€ ìƒˆë¡œ ë°œê¸‰ ê°€ëŠ¥í•œ ì¹´ë“œë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ì•ˆë‚´í•´ì£¼ì„¸ìš”.
ë‹µë³€ì— ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ì—¬ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
                            print(f"âœ… [vLLM {process_id}] MCP ê°œì¸í™” ì§ˆì˜ ìƒì„± ì™„ë£Œ: {len(enhanced_question)}ì")
                            print(f"ğŸ¯ [vLLM {process_id}] ê°œì¸í™” ì „ìš© í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ì˜ˆì •")
                            
                        except Exception as e:
                            print(f"âŒ [vLLM {process_id}] MCP ì¹´ë“œ ë¶„ì„ ì˜¤ë¥˜: {e}")
                            import traceback
                            print(f"ğŸ” [vLLM {process_id}] ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
                            enhanced_question = question
                
                try:
                    # í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì•Œë¦¼
                    result_queue.put({
                        'type': 'process_start',
                        'process_name': process_name,
                        'process_id': process_id,
                        'session_id': session_id
                    })
                    
                    # vLLM ì—°ê²° í™•ì¸
                    from models.llm import LLMManager
                    print(f"ğŸ”Œ [vLLM {process_id}] LLM ë§¤ë‹ˆì € ìƒì„± ì¤‘...")
                    llm_manager = LLMManager()
                    
                    print(f"ğŸ¤– [vLLM {process_id}] vLLM ì—°ê²° ì‹œë„: 192.168.0.224:8412")
                    vllm_llm = llm_manager.get_vllm_llm()
                    print(f"âœ… [vLLM {process_id}] vLLM ì—°ê²° ì„±ê³µ")
                    
                    # ë¬¸ì„œ ê²€ìƒ‰
                    print(f"ğŸ” [vLLM {process_id}] ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘...")
                    chain = get_rag_chain()
                    
                    # ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” í™•ì¸
                    chain._initialize_vectorstore()
                    
                    if chunking_type == "basic":
                        # s3ê¸°ë³¸: DualVectorStoreì˜ basic ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰
                        if hasattr(chain, 'dual_vectorstore_manager') and chain.dual_vectorstore_manager:
                            print(f"ğŸ“š [vLLM {process_id}] basic ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰")
                            search_results = chain.dual_vectorstore_manager.similarity_search_with_score(question, "basic", k=10)  # ë” ë§ì€ ê²°ê³¼
                        else:
                            print(f"âš ï¸ [vLLM {process_id}] í´ë°±: ê¸°ë³¸ ë²¡í„°ìŠ¤í† ì–´ ì‚¬ìš©")
                            search_query = enhanced_question if is_personalized else question
                            search_results = chain.vectorstore_manager.similarity_search_with_score(search_query, k=10)
                    else:
                        # s3-chunking: ë‹¤ì–‘í•œ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•˜ì—¬ ì¹´ë“œ ìƒì„¸ ì •ë³´ í¬í•¨
                        print(f"ğŸ“š [vLLM {process_id}] custom ì»¬ë ‰ì…˜ì—ì„œ í™•ì¥ ê²€ìƒ‰ ì‹œì‘")
                        
                        # 1ì°¨: ê¸°ë³¸ ê²€ìƒ‰ (ê°œì¸í™”ëœ ê²½ìš° enhanced_question ì‚¬ìš©)
                        search_query = enhanced_question if is_personalized else question
                        basic_search = chain.dual_vectorstore_manager.similarity_search_with_score(search_query, "custom", k=5)
                        
                        # 2ì°¨: ì¹´ë“œ ë°œê¸‰ ìƒì„¸ ì •ë³´ë¥¼ ìœ„í•œ í‚¤ì›Œë“œ ê²€ìƒ‰
                        detailed_keywords = [
                            "ì¹´ë“œë°œê¸‰ ì ˆì°¨",
                            "ë°œê¸‰ëŒ€ìƒ",
                            "ì‹ ì²­ë°©ë²• êµ¬ë¹„ì„œë¥˜",
                            "íšŒì›ì€í–‰ ì˜ì—…ì ",
                            "ì¹´ë“œ ì‹¬ì‚¬",
                            "ê²°ì œëŠ¥ë ¥ ì‹¬ì‚¬",
                            "BCì¹´ë“œë°œê¸‰ì•ˆë‚´",
                            f"{detected_user} ê³ ê° í˜„í™©",
                            "ë³´ìœ  ì¹´ë“œ ë¯¸ë³´ìœ  ì¹´ë“œ",
                            "ì¹´ë“œ ì´ë¯¸ì§€"
                        ]
                        
                        additional_results = []
                        for keyword in detailed_keywords:
                            try:
                                keyword_results = chain.dual_vectorstore_manager.similarity_search_with_score(keyword, "custom", k=3)
                                additional_results.extend(keyword_results)
                                print(f"ğŸ”‘ [vLLM {process_id}] '{keyword}' ê²€ìƒ‰: {len(keyword_results)}ê°œ ê²°ê³¼")
                            except Exception as e:
                                print(f"âš ï¸ [vLLM {process_id}] '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                                continue
                        
                        # 3ì°¨: s3ê¸°ë³¸ ì»¬ë ‰ì…˜ì—ì„œë„ ì¹´ë“œ ë°œê¸‰ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (í¬ë¡œìŠ¤ ê²€ìƒ‰)
                        s3_basic_results = []
                        try:
                            s3_basic_results = chain.dual_vectorstore_manager.similarity_search_with_score("ì¹´ë“œë°œê¸‰ ì ˆì°¨ ì‹ ì²­ë°©ë²•", "basic", k=5)
                            print(f"ğŸ”„ [vLLM {process_id}] s3ê¸°ë³¸ì—ì„œ í¬ë¡œìŠ¤ ê²€ìƒ‰: {len(s3_basic_results)}ê°œ ê²°ê³¼")
                            additional_results.extend(s3_basic_results)
                        except Exception as e:
                            print(f"âš ï¸ [vLLM {process_id}] s3ê¸°ë³¸ í¬ë¡œìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                        
                        # ê²°ê³¼ í•©ì¹˜ê¸° ë° ì¤‘ë³µ ì œê±°
                        all_results = basic_search + additional_results
                        seen_content = set()
                        search_results = []
                        
                        for doc, score in all_results:
                            content_hash = hash(doc.page_content[:100])  # ì²« 100ìë¡œ ì¤‘ë³µ ì²´í¬
                            if content_hash not in seen_content:
                                search_results.append((doc, score))
                                seen_content.add(content_hash)
                        
                        # ì ìˆ˜ìˆœ ì •ë ¬
                        search_results.sort(key=lambda x: x[1], reverse=True)
                        search_results = search_results[:15]  # ìƒìœ„ 15ê°œë¡œ í™•ëŒ€
                        
                        print(f"ğŸ” [vLLM {process_id}] í™•ì¥ ê²€ìƒ‰ ì™„ë£Œ: ê¸°ë³¸ {len(basic_search)}ê°œ + í‚¤ì›Œë“œ {len(additional_results)-len(s3_basic_results)}ê°œ + s3ê¸°ë³¸ {len(s3_basic_results)}ê°œ â†’ ìµœì¢… {len(search_results)}ê°œ")
                    
                    print(f"ğŸ“Š [vLLM {process_id}] ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ")
                    if search_results:
                        print(f"ğŸ¯ [vLLM {process_id}] ìµœê³  ìœ ì‚¬ë„: {search_results[0][1]:.2%}")
                    
                    if not search_results:
                        result_queue.put({
                            'type': 'process_error',
                            'process_name': process_name,
                            'process_id': process_id,
                            'session_id': session_id,
                            'error': 'no_results',
                            'message': 'ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.',
                            'similarity_info': [],
                            'total_time': round(time.time() - start_time, 2),
                            'status': 'failed'
                        })
                        return
                    
                    # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì¹´ë“œ ìƒì„¸ ì •ë³´ ìš°ì„  í¬í•¨)
                    context = ""
                    max_doc_length = 3000  # ì¹´ë“œ ì´ë¯¸ì§€ì™€ ìƒì„¸ ì •ë³´ë¥¼ ìœ„í•´ ë” í™•ëŒ€
                    included_docs = 0
                    
                    # ìš°ì„ ìˆœìœ„ 1: ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ë¬¸ì„œ ë¨¼ì € í¬í•¨
                    image_docs = []
                    other_docs = []
                    
                    for doc, score in search_results:
                        if '![' in doc.page_content and '](' in doc.page_content:  # ì´ë¯¸ì§€ ë¬¸ë²•ì´ í¬í•¨ëœ ë¬¸ì„œ
                            image_docs.append((doc, score))
                        else:
                            other_docs.append((doc, score))
                    
                    # ì´ë¯¸ì§€ê°€ ìˆëŠ” ë¬¸ì„œ ë¨¼ì € í¬í•¨
                    for doc, score in (image_docs + other_docs):
                        if included_docs >= 8:  # ìµœëŒ€ 8ê°œ ë¬¸ì„œê¹Œì§€
                            break
                            
                        doc_content = doc.page_content
                        if len(doc_content) > max_doc_length:
                            doc_content = doc_content[:max_doc_length] + "..."
                        
                        context += f"[ìœ ì‚¬ë„: {score:.1%}] {doc_content}\n\n"
                        included_docs += 1
                        
                        if len(context) > 10000:  # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ë” í™•ëŒ€ (ì´ë¯¸ì§€ ì •ë³´ í¬í•¨)
                            break
                    
                    print(f"ğŸ“ [vLLM {process_id}] ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±: ì´ë¯¸ì§€ ë¬¸ì„œ {len(image_docs)}ê°œ, ì¼ë°˜ ë¬¸ì„œ {len(other_docs)}ê°œ, ì´ {included_docs}ê°œ í¬í•¨")
                    
                    print(f"ğŸ“ [vLLM {process_id}] ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)}ì")
                    
                    # vLLM í˜¸ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ (ê°œì¸í™” ëŒ€ì‘)
                    print(f"ğŸ¤– [vLLM {process_id}] í”„ë¡¬í”„íŠ¸ ì„ íƒ: is_personalized={is_personalized}, detected_user={detected_user}")
                    
                    if is_personalized and detected_user:
                        # ê°œì¸í™”ëœ ì¹´ë“œ ë°œê¸‰ ì „ìš© í”„ë¡¬í”„íŠ¸
                        print(f"ğŸ† [vLLM {process_id}] {detected_user} ê³ ê° ì „ìš© ê°œì¸í™” í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
                        prompt = f"""ë‹¹ì‹ ì€ BCì¹´ë“œ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. {detected_user} ê³ ê°ì—ê²Œ ë§ì¶¤í˜• ì¹´ë“œ ë°œê¸‰ ì•ˆë‚´ë¥¼ ì œê³µí•˜ì„¸ìš”.

=== ì œê³µëœ ìë£Œ ===
{context}

=== {detected_user} ê³ ê° ê°œì¸í™” ì •ë³´ ===
{enhanced_question}

**ë‹µë³€ ì‘ì„± ê°€ì´ë“œ:**
1. **ì œëª© í‘œì‹œ**: "**1. í˜„ì¬ ë³´ìœ  ì¤‘ì¸ ì¹´ë“œ**", "**2. ë°œê¸‰ ì¶”ì²œ ì¹´ë“œ**" í˜•ì‹ìœ¼ë¡œ êµµê²Œ í‘œì‹œ (### ë§ˆí¬ë‹¤ìš´ í—¤ë” ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€)
2. ë³´ìœ  ì¹´ë“œëŠ” "âœ… í˜„ì¬ ë³´ìœ ì¤‘" ìœ¼ë¡œ í‘œì‹œ
3. ì¶”ì²œ ì¹´ë“œëŠ” "â­ ë°œê¸‰ ì¶”ì²œ" ìœ¼ë¡œ í‘œì‹œ  
4. ë°œê¸‰ ê°€ëŠ¥ ì¹´ë“œëŠ” "ğŸ†• ë°œê¸‰ê°€ëŠ¥" ìœ¼ë¡œ í‘œì‹œ
5. ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ![\uce74\ub4dc\uc774\ub984](\uc774\ubbf8\uc9c0\ud30c\uc77c) í˜•ì‹ìœ¼ë¡œ í¬í•¨
6. **ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš©**: ì œê³µëœ ìë£Œì— ìˆëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ê³ , ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì„ì˜ë¡œ ìƒì„±í•˜ì§€ ë§ ê²ƒ
7. BCì¹´ë“œ ë°œê¸‰ ì ˆì°¨ëŠ” ì œê³µëœ ìë£Œì— ìˆëŠ” ê²½ìš°ë§Œ í¬í•¨
8. **ê°€ë…ì„± ê°œì„ **: ê° ì„¹ì…˜ ì‚¬ì´ì— ì ì ˆí•œ ê³µë°± í¬í•¨í•˜ì—¬ ì½ê¸° í¸í•˜ê²Œ êµ¬ì„±
9. **ì¤‘ìš”**: ë§ˆí¬ë‹¤ìš´ ### í—¤ë” ëŒ€ì‹  **êµµì€ ê¸€ì”¨**ë¡œ ì œëª©ì„ í‘œì‹œí•˜ì—¬ ì›¹ì—ì„œ ê¹”ë”í•˜ê²Œ ë³´ì´ë„ë¡ í•  ê²ƒ
10. **í—ˆìœ„ì •ë³´ ê¸ˆì§€**: VIP í˜œíƒ, íŠ¹ë³„ ìš°ëŒ€ ë“± ì œê³µëœ ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ ê²ƒ

ë‹µë³€:"""
                    else:
                        # ì¼ë°˜ í”„ë¡¬í”„íŠ¸
                        print(f"ğŸ—ï¸ [vLLM {process_id}] ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
                        prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:

{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
                    
                    print(f"ğŸ¤– [vLLM {process_id}] vLLM í˜¸ì¶œ ì‹œì‘...")
                    print(f"ğŸŒ [vLLM {process_id}] ì„œë²„: 192.168.0.224:8412")
                    print(f"ğŸ”§ [vLLM {process_id}] ëª¨ë¸: {selected_model}")
                    
                    # ì„ íƒëœ ëª¨ë¸ë¡œ vLLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                    from langchain_openai import ChatOpenAI
                    config = Config.LLM_MODELS['local']
                    custom_vllm = ChatOpenAI(
                        model=selected_model,  # ì„ íƒëœ ëª¨ë¸ ì‚¬ìš©
                        openai_api_base=config['base_url'] + '/v1',
                        openai_api_key='EMPTY',
                        temperature=config['temperature'],
                        max_tokens=config['max_tokens']
                    )
                    
                    # ì»¤ìŠ¤í…€ vLLM í˜¸ì¶œ
                    response = custom_vllm.invoke(prompt)
                    print(f"ğŸ“¨ [vLLM {process_id}] vLLM ì‘ë‹µ ë°›ìŒ")
                    
                    # ì‘ë‹µ ì²˜ë¦¬
                    if hasattr(response, 'content'):
                        answer = response.content
                    elif isinstance(response, str):
                        answer = response
                    else:
                        answer = str(response)
                    
                    print(f"âœ… [vLLM {process_id}] ë‹µë³€ ìƒì„± ì™„ë£Œ: {len(answer)}ì")
                    
                    # ìœ ì‚¬ë„ ì •ë³´ ìƒì„±
                    similarity_info = []
                    for i, (doc, score) in enumerate(search_results[:3], 1):
                        # ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì´ë¯¸ 0-1 ë²”ìœ„ì˜ cosine similarityë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        normalized_score = max(0.0, min(1.0, score))  # 0-1 ë²”ìœ„ë¡œ í´ë¨í•‘ë§Œ ìˆ˜í–‰
                        
                        similarity_info.append({
                            'rank': i,
                            'score': f'{normalized_score:.2%}',  # ì†Œìˆ˜ì  2ìë¦¬ë¡œ í‘œì‹œ
                            'source': doc.metadata.get('source_file', doc.metadata.get('source', 'Unknown')),
                            'content_preview': doc.page_content[:100] + '...' if len(doc.page_content) > 100 else doc.page_content
                        })
                    
                    end_time = time.time()
                    total_time = end_time - start_time
                    print(f"â±ï¸ [vLLM {process_id}] ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
                    
                    # RAG ì‘ë‹µì—ì„œ ì´ë¯¸ì§€ ê²½ë¡œ ìë™ ì¶”ì¶œ
                    extracted_images = extract_images_from_context(context, answer)
                    print(f"ğŸ–¼ï¸ [vLLM {process_id}] ì¶”ì¶œëœ ì´ë¯¸ì§€: {len(extracted_images)}ê°œ")
                    
                    result_queue.put({
                        'type': 'process_complete',
                        'process_name': process_name,
                        'process_id': process_id,
                        'session_id': session_id,
                        'answer': answer,
                        'similarity_info': similarity_info,
                        'total_time': round(total_time, 2),
                        'status': 'success',
                        'chunking_type': chunking_type,
                        'images': extracted_images  # ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
                    })
                    
                    print(f"ğŸ‰ [vLLM {process_id}] í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
                    
                except Exception as e:
                    end_time = time.time()
                    total_time = end_time - start_time
                    error_msg = str(e)
                    print(f"âŒ [vLLM {process_id}] ì˜¤ë¥˜ ë°œìƒ: {error_msg}")
                    import traceback
                    print(f"ğŸ” [vLLM {process_id}] ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
                    
                    # vLLM íŠ¹í™” ì—ëŸ¬ ë©”ì‹œì§€
                    if "connection" in error_msg.lower() or "connect" in error_msg.lower():
                        user_message = f"vLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨ (192.168.0.224:8412). ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
                    elif "timeout" in error_msg.lower():
                        user_message = f"vLLM ì‘ë‹µ ì‹œê°„ ì´ˆê³¼. ì„œë²„ ë¶€í•˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
                    else:
                        user_message = f"vLLM ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {error_msg[:100]}"
                    
                    result_queue.put({
                        'type': 'process_error',
                        'process_name': process_name,
                        'process_id': process_id,
                        'session_id': session_id,
                        'error': 'vllm_error',
                        'message': user_message,
                        'similarity_info': [],
                        'total_time': round(total_time, 2),
                        'status': 'failed'
                    })
            
            # ì‹œì‘ ì•Œë¦¼
            yield f"data: {json.dumps({'type': 'all_processes_start', 'total_processes': 2, 'session_id': session_id}, ensure_ascii=False)}\n\n"
            print(f"ğŸš€ [MAIN] vLLM ë“€ì–¼ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ - ì„¸ì…˜: {session_id}")
            
            # 2ê°œ í”„ë¡œì„¸ìŠ¤ ë³‘ë ¬ ì‹¤í–‰
            with ThreadPoolExecutor(max_workers=2) as executor:
                futures = []
                for process_info in processes:
                    future = executor.submit(vllm_process_task, process_info, question, session_id, selected_model)
                    futures.append(future)
                    print(f"ğŸ”„ [MAIN] {process_info['name']} ìŠ¤ë ˆë“œ ì‹œì‘")
                
                # ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°
                completed_processes = 0
                total_processes = 2
                
                while completed_processes < total_processes:
                    try:
                        result = result_queue.get(timeout=0.1)
                        yield f"data: {json.dumps(result, ensure_ascii=False)}\n\n"
                        
                        if result['type'] in ['process_complete', 'process_error']:
                            completed_processes += 1
                            print(f"âœ… [MAIN] í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ: {result.get('process_name', 'Unknown')} ({completed_processes}/{total_processes})")
                            
                    except Empty:
                        time.sleep(0.01)
                        continue
                
                # futures ì •ë¦¬
                for future in as_completed(futures, timeout=60):
                    try:
                        future.result()
                    except Exception as e:
                        print(f"âŒ [MAIN] ìŠ¤ë ˆë“œ ì™„ë£Œ ì˜¤ë¥˜: {e}")
            
            yield f"data: {json.dumps({'type': 'all_complete', 'message': 'vLLM ë“€ì–¼ ì²˜ë¦¬ ì™„ë£Œ'}, ensure_ascii=False)}\n\n"
            yield "data: [DONE]\n\n"
            print(f"ğŸ [MAIN] ëª¨ë“  vLLM í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ")
        
        return Response(
            stream_with_context(generate()),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Headers': 'Content-Type',
                'X-Accel-Buffering': 'no'
            }
        )
    
    except Exception as e:
        print(f"âŒ [MAIN] vLLM ë“€ì–¼ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
        return jsonify({"error": str(e)}), 500

@chat_bp.route('/clear-memory', methods=['POST'])
def clear_memory():
    """Clear conversation memory"""
    try:
        chain = get_rag_chain()
        chain._initialize_vectorstore()
        chain.clear_memory()
        return jsonify({"message": "Memory cleared successfully"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500