from flask import Blueprint, jsonify, request
from services.enhanced_rag_chain import EnhancedRAGChain
from services.chunking_strategies import get_chunking_strategy, benchmark_chunking_strategies
from models.vectorstore import get_vectorstore, get_dual_vectorstore
from models.dual_llm import DualLLMManager
from config import Config
import time
import json
import os
from datetime import datetime

unified_bp = Blueprint('unified', __name__)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ë“¤
enhanced_rag_chain = None
similarity_cache = {}

def get_enhanced_rag_chain():
    """Enhanced RAG Chain ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    global enhanced_rag_chain
    if enhanced_rag_chain is None:
        dual_vectorstore = get_dual_vectorstore()
        enhanced_rag_chain = EnhancedRAGChain(dual_vectorstore)
    return enhanced_rag_chain

@unified_bp.route('/unified-query', methods=['POST'])
def unified_query():
    """í†µí•© ë²¤ì¹˜ë§ˆí‚¹ ì§ˆì˜ ì²˜ë¦¬"""
    try:
        data = request.json
        query = data.get('query', '').strip()
        mode = data.get('mode', '')
        summarize = data.get('summarize', False)
        
        if not query:
            return jsonify({'error': 'ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.'}), 400
        
        if not mode:
            return jsonify({'error': 'ë²¤ì¹˜ë§ˆí‚¹ ëª¨ë“œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.'}), 400
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰ ë° ì§ˆë¬¸ ì²˜ë¦¬ í”Œë¡œìš°
        result = process_query_with_similarity_check(query, mode, summarize)
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({'error': f'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'}), 500

def process_query_with_similarity_check(query, mode, summarize, local_model=None):
    """ìœ ì‚¬ë„ ê¸°ì¤€ ì§ˆë¬¸ ì²˜ë¦¬ í”Œë¡œìš°"""
    
    try:
        print(f"[UNIFIED] process_query_with_similarity_check ì‹œì‘")
        print(f"[UNIFIED] ì…ë ¥ íŒŒë¼ë¯¸í„° - mode: {mode}, query: '{query[:50]}...', summarize: {summarize}, local_model: {local_model}")
        
        # 1. ì§ˆë¬¸ ì „ì²˜ë¦¬ (LLMì„ í†µí•œ ì§ˆë¬¸ ê°€ê³µ)
        print(f"[UNIFIED] 1ë‹¨ê³„: ì§ˆë¬¸ ì „ì²˜ë¦¬ ì‹œì‘")
        processed_query = preprocess_question(query)
        print(f"[UNIFIED] 1ë‹¨ê³„ ì™„ë£Œ: ì „ì²˜ë¦¬ëœ ì¿¼ë¦¬ '{processed_query[:50]}...'")
    
        # 2. ë²¡í„° DB ìœ ì‚¬ë„ ê²€ìƒ‰ (ëª¨ë“œë³„ ì²­í‚¹ íƒ€ì… ë¶„ë¦¬)
        print(f"[UNIFIED] 2ë‹¨ê³„: ë²¡í„° DB ê²€ìƒ‰ ì‹œì‘")
        rag_chain = get_enhanced_rag_chain()
        print(f"[UNIFIED] EnhancedRAGChain ì¸ìŠ¤í„´ìŠ¤ íšë“ ì™„ë£Œ")
        
        # ëª¨ë“œì— ë”°ë¥¸ ì²­í‚¹ íƒ€ì… ê²°ì •
        if 'basic' in mode:
            chunking_type = 'basic'
        elif 'custom' in mode:
            chunking_type = 'custom'
        else:
            chunking_type = None  # dual_search ì‚¬ìš©
        
        print(f"[UNIFIED] ì²­í‚¹ íƒ€ì… ê²°ì •: {chunking_type} (ëª¨ë“œ: {mode})")
        print(f"[UNIFIED] _search_documents í˜¸ì¶œ ì‹œì‘: query='{processed_query}', k=5, chunking_type={chunking_type}")
        
        search_results = rag_chain._search_documents(processed_query, 5, chunking_type)
        
        print(f"[UNIFIED] 2ë‹¨ê³„ ì™„ë£Œ: ë²¡í„° DB ê²€ìƒ‰ ê²°ê³¼ {len(search_results) if search_results else 0}ê°œ")
        if search_results:
            for i, (doc, score) in enumerate(search_results[:3]):
                print(f"[UNIFIED] ê²€ìƒ‰ê²°ê³¼ {i+1}: score={score:.3f}, ë‚´ìš©='{doc.page_content[:100]}...'")
        else:
            print(f"[UNIFIED] âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤!")
        
        if not search_results:
            print(f"[UNIFIED] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return {
                'type': 'single',
                'answer': 'ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'response_time': 0,
                'model': mode
            }
        
        # 3. ìœ ì‚¬ë„ ë¶„ì„ ë° ì¤‘ë³µ ì œê±° (Top 3 ì¶”ì¶œ)
        print(f"[UNIFIED] 3ë‹¨ê³„: ìœ ì‚¬ë„ ë¶„ì„ ì‹œì‘")
        unique_results = []
        seen_content = set()
        
        for i, (doc, score) in enumerate(search_results[:5]):
            # ë‚´ìš© ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ (ì²« 100ìë¡œ ë¹„êµ)
            content_key = doc.page_content[:100].strip()
            print(f"[UNIFIED] ê²€ìƒ‰ê²°ê³¼ {i+1}: ì ìˆ˜={score:.3f}, ë‚´ìš©={content_key[:50]}...")
            if content_key not in seen_content:
                unique_results.append((doc, score))
                seen_content.add(content_key)
                print(f"[UNIFIED] â†’ ê³ ìœ  ê²°ê³¼ë¡œ ì¶”ê°€ë¨")
            else:
                print(f"[UNIFIED] â†’ ì¤‘ë³µìœ¼ë¡œ ì œì™¸ë¨")
            
            # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ
            if len(unique_results) >= 3:
                break
        
        top_3_results = unique_results
        high_similarity_results = [
            (doc, score) for doc, score in top_3_results if score >= 0.45
        ]
        print(f"[UNIFIED] 3ë‹¨ê³„ ì™„ë£Œ: ê³ ìœ ì‚¬ë„ ê²°ê³¼ {len(high_similarity_results)}ê°œ (0.45 ì´ìƒ)")
        print(f"[UNIFIED] ì „ì²´ ìœ ë‹ˆí¬ ê²°ê³¼: {len(top_3_results)}ê°œ")
        
        # 4. ìœ ì‚¬ë„ ê¸°ì¤€ ë¶„ê¸°
        print(f"[UNIFIED] 4ë‹¨ê³„: ìœ ì‚¬ë„ ê¸°ì¤€ ë¶„ê¸° ì‹œì‘")
        if high_similarity_results:
            # 0.45 ì´ìƒì¸ ê²½ìš° ë°”ë¡œ ë‹µë³€ ìƒì„±
            print(f"[UNIFIED] ê³ ìœ ì‚¬ë„ ê²½ë¡œ: generate_answer_for_mode í˜¸ì¶œ")
            print(f"[UNIFIED] ë‹µë³€ ìƒì„± íŒŒë¼ë¯¸í„°: query='{query[:30]}...', results={len(high_similarity_results)}ê°œ, mode={mode}")
            
            result = generate_answer_for_mode(query, high_similarity_results, mode, summarize, local_model)
            
            print(f"[UNIFIED] ë‹µë³€ ìƒì„± ì™„ë£Œ: type={result.get('type', 'unknown')}")
            return result
        else:
            # 0.45 ë¯¸ë§Œì¸ ê²½ìš° ì„ íƒ ì˜µì…˜ ì œê³µ
            print(f"[UNIFIED] ì €ìœ ì‚¬ë„ ê²½ë¡œ: ì„ íƒ ì˜µì…˜ ì œê³µ")
            global similarity_cache
            cache_key = f"{query}_{mode}_{int(time.time())}"
            similarity_cache[cache_key] = {
                'query': query,
                'results': top_3_results,
                'mode': mode,
                'summarize': summarize
            }
            
            options = []
            for i, (doc, score) in enumerate(top_3_results):
                options.append({
                    'similarity': score,
                    'preview': doc.page_content[:100],
                    'index': i
                })
            
            print(f"[UNIFIED] ì„ íƒ ì˜µì…˜ ìƒì„± ì™„ë£Œ: {len(options)}ê°œ ì˜µì…˜")
            return {
                'type': 'similarity_choice',
                'options': options,
                'cache_key': cache_key
            }
        
    except Exception as e:
        import traceback
        print(f"[UNIFIED ERROR] process_query_with_similarity_check ì˜¤ë¥˜: {str(e)}")
        print(f"[UNIFIED ERROR] íŠ¸ë ˆì´ìŠ¤ë°±:")
        traceback.print_exc()
        
        return {
            'type': 'single',
            'answer': f'ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}',
            'response_time': 0,
            'model': mode,
            'success': False,
            'error': str(e)
        }

def preprocess_question(query):
    """LLMì„ í†µí•œ ì§ˆë¬¸ ì „ì²˜ë¦¬ - ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œë§Œ ìˆ˜í–‰"""
    try:
        # ì›ë³¸ ì¿¼ë¦¬ê°€ ì¶©ë¶„íˆ ê°„ë‹¨í•œ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if len(query) <= 30:
            print(f"[PREPROCESS] ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©: {query}")
            return query
            
        rag_chain = get_enhanced_rag_chain()
        if rag_chain.dual_llm.get_available_models()['api']:
            api_chain = rag_chain.dual_llm.get_api_chain()
            
            prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•˜ì—¬ 5ë‹¨ì–´ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
            
            ì›ë³¸ ì§ˆë¬¸: {query}
            
            í‚¤ì›Œë“œ:"""
            
            response = api_chain.invoke({"question": prompt, "context": ""})
            processed = ""
            if hasattr(response, 'content'):
                processed = response.content.strip()
            else:
                processed = str(response).strip()
            
            # ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ë©´ ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
            if len(processed) > 50:
                print(f"[PREPROCESS] ì „ì²˜ë¦¬ ê²°ê³¼ê°€ ë„ˆë¬´ ê¸¸ì–´ì„œ ì›ë³¸ ì‚¬ìš©: {query}")
                return query
            
            # ë¹ˆ ì‘ë‹µì´ë©´ ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
            if not processed:
                print(f"[PREPROCESS] ì „ì²˜ë¦¬ ê²°ê³¼ê°€ ë¹„ì–´ì„œ ì›ë³¸ ì‚¬ìš©: {query}")
                return query
                
            print(f"[PREPROCESS] í‚¤ì›Œë“œ ì¶”ì¶œ: '{query}' â†’ '{processed}'")
            return processed
        else:
            print(f"[PREPROCESS] API ì—†ì–´ì„œ ì›ë³¸ ì‚¬ìš©: {query}")
            return query
    except Exception as e:
        print(f"[PREPROCESS] ì˜¤ë¥˜ë¡œ ì¸í•´ ì›ë³¸ ì‚¬ìš©: {query}, ì˜¤ë¥˜: {e}")
        return query

@unified_bp.route('/similarity-choice', methods=['POST'])
def similarity_choice():
    """ìœ ì‚¬ë„ ì„ íƒ ì²˜ë¦¬"""
    try:
        data = request.json
        choice = data.get('choice', 0)
        cache_key = data.get('cache_key', '')
        
        global similarity_cache
        if cache_key not in similarity_cache:
            return jsonify({'error': 'ì„ íƒ ì„¸ì…˜ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤.'}), 400
        
        cached_data = similarity_cache[cache_key]
        selected_result = [cached_data['results'][choice]]
        
        result = generate_answer_for_mode(
            cached_data['query'],
            selected_result,
            cached_data['mode'],
            cached_data['summarize']
        )
        
        # ìºì‹œ ì •ë¦¬
        del similarity_cache[cache_key]
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({'error': f'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'}), 500

def generate_answer_for_mode(query, results, mode, summarize, local_model=None):
    """ëª¨ë“œë³„ ë‹µë³€ ìƒì„±"""
    
    start_time = time.time()
    print(f"[ANSWER_GEN] generate_answer_for_mode ì‹œì‘")
    print(f"[ANSWER_GEN] íŒŒë¼ë¯¸í„°: query='{query[:30]}...', results={len(results)}ê°œ, mode={mode}, summarize={summarize}")
    
    # ëª¨ë“œ ë¶„ì„
    llm_type, chunking_type = mode.split('-')  # ì˜ˆ: 'chatgpt-basic'
    print(f"[ANSWER_GEN] ëª¨ë“œ ë¶„ì„: {mode} â†’ LLM={llm_type}, ì²­í‚¹={chunking_type}")
    
    # ìœ ì‚¬ë„ ì ìˆ˜ ì¶”ì¶œ (Top 3)
    similarity_scores = []
    for i, (doc, score) in enumerate(results[:3]):
        similarity_scores.append({
            'rank': i + 1,
            'score': round(score, 3),
            'content_preview': doc.page_content[:100] + '...' if doc.page_content else ''
        })
    print(f"[ANSWER_GEN] ìœ ì‚¬ë„ ì ìˆ˜ ì¶”ì¶œ ì™„ë£Œ: {len(similarity_scores)}ê°œ")
    
    # ì²­í‚¹ ì „ëµ ì ìš© (s3 vs s3-chunking í´ë” êµ¬ë¶„)
    print(f"[ANSWER_GEN] ì²­í‚¹ ì „ëµ ì ìš© ì‹œì‘")
    processed_results = results
    if chunking_type == 'custom':
        # s3-chunking ì»¤ìŠ¤í…€ ì²­í‚¹ ì ìš©
        print(f"[ANSWER_GEN] apply_s3_custom_chunking í˜¸ì¶œ")
        processed_results = apply_s3_custom_chunking(results)
        print(f"[ANSWER_GEN] s3-chunking ì»¤ìŠ¤í…€ ì²­í‚¹ ì ìš©: {len(results)} -> {len(processed_results)} ì²­í¬")
    else:
        # s3 ê¸°ë³¸ ì²­í‚¹ ì ìš©
        print(f"[ANSWER_GEN] apply_s3_basic_chunking í˜¸ì¶œ")
        processed_results = apply_s3_basic_chunking(results)
        print(f"[ANSWER_GEN] s3 ê¸°ë³¸ ì²­í‚¹ ì‚¬ìš©: {len(results)} -> {len(processed_results)} ì²­í¬")
    
    # LLM ì„ íƒ ë° ì‹¤í–‰
    print(f"[ANSWER_GEN] LLM ì‹¤í–‰ ì‹œì‘: {llm_type}")
    if llm_type == 'chatgpt':
        print(f"[ANSWER_GEN] execute_chatgpt_query í˜¸ì¶œ")
        result = execute_chatgpt_query(query, processed_results, summarize, chunking_type)
        print(f"[ANSWER_GEN] execute_chatgpt_query ì™„ë£Œ")
    else:  # local
        print(f"[ANSWER_GEN] execute_local_query í˜¸ì¶œ")
        result = execute_local_query(query, processed_results, summarize, chunking_type, local_model)
        print(f"[ANSWER_GEN] execute_local_query ì™„ë£Œ")
    
    # ë²¤ì¹˜ë§ˆí‚¹ ì ìˆ˜ ê³„ì‚°
    benchmark_score = calculate_benchmark_score(result, time.time() - start_time)
    result['benchmark_score'] = benchmark_score
    result['mode'] = mode
    result['chunking_type'] = chunking_type
    result['chunks_used'] = len(processed_results)
    result['similarity_scores'] = similarity_scores
    result['timestamp'] = datetime.now().isoformat()
    
    # ì¡°íšŒ íšŸìˆ˜ ê¸°ë¡ (Redis/RDB ì •ì±…)
    record_query_usage(query, result)
    
    return {
        'type': 'single',
        **result
    }

def execute_chatgpt_query(query, results, summarize, chunking_type):
    """ChatGPT APIë¡œ ì§ˆì˜ ì‹¤í–‰"""
    try:
        print(f"[CHATGPT] execute_chatgpt_query ì‹œì‘")
        print(f"[CHATGPT] íŒŒë¼ë¯¸í„°: query='{query[:30]}...', results={len(results)}ê°œ, summarize={summarize}, chunking_type={chunking_type}")
        
        rag_chain = get_enhanced_rag_chain()
        print(f"[CHATGPT] EnhancedRAGChain ì¸ìŠ¤í„´ìŠ¤ íšë“ ì™„ë£Œ")
        
        api_chain = rag_chain.dual_llm.get_api_chain()
        print(f"[CHATGPT] API ì²´ì¸ íšë“ ì™„ë£Œ")
        
        context = rag_chain._format_context([doc for doc, score in results])
        print(f"[CHATGPT] ì»¨í…ìŠ¤íŠ¸ í¬ë§· ì™„ë£Œ: {len(context)} ê¸€ì")
        
        # ì²­í‚¹ ì „ëµì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì¡°ì •
        chunking_info = ""
        if chunking_type == 'custom':
            chunking_info = " (ì»¤ìŠ¤í…€ ì²­í‚¹ ì „ëµìœ¼ë¡œ ì²˜ë¦¬ëœ ë¬¸ì„œ ê¸°ë°˜)"
        else:
            chunking_info = " (ê¸°ë³¸ ì²­í‚¹ ì „ëµìœ¼ë¡œ ì²˜ë¦¬ëœ ë¬¸ì„œ ê¸°ë°˜)"
        
        print(f"[CHATGPT] ì²­í‚¹ ì •ë³´: {chunking_info}")
        
        # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ - ì²´ê³„ì ì´ê³  ì •ë¦¬ëœ ë‹µë³€ ìƒì„±
        if summarize:
            system_prompt = f"""ë‹¹ì‹ ì€ BCì¹´ë“œ ì—…ë¬´ì²˜ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤{chunking_info}.

ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”:

1. **í•µì‹¬ ì ˆì°¨ë‚˜ ë‚´ìš©ì„ ë‹¨ê³„ë³„ë¡œ ì •ë¦¬** (ë²ˆí˜¸ë‚˜ ë‹¨ê³„ ì‚¬ìš©)
2. **ì¤‘ìš”í•œ ì£¼ì˜ì‚¬í•­ì´ë‚˜ ì¶”ê°€ ì •ë³´** í¬í•¨
3. **ê´€ë ¨ ì¡°ê±´ì´ë‚˜ ìš”êµ¬ì‚¬í•­** ëª…ì‹œ
4. í•„ìš”ì‹œ **í‘œ í˜•ì‹**ìœ¼ë¡œ ì •ë³´ ì •ë¦¬

ë‹µë³€ì€ ëª…í™•í•˜ê³  ì‹¤ë¬´ì— ë„ì›€ì´ ë˜ë„ë¡ êµ¬ì„±í•˜ì„¸ìš”."""
        else:
            system_prompt = f"""ë‹¹ì‹ ì€ BCì¹´ë“œ ì—…ë¬´ì²˜ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤{chunking_info}.

ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì²´ê³„ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”:

**ë‹µë³€ êµ¬ì¡°:**
1. **ê°œìš”**: ì§ˆë¬¸ì— ëŒ€í•œ ê°„ë‹¨í•œ ì†Œê°œ
2. **ìƒì„¸ ì ˆì°¨**: ë‹¨ê³„ë³„ë¡œ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ì„œ ì„¤ëª… (1), 2), 3)... í˜•ì‹)
3. **ì¤‘ìš” í¬ì¸íŠ¸**: ì£¼ì˜ì‚¬í•­, ì¡°ê±´, ìš”êµ¬ì‚¬í•­ ë“±ì„ ëª…ì‹œ
4. **ì¶”ê°€ ì•ˆë‚´**: ê´€ë ¨ ì •ë³´ë‚˜ ì°¸ê³ ì‚¬í•­
5. **í‘œ í˜•ì‹**: ë³µì¡í•œ ì •ë³´ëŠ” í‘œë¡œ ì •ë¦¬ (| êµ¬ë¶„ì ì‚¬ìš©)

**ì‘ì„± ì›ì¹™:**
- ì‹¤ë¬´ì§„ì´ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±
- ë‹¨ê³„ë³„ ì ˆì°¨ëŠ” ëª…í™•í•œ ìˆœì„œë¡œ ì œì‹œ
- ì¤‘ìš”í•œ ë‚´ìš©ì€ **êµµê²Œ** í‘œì‹œ
- ì°¸ì¡° ë¬¸ì„œëª…ì„ ì–¸ê¸‰í•˜ì—¬ ì‹ ë¢°ì„± í™•ë³´

ë‹µë³€ì€ ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš”."""
        
        print(f"[LLM_LOG] ğŸ¤– ChatGPT API í˜¸ì¶œ ì‹œì‘: OpenAI GPT-4")
        start_time = time.time()
        response = api_chain.invoke({
            "question": query,
            "context": context
        })
        response_time = time.time() - start_time
        print(f"[LLM_LOG] âœ… ChatGPT API ì‘ë‹µ ì™„ë£Œ: {response_time:.2f}ì´ˆ, {len(str(response))}ì")
        
        answer = response.content if hasattr(response, 'content') else str(response)
        print(f"[CHATGPT] ì‘ë‹µ ì²˜ë¦¬ ì™„ë£Œ: {len(answer)} ê¸€ì")
        
        result = {
            'answer': answer,
            'response_time': response_time,
            'model': f'ChatGPT API ({chunking_type} ì²­í‚¹)',
            'estimated_tokens': len(answer.split()) * 1.3,
            'sources': [doc.metadata.get('source', 'Unknown') for doc, score in results],
            'success': True
        }
        
        print(f"[CHATGPT] ê²°ê³¼ ìƒì„± ì™„ë£Œ: success={result['success']}")
        return result
        
    except Exception as e:
        # ìƒì„¸ ì˜¤ë¥˜ ë¡œê¹…
        import traceback
        print(f"[ChatGPT ERROR] ìƒì„¸ ì˜¤ë¥˜: {str(e)}")
        print(f"[ChatGPT ERROR] íŠ¸ë ˆì´ìŠ¤ë°±: {traceback.format_exc()}")
        
        # ì˜¤ë¥˜ ì›ì¸ ë¶„ì„
        error_reason = 'unknown_error'
        error_detail = str(e)
        
        if 'api key' in str(e).lower() or 'unauthorized' in str(e).lower():
            error_reason = 'api_key_error'
            error_detail = 'ChatGPT API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. .env íŒŒì¼ì˜ OPENAI_API_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”.'
        elif 'rate limit' in str(e).lower():
            error_reason = 'rate_limit'
            error_detail = 'API ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.'
        elif 'timeout' in str(e).lower():
            error_reason = 'timeout'
            error_detail = 'ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.'
        
        return {
            'answer': f'ChatGPT API ì˜¤ë¥˜: {error_detail}',
            'response_time': 0,
            'model': f'ChatGPT API (Error - {chunking_type} ì²­í‚¹)',
            'estimated_tokens': 0,
            'sources': [],
            'success': False,
            'error_reason': error_reason,
            'error_detail': str(e),
            'timestamp': datetime.now().isoformat()
        }

def execute_local_query(query, results, summarize, chunking_type, local_model=None):
    """ë¡œì»¬ LLMìœ¼ë¡œ ì§ˆì˜ ì‹¤í–‰"""
    try:
        print(f"[LOCAL] execute_local_query ì‹œì‘")
        print(f"[LOCAL] íŒŒë¼ë¯¸í„°: query='{query[:30]}...', results={len(results)}ê°œ, chunking_type={chunking_type}, local_model={local_model}")
        
        rag_chain = get_enhanced_rag_chain()
        print(f"[LOCAL] EnhancedRAGChain ì¸ìŠ¤í„´ìŠ¤ íšë“ ì™„ë£Œ")
        
        local_chain = rag_chain.dual_llm.get_local_chain(local_model)
        print(f"[LOCAL] ë¡œì»¬ ì²´ì¸ íšë“ ì™„ë£Œ")
        
        context = rag_chain._format_context([doc for doc, score in results])
        print(f"[LOCAL] ì»¨í…ìŠ¤íŠ¸ í¬ë§· ì™„ë£Œ: {len(context)} ê¸€ì")
        
        start_time = time.time()
        print(f"[LOCAL] ë¡œì»¬ LLM í˜¸ì¶œ ì‹œì‘ (60ì´ˆ íƒ€ì„ì•„ì›ƒ)")
        
        # 60ì´ˆ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ë¡œì»¬ LLM í˜¸ì¶œ (Windows í˜¸í™˜)
        from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
        
        # ë¡œì»¬ ëª¨ë¸ëª… ì„¤ì •
        selected_model = local_model if local_model else Config.LLM_MODELS['local']['model_name']
        local_config = Config.LLM_MODELS['local']
        
        def call_local_llm():
            print(f"[LLM_LOG] ğŸ  ì‚¬ë‚´ì„œë²„ vLLM í˜¸ì¶œ ì‹œì‘: {selected_model}@{local_config['base_url']}")
            result = local_chain.invoke({
                "question": query,
                "context": context
            })
            print(f"[LLM_LOG] âœ… ì‚¬ë‚´ì„œë²„ vLLM ì‘ë‹µ ì™„ë£Œ: {len(str(result))}ì")
            print(f"[LOCAL] call_local_llm í•¨ìˆ˜ ë‚´ë¶€: LLM ì‘ë‹µ ë°›ìŒ")
            return result
        
        try:
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(call_local_llm)
                print(f"[LOCAL] ThreadPoolExecutor ì‹¤í–‰ ì¤‘...")
                response = future.result(timeout=60)  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ
                print(f"[LOCAL] ThreadPoolExecutor ì™„ë£Œ")
                
            response_time = time.time() - start_time
            print(f"[LOCAL] ë¡œì»¬ LLM í˜¸ì¶œ ì™„ë£Œ: {response_time:.2f}ì´ˆ")
            
        except FutureTimeoutError:
            response_time = time.time() - start_time
            print(f"[LOCAL] íƒ€ì„ì•„ì›ƒ ë°œìƒ: {response_time:.2f}ì´ˆ")
            raise TimeoutError(f"ë¡œì»¬ LLM ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ (60ì´ˆ): kanana8b ëª¨ë¸ì´ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        except Exception as e:
            response_time = time.time() - start_time
            print(f"[LOCAL] ì˜ˆì™¸ ë°œìƒ: {str(e)} ({response_time:.2f}ì´ˆ)")
            raise e
        
        answer = response.content if hasattr(response, 'content') else str(response)
        print(f"[LOCAL] ì‘ë‹µ ì²˜ë¦¬ ì™„ë£Œ: {len(answer)} ê¸€ì")
        
        # ìš”ì•½ ì²˜ë¦¬ (ë¡œì»¬ LLMì˜ ê²½ìš° í›„ì²˜ë¦¬)
        if summarize:
            print(f"[LOCAL] ìš”ì•½ ì²˜ë¦¬ ì‹œì‘")
            answer = summarize_answer(answer)
            print(f"[LOCAL] ìš”ì•½ ì²˜ë¦¬ ì™„ë£Œ")
        
        result = {
            'answer': answer,
            'response_time': response_time,
            'model': f'Local LLM ({chunking_type} ì²­í‚¹)',
            'estimated_tokens': len(answer.split()) * 1.3,
            'sources': [doc.metadata.get('source', 'Unknown') for doc, score in results],
            'success': True
        }
        
        print(f"[LOCAL] ê²°ê³¼ ìƒì„± ì™„ë£Œ: success={result['success']}")
        return result
        
    except Exception as e:
        # ì˜¤ë¥˜ ì›ì¸ ë¶„ì„
        error_reason = 'unknown_error'
        error_detail = str(e)
        
        if 'connection' in str(e).lower() or 'refused' in str(e).lower() or 'reach' in str(e).lower():
            error_reason = 'vllm_connection_failed'
            error_detail = 'vLLM ì„œë²„(192.168.0.224:8412)ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.'
        elif 'model' in str(e).lower() and 'not found' in str(e).lower():
            error_reason = 'model_not_found'
            error_detail = './models/kanana8b ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. vLLM ì„œë²„ì—ì„œ ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.'
        elif 'timeout' in str(e).lower() or '60ì´ˆ' in str(e):
            error_reason = 'timeout'
            error_detail = 'ë¡œì»¬ LLM ì‘ë‹µ ì‹œê°„ì´ 60ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. kanana8b ëª¨ë¸ì´ ì‘ë‹µí•˜ì§€ ì•Šê±°ë‚˜ ì²˜ë¦¬ ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤.'
        
        return {
            'answer': f'ì‚¬ë‚´ì„œë²„ vLLM ì‚¬ìš© ë¶ˆê°€: {error_detail}',
            'response_time': 0,
            'model': f'Local LLM (Error - {chunking_type} ì²­í‚¹)',
            'estimated_tokens': 0,
            'sources': [],
            'success': False,
            'error_reason': error_reason,
            'error_detail': str(e),
            'timestamp': datetime.now().isoformat()
        }

def apply_s3_basic_chunking(results):
    """s3 í´ë” ê¸°ë°˜ ê¸°ë³¸ ì²­í‚¹ ì „ëµ"""
    from services.chunking_strategies import get_chunking_strategy
    
    try:
        s3_basic_strategy = get_chunking_strategy('s3-basic')
        processed_results = []
        
        print(f"[START] s3 ê¸°ë³¸ ì²­í‚¹ ì „ëµ ì‹œì‘: {len(results)}ê°œ ë¬¸ì„œ")
        
        for doc, score in results:
            try:
                basic_chunks = s3_basic_strategy.split_documents([doc])
                
                for chunk in basic_chunks:
                    processed_results.append((chunk, score))
                    print(f"[OK] s3 ê¸°ë³¸ ì²­í¬: {len(chunk.page_content)}ì, ì ìˆ˜: {score:.3f}")
            
            except Exception as chunk_error:
                print(f"[WARNING] s3 ê¸°ë³¸ ì²­í‚¹ ì‹¤íŒ¨: {str(chunk_error)}")
                processed_results.append((doc, score * 0.9))
        
        processed_results.sort(key=lambda x: x[1], reverse=True)
        return processed_results[:5]
        
    except Exception as e:
        print(f"[ERROR] s3 ê¸°ë³¸ ì²­í‚¹ ì˜¤ë¥˜: {str(e)}")
        return results[:3]

def apply_s3_custom_chunking(results):
    """s3-chunking í´ë” ê¸°ë°˜ ì»¤ìŠ¤í…€ ì²­í‚¹ ì „ëµ"""
    from services.chunking_strategies import get_chunking_strategy
    
    try:
        s3_custom_strategy = get_chunking_strategy('s3-custom')
        processed_results = []
        
        print(f"[START] s3-chunking ì»¤ìŠ¤í…€ ì²­í‚¹ ì‹œì‘: {len(results)}ê°œ ë¬¸ì„œ")
        
        for doc, score in results:
            try:
                enhanced_chunks = s3_custom_strategy.split_documents([doc])
                
                for enhanced_chunk in enhanced_chunks:
                    # ì „ë© ì§€ì‹ í–¥ìƒ ì—¬ë¶€ì— ë”°ë¼ ì ìˆ˜ ì¡°ì •
                    if enhanced_chunk.metadata.get('enhanced_with_s3'):
                        adjusted_score = score * 1.02  # í–¥ìƒëœ ì²­í¬ëŠ” ì ìˆ˜ ìƒìŠ¹
                        chunk_info = "s3-chunking í–¥ìƒ"
                    else:
                        adjusted_score = score * 0.98
                        chunk_info = "s3-chunking ê¸°ë³¸"
                    
                    processed_results.append((enhanced_chunk, adjusted_score))
                    print(f"[OK] {chunk_info}: {len(enhanced_chunk.page_content)}ì, ì ìˆ˜: {adjusted_score:.3f}")
            
            except Exception as chunk_error:
                print(f"[WARNING] s3-chunking ì²­í‚¹ ì‹¤íŒ¨: {str(chunk_error)}")
                processed_results.append((doc, score * 0.9))
        
        processed_results.sort(key=lambda x: x[1], reverse=True)
        final_results = processed_results[:5]
        
        print(f"[COMPLETE] s3-chunking ì»¤ìŠ¤í…€ ì™„ë£Œ: {len(final_results)}ê°œ ì²­í¬")
        return final_results
        
    except Exception as e:
        print(f"[ERROR] s3-chunking ì˜¤ë¥˜: {str(e)}")
        return results[:3]

def summarize_answer(answer):
    """ë‹µë³€ ìš”ì•½ (ê°„ë‹¨í•œ ë²„ì „)"""
    # ì‹¤ì œë¡œëŠ” LLMì„ ë‹¤ì‹œ í˜¸ì¶œí•´ì„œ ìš”ì•½í•´ì•¼ í•˜ì§€ë§Œ,
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì²« 2ë¬¸ì¥ë§Œ ë°˜í™˜
    sentences = answer.split('.')
    if len(sentences) >= 2:
        return '. '.join(sentences[:2]) + '.'
    return answer

def calculate_benchmark_score(result, processing_time):
    """ë²¤ì¹˜ë§ˆí‚¹ ì ìˆ˜ ê³„ì‚°"""
    if not result['success']:
        return 0.0
    
    # ì ìˆ˜ ê³„ì‚° ìš”ì†Œë“¤
    speed_score = max(0, 10 - processing_time)  # ë¹ ë¥¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
    length_score = min(10, len(result['answer']) / 100)  # ì ì ˆí•œ ê¸¸ì´
    token_efficiency = min(10, 1000 / (result['estimated_tokens'] + 1))
    
    total_score = (speed_score + length_score + token_efficiency) / 3
    return min(10.0, max(0.0, total_score))

def record_query_usage(query, result):
    """ì¿¼ë¦¬ ì‚¬ìš© ê¸°ë¡ (Redis/RDB ì •ì±…)"""
    try:
        # ê°„ë‹¨í•œ íŒŒì¼ ê¸°ë°˜ ê¸°ë¡ (ì‹¤ì œë¡œëŠ” Redis/RDB ì‚¬ìš©)
        usage_file = 'data/query_usage.json'
        os.makedirs(os.path.dirname(usage_file), exist_ok=True)
        
        if os.path.exists(usage_file):
            with open(usage_file, 'r', encoding='utf-8') as f:
                usage_data = json.load(f)
        else:
            usage_data = {}
        
        query_key = query.lower().strip()
        if query_key not in usage_data:
            usage_data[query_key] = {'count': 0, 'first_used': datetime.now().isoformat()}
        
        usage_data[query_key]['count'] += 1
        usage_data[query_key]['last_used'] = datetime.now().isoformat()
        
        with open(usage_file, 'w', encoding='utf-8') as f:
            json.dump(usage_data, f, indent=2, ensure_ascii=False)
            
    except Exception as e:
        print(f"ì‚¬ìš© ê¸°ë¡ ì €ì¥ ì˜¤ë¥˜: {e}")