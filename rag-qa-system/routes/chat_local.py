from flask import Blueprint, request, jsonify
from services.rag_chain import RAGChain
from models.vectorstore import get_vectorstore
from services.card_manager import process_user_card_query
import time
import os

chat_local_bp = Blueprint('chat_local', __name__)

# ì „ì—­ RAG ì²´ì¸ ì¸ìŠ¤í„´ìŠ¤
rag_chain = None

def get_rag_chain():
    """RAG Chain ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    global rag_chain
    if rag_chain is None:
        rag_chain = RAGChain()
    return rag_chain

def load_user_profile(user_name):
    """ì‚¬ìš©ì ê°œì¸ì •ë³´ íŒŒì¼ ë¡œë“œ"""
    try:
        # s3-chunking í´ë”ì—ì„œ ì‚¬ìš©ì ì •ë³´ íŒŒì¼ ì°¾ê¸°
        profile_path = os.path.join(os.path.dirname(__file__), '../../s3-chunking', f'{user_name}_ê°œì¸ì •ë³´.txt')
        if os.path.exists(profile_path):
            with open(profile_path, 'r', encoding='utf-8') as f:
                return f.read()
    except Exception as e:
        print(f"ì‚¬ìš©ì í”„ë¡œí•„ ë¡œë“œ ì˜¤ë¥˜: {e}")
    return None

def detect_user_and_enhance_query(question):
    """ì‚¬ìš©ì ê°ì§€ ë° ì¿¼ë¦¬ ê°œì„  (ì¼ë°˜í™”ëœ ë²„ì „)"""
    # ë‹¤ì–‘í•œ ì‚¬ìš©ìëª… íŒ¨í„´ ê°ì§€
    user_patterns = ['ê¹€ëª…ì •', 'ì´ì˜í¬', 'ë°•ì² ìˆ˜', 'ìµœì˜ìˆ˜']  # í™•ì¥ ê°€ëŠ¥
    detected_user = None
    
    for user in user_patterns:
        if user in question:
            detected_user = user
            break
    
    if detected_user:
        print(f"ğŸ” {detected_user} ê³ ê° ê°ì§€ - ê°œì¸ì •ë³´ ì—°ë™ ì²˜ë¦¬")
        
        # ì¹´ë“œ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ í™•ì¸
        card_keywords = ['ì¹´ë“œ', 'ë°œê¸‰', 'íšŒì›ì€í–‰', 'ì€í–‰ë³„']
        if any(keyword in question for keyword in card_keywords):
            print("ğŸ’³ ì¹´ë“œ ê´€ë ¨ ì§ˆì˜ ê°ì§€ - ë™ì  ì¹´ë“œ ë¶„ë¥˜ ì²˜ë¦¬")
            
            # BCì¹´ë“œ ë°œê¸‰ì•ˆë‚´ MD íŒŒì¼ ê²½ë¡œ
            md_file_path = '/mnt/d/99_DEOTIS_QA_SYSTEM/03_DEOTIS_QA/s3-chunking/BCì¹´ë“œ(ì¹´ë“œì´ìš©ì•ˆë‚´).md'
            
            try:
                # ë™ì ìœ¼ë¡œ ì¹´ë“œ ì •ë³´ ì²˜ë¦¬
                card_summary = process_user_card_query(detected_user, md_file_path)
                
                enhanced_query = f"""
ì§ˆë¬¸: {question}

{card_summary}

ìœ„ {detected_user} ê³ ê°ì˜ ì¹´ë“œ í˜„í™©ì„ ë°”íƒ•ìœ¼ë¡œ ë§ì¶¤í˜• ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ë³´ìœ  ì¹´ë“œì™€ ë¯¸ë³´ìœ  ì¹´ë“œë¥¼ êµ¬ë¶„í•˜ì—¬ ì´ë¯¸ì§€ì™€ í•¨ê»˜ ì•ˆë‚´í•´ì£¼ì„¸ìš”.
"""
                return enhanced_query, True, card_summary
                
            except Exception as e:
                print(f"âŒ ë™ì  ì¹´ë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ì²˜ë¦¬
                user_profile = load_user_profile(detected_user)
                if user_profile:
                    enhanced_query = f"""
ì§ˆë¬¸: {question}

{detected_user} ê³ ê° ê°œì¸ì •ë³´:
{user_profile}

ìœ„ ê³ ê° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§ì¶¤í˜• ì¹´ë“œ ë°œê¸‰ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
"""
                    return enhanced_query, True, None
    
    return question, False, None

@chat_local_bp.route('/local', methods=['POST'])
def chat_local():
    """ë¡œì»¬ LLMì„ ì‚¬ìš©í•œ ë‹¨ì¼ ì§ˆì˜ ì²˜ë¦¬"""
    try:
        data = request.json
        question = data.get('question', '').strip()
        use_memory = data.get('use_memory', False)
        
        if not question:
            return jsonify({'error': 'ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.'}), 400
        
        # ì‚¬ìš©ì ê°ì§€ ë° ì¿¼ë¦¬ ê°œì„ 
        enhanced_question, is_personalized, card_summary = detect_user_and_enhance_query(question)
        
        start_time = time.time()
        
        # RAG Chainì—ì„œ ë¡œì»¬ LLMë§Œ ì‚¬ìš©
        rag_chain = get_rag_chain()
        
        # ë²¤ì¹˜ë§ˆí‚¹ ëª¨ë“œë¥¼ ë¹„í™œì„±í™”í•˜ê³  ë¡œì»¬ LLMë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
        rag_chain.use_benchmarking = False
        
        # ë¡œì»¬ LLM ì²´ì¸ ê°€ì ¸ì˜¤ê¸°
        try:
            local_chain = rag_chain.dual_llm.get_local_chain()
            
            # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (ì›ë³¸ ì§ˆë¬¸ ë˜ëŠ” í–¥ìƒëœ ì§ˆë¬¸ ì‚¬ìš©)
            search_query = enhanced_question if is_personalized else question
            results = rag_chain._search_documents(search_query, 7)  # ê°œì¸í™”ëœ ê²½ìš° ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
            
            if not results:
                return jsonify({
                    'answer': 'ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                    'similarity_search': [],
                    'processing_time': time.time() - start_time,
                    'model_used': 'Local LLM',
                    'error': 'no_documents',
                    'is_personalized': is_personalized
                })
            
            # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = rag_chain._format_context([doc for doc, score in results])
            
            # ê°œì¸í™”ëœ ê²½ìš° ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì œê³µ
            if is_personalized:
                print("ğŸ“ ê°œì¸í™”ëœ ì§ˆì˜ ì²˜ë¦¬ ì¤‘...")
            
            # ë¡œì»¬ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± (í–¥ìƒëœ ì§ˆë¬¸ ì‚¬ìš©)
            response = local_chain.invoke({
                "question": enhanced_question,
                "context": context
            })
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if hasattr(response, 'content'):
                answer = response.content
            else:
                answer = str(response)
            
            processing_time = time.time() - start_time
            
            # ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ í¬ë§·
            similarity_search = []
            for doc, score in results:
                similarity_search.append({
                    'content': doc.page_content[:200] + '...' if len(doc.page_content) > 200 else doc.page_content,
                    'score': float(score),
                    'metadata': doc.metadata
                })
            
            response_data = {
                'answer': answer,
                'similarity_search': similarity_search,
                'processing_time': processing_time,
                'model_used': 'Local LLM (LLaMA)',
                'chunk_info': {
                    'chunk_size': 1000,
                    'chunk_overlap': 200,
                    'total_chunks': len(results)
                },
                'is_personalized': is_personalized
            }
            
            # ì¹´ë“œ ìš”ì•½ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if card_summary:
                response_data['card_summary'] = card_summary
                response_data['dynamic_card_processing'] = True
                print("ğŸ“‹ ë™ì  ì¹´ë“œ ì •ë³´ê°€ ì‘ë‹µì— í¬í•¨ë¨")
            
            return jsonify(response_data)
            
        except Exception as llm_error:
            return jsonify({
                'answer': f'ë¡œì»¬ LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Ollamaê°€ ì„¤ì¹˜ë˜ê³  ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”. ì˜¤ë¥˜: {str(llm_error)}',
                'similarity_search': [],
                'processing_time': time.time() - start_time,
                'model_used': 'Local LLM (Error)',
                'error': 'local_llm_unavailable'
            })
        
    except Exception as e:
        return jsonify({
            'error': f'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'processing_time': 0,
            'model_used': 'Error'
        }), 500

@chat_local_bp.route('/custom', methods=['POST'])
def chat_custom():
    """s3-chunking ì»¤ìŠ¤í…€ ê²€ìƒ‰ì„ ì‚¬ìš©í•œ ì§ˆì˜ ì²˜ë¦¬"""
    try:
        data = request.json
        question = data.get('question', '').strip()
        use_s3_chunking = data.get('use_s3_chunking', True)
        
        if not question:
            return jsonify({'error': 'ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.'}), 400
        
        # ì‚¬ìš©ì ê°ì§€ ë° ì¿¼ë¦¬ ê°œì„  (ê¹€ëª…ì • ë“±)
        enhanced_question, is_personalized, card_summary = detect_user_and_enhance_query(question)
        
        start_time = time.time()
        
        # RAG Chain ê°€ì ¸ì˜¤ê¸°
        rag_chain = get_rag_chain()
        
        # custom ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰í•˜ë„ë¡ ì„¤ì •
        rag_chain.use_benchmarking = False
        
        # ë¡œì»¬ LLM ì²´ì¸ ê°€ì ¸ì˜¤ê¸°
        try:
            local_chain = rag_chain.dual_llm.get_local_chain()
            
            # custom ì»¬ë ‰ì…˜ì—ì„œ ë¬¸ì„œ ê²€ìƒ‰
            # s3-chunking í´ë”ì˜ ë¬¸ì„œë¥¼ ì‚¬ìš©
            search_query = enhanced_question if is_personalized else question
            results = rag_chain._search_documents(search_query, 7)  # ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
            
            if not results:
                return jsonify({
                    'answer': 'ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                    'similarity_search': [],
                    'processing_time': time.time() - start_time,
                    'model_used': 'Local LLM',
                    'error': 'no_documents',
                    'chunking_type': 'custom'
                })
            
            # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = rag_chain._format_context([doc for doc, score in results])
            
            # ê°œì¸í™”ëœ ê²½ìš° ë¡œê·¸
            if is_personalized:
                print(f"ğŸ“ ê°œì¸í™”ëœ ì§ˆì˜ ì²˜ë¦¬ ì¤‘... (s3-chunking)")
                print(f"ğŸ” í–¥ìƒëœ ì¿¼ë¦¬ ê¸¸ì´: {len(enhanced_question)}ì")
            
            # ë¡œì»¬ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± (í–¥ìƒëœ ì§ˆë¬¸ ì‚¬ìš©)
            response = local_chain.invoke({
                "question": enhanced_question,
                "context": context
            })
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if hasattr(response, 'content'):
                answer = response.content
            else:
                answer = str(response)
            
            processing_time = time.time() - start_time
            
            # ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ í¬ë§·
            similarity_search = []
            for doc, score in results:
                similarity_search.append({
                    'content': doc.page_content[:200] + '...' if len(doc.page_content) > 200 else doc.page_content,
                    'score': float(score),
                    'metadata': doc.metadata
                })
            
            response_data = {
                'answer': answer,
                'similarity_search': similarity_search,
                'processing_time': processing_time,
                'model_used': 'Local LLM (s3-chunking)',
                'chunk_info': {
                    'chunk_size': 1500,
                    'chunk_overlap': 250,
                    'total_chunks': len(results)
                },
                'chunking_type': 'custom',
                'is_personalized': is_personalized
            }
            
            # ì¹´ë“œ ìš”ì•½ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if card_summary:
                response_data['card_summary'] = card_summary
                response_data['dynamic_card_processing'] = True
                print("ğŸ“‹ ë™ì  ì¹´ë“œ ì •ë³´ê°€ ì‘ë‹µì— í¬í•¨ë¨ (s3-chunking)")
            
            return jsonify(response_data)
            
        except Exception as llm_error:
            return jsonify({
                'answer': f'ë¡œì»¬ LLM ì˜¤ë¥˜: {str(llm_error)}',
                'similarity_search': [],
                'processing_time': time.time() - start_time,
                'model_used': 'Local LLM (Error)',
                'error': 'local_llm_error',
                'chunking_type': 'custom'
            })
        
    except Exception as e:
        return jsonify({
            'error': f'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
            'processing_time': 0,
            'model_used': 'Error',
            'chunking_type': 'custom'
        }), 500