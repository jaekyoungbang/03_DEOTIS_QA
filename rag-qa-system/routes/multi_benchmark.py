from flask import Blueprint, jsonify, request
from services.enhanced_rag_chain import EnhancedRAGChain
from models.vectorstore import get_vectorstore, get_dual_vectorstore
from routes.unified_benchmark import process_query_with_similarity_check
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from flask import Response
import json

multi_benchmark_bp = Blueprint('multi_benchmark', __name__)

# Ï†ÑÏó≠ Ïù∏Ïä§ÌÑ¥Ïä§Îì§
enhanced_rag_chain = None
executor = ThreadPoolExecutor(max_workers=4)

def get_enhanced_rag_chain():
    """Enhanced RAG Chain Ïù∏Ïä§ÌÑ¥Ïä§ Í∞ÄÏ†∏Ïò§Í∏∞"""
    global enhanced_rag_chain
    if enhanced_rag_chain is None:
        dual_vectorstore = get_dual_vectorstore()
        enhanced_rag_chain = EnhancedRAGChain(dual_vectorstore)
    return enhanced_rag_chain

@multi_benchmark_bp.route('/multi-query-stream', methods=['POST'])
def multi_benchmark_query_stream():
    """Ïã§ÏãúÍ∞Ñ Ïä§Ìä∏Î¶¨Î∞ç Î©ÄÌã∞ Î≤§ÏπòÎßàÌÇπ - Phase 1,2 Î≥ëÎ†¨ Ïã§Ìñâ"""
    
    print(f"[STREAM] üéØ multi_benchmark_query_stream ÏóîÎìúÌè¨Ïù∏Ìä∏ Ìò∏Ï∂úÎê®")
    print(f"[STREAM] Request method: {request.method}")
    print(f"[STREAM] Request headers: {dict(request.headers)}")
    
    # Ïä§Ìä∏Î¶¨Î∞ç Î∞ñÏóêÏÑú request Îç∞Ïù¥ÌÑ∞Î•º Î®ºÏ†Ä Í∞ÄÏ†∏Ïò§Í∏∞
    try:
        data = request.get_json()
        print(f"[STREAM] ÏöîÏ≤≠ Îç∞Ïù¥ÌÑ∞ ÌååÏã± ÏôÑÎ£å: {data}")
        
        query = data.get('query', '').strip()
        summarize = data.get('summarize', False) 
        local_model = data.get('local_model', './models/kanana8b')
        
        print(f"[STREAM] ÌååÎùºÎØ∏ÌÑ∞ Ï∂îÏ∂ú: query='{query[:50]}...', summarize={summarize}, local_model={local_model}")
        
    except Exception as json_error:
        print(f"[STREAM] ‚ùå JSON ÌååÏã± Ïò§Î•ò: {json_error}")
        return Response(f"data: {json.dumps({'type': 'error', 'message': f'JSON ÌååÏã± Ïò§Î•ò: {str(json_error)}'})}\n\n", 
                       mimetype='text/event-stream')
    
    def generate_stream():
        try:
            print(f"[STREAM] üöÄ generate_stream ÏãúÏûë")
            
            if not query:
                print(f"[STREAM] ‚ùå ÏßàÎ¨∏Ïù¥ ÎπÑÏñ¥ÏûàÏùå")
                yield f"data: {json.dumps({'type': 'error', 'message': 'ÏßàÎ¨∏ÏùÑ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'})}\n\n"
                return
            
            
            # ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îìú ÌôïÏù∏
            print(f"[STREAM] üîç Î™®Îç∏ Í∞ÄÏö©ÏÑ± ÌôïÏù∏ ÏãúÏûë")
            try:
                from models.dual_llm import DualLLMManager
                from config import Config
                
                print(f"[STREAM] DualLLMManager ÏÉùÏÑ± Ï§ë...")
                dual_llm = DualLLMManager()
                print(f"[STREAM] get_available_models Ìò∏Ï∂ú...")
                available_models = dual_llm.get_available_models()
                print(f"[STREAM] Î™®Îç∏ Í∞ÄÏö©ÏÑ±: {available_models}")
                
                yield f"data: {json.dumps({'type': 'models_check', 'available': available_models})}\n\n"
                
            except Exception as e:
                print(f"[STREAM] ‚ùå Î™®Îç∏ ÌôïÏù∏ Ïò§Î•ò: {str(e)}")
                available_models = {'api': True, 'local': False}
                yield f"data: {json.dumps({'type': 'models_error', 'error': str(e)})}\n\n"
            
            # Phase 1 Î™®Îìú ÏÑ§Ï†ï
            phase1_modes = []
            if available_models.get('local'):
                phase1_modes.append('local-basic')
            if available_models.get('api'):
                phase1_modes.append('chatgpt-basic')
            
            # Phase 2 Î™®Îìú ÏÑ§Ï†ï
            phase2_modes = []
            if available_models.get('local'):
                phase2_modes.append('local-custom')
            if available_models.get('api'):
                phase2_modes.append('chatgpt-custom')
            
            yield f"data: {json.dumps({'type': 'phase_info', 'phase1': phase1_modes, 'phase2': phase2_modes})}\n\n"
            
            # === PHASE 1: Î≥ëÎ†¨ Ïã§Ìñâ (local-basic + chatgpt-basic) ===
            yield f"data: {json.dumps({'type': 'phase_start', 'phase': 1, 'modes': phase1_modes})}\n\n"
            
            with ThreadPoolExecutor(max_workers=2) as executor:
                # Phase 1 Î≥ëÎ†¨ Ïã§Ìñâ
                phase1_futures = {}
                for mode in phase1_modes:
                    if mode.startswith('local') and available_models.get('local'):
                        future = executor.submit(execute_single_mode, mode, query, summarize, local_model, "PHASE1")
                        phase1_futures[future] = mode
                    elif mode.startswith('chatgpt') and available_models.get('api'):
                        future = executor.submit(execute_single_mode, mode, query, summarize, local_model, "PHASE1")
                        phase1_futures[future] = mode
                
                # Phase 1 Í≤∞Í≥º Ïã§ÏãúÍ∞Ñ Ï†ÑÏÜ°
                for future in as_completed(phase1_futures):
                    mode = phase1_futures[future]
                    try:
                        result = future.result()
                        yield f"data: {json.dumps({'type': 'result', 'phase': 1, 'mode': mode, 'result': result})}\n\n"
                    except Exception as e:
                        error_result = create_error_result_stream(mode, str(e))
                        yield f"data: {json.dumps({'type': 'result', 'phase': 1, 'mode': mode, 'result': error_result})}\n\n"
            
            yield f"data: {json.dumps({'type': 'phase_complete', 'phase': 1})}\n\n"
            
            # === PHASE 2: Î≥ëÎ†¨ Ïã§Ìñâ (local-custom + chatgpt-custom) ===
            yield f"data: {json.dumps({'type': 'phase_start', 'phase': 2, 'modes': phase2_modes})}\n\n"
            
            with ThreadPoolExecutor(max_workers=2) as executor:
                # Phase 2 Î≥ëÎ†¨ Ïã§Ìñâ
                phase2_futures = {}
                for mode in phase2_modes:
                    if mode.startswith('local') and available_models.get('local'):
                        future = executor.submit(execute_single_mode, mode, query, summarize, local_model, "PHASE2")
                        phase2_futures[future] = mode
                    elif mode.startswith('chatgpt') and available_models.get('api'):
                        future = executor.submit(execute_single_mode, mode, query, summarize, local_model, "PHASE2")
                        phase2_futures[future] = mode
                
                # Phase 2 Í≤∞Í≥º Ïã§ÏãúÍ∞Ñ Ï†ÑÏÜ°
                for future in as_completed(phase2_futures):
                    mode = phase2_futures[future]
                    try:
                        result = future.result()
                        yield f"data: {json.dumps({'type': 'result', 'phase': 2, 'mode': mode, 'result': result})}\n\n"
                    except Exception as e:
                        error_result = create_error_result_stream(mode, str(e))
                        yield f"data: {json.dumps({'type': 'result', 'phase': 2, 'mode': mode, 'result': error_result})}\n\n"
            
            yield f"data: {json.dumps({'type': 'phase_complete', 'phase': 2})}\n\n"
            yield f"data: {json.dumps({'type': 'complete'})}\n\n"
            
        except Exception as e:
            print(f"[STREAM] ‚ùå generate_stream Ï†ÑÏ≤¥ Ïò§Î•ò: {str(e)}")
            import traceback
            print(f"[STREAM] Ïò§Î•ò Ìä∏Î†àÏù¥Ïä§:")
            traceback.print_exc()
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
    
    return Response(generate_stream(), mimetype='text/event-stream',
                   headers={'Cache-Control': 'no-cache', 'Connection': 'keep-alive'})

def execute_single_mode(mode, query, summarize, local_model, phase_info):
    """Îã®Ïùº Î™®Îìú Ïã§Ìñâ Ìï®Ïàò (Ïä§Ìä∏Î¶¨Î∞çÏö©)"""
    try:
        start_time = time.time()
        print(f"[{phase_info}] {mode} Î™®Îìú ÏãúÏûë - ÏøºÎ¶¨: '{query[:50]}...'")
        print(f"[{phase_info}] {mode} Îß§Í∞úÎ≥ÄÏàò: summarize={summarize}, local_model={local_model}")
        
        print(f"[{phase_info}] {mode} process_query_with_similarity_check Ìò∏Ï∂ú ÏãúÏûë")
        result = process_query_with_similarity_check(query, mode, summarize, local_model)
        print(f"[{phase_info}] {mode} process_query_with_similarity_check ÏôÑÎ£å")
        
        end_time = time.time()
        print(f"[{phase_info}] {mode} Î™®Îìú ÏôÑÎ£å ({end_time - start_time:.2f}Ï¥à)")
        print(f"[{phase_info}] {mode} Í≤∞Í≥º ÌÉÄÏûÖ: {result.get('type', 'unknown')}")
        print(f"[{phase_info}] {mode} Í≤∞Í≥º ÏÑ±Í≥µ Ïó¨Î∂Ä: {result.get('success', 'unknown')}")
        
        if 'response_time' not in result:
            result['response_time'] = end_time - start_time
            
        return {
            'mode': mode,
            'result': result,
            'success': True,
            'processing_time': end_time - start_time
        }
    except Exception as e:
        print(f"[{phase_info}] {mode} Î™®Îìú Ïò§Î•ò: {str(e)}")
        import traceback
        print(f"[{phase_info}] {mode} Ïò§Î•ò Ìä∏Î†àÏù¥Ïä§:")
        traceback.print_exc()
        return {
            'mode': mode,
            'result': {
                'type': 'single',
                'answer': f'Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}',
                'response_time': 0,
                'model': mode,
                'success': False,
                'error_reason': 'processing_error',
                'timestamp': datetime.now().isoformat()
            },
            'success': False,
            'error': str(e)
        }

def create_error_result_stream(mode, error_msg):
    """Ïä§Ìä∏Î¶¨Î∞çÏö© Ïò§Î•ò Í≤∞Í≥º ÏÉùÏÑ±"""
    if mode.startswith('local'):
        error_detail = 'vLLM ÏÑúÎ≤Ñ Ïó∞Í≤∞ Ïò§Î•ò'
    else:
        error_detail = 'ChatGPT API Ïò§Î•ò'
    
    return {
        'mode': mode,
        'result': {
            'type': 'single',
            'answer': f'{error_detail}: {error_msg}',
            'response_time': 0,
            'model': mode,
            'success': False,
            'timestamp': datetime.now().isoformat()
        },
        'success': False,
        'error': error_msg
    }

@multi_benchmark_bp.route('/multi-query', methods=['POST'])
def multi_benchmark_query():
    """4Í∞ÄÏßÄ Î™®Îìú ÏàúÏ∞®Ï†Å Î≤§ÏπòÎßàÌÇπ ÏßàÏùò Ï≤òÎ¶¨ - Phase 1, 2Î°ú Î∂ÑÎ¶¨ÌïòÏó¨ ÏÑúÎ≤Ñ Í≥ºÎ∂ÄÌïò Î∞©ÏßÄ"""
    try:
        data = request.json
        query = data.get('query', '').strip()
        summarize = data.get('summarize', False)
        local_model = data.get('local_model', 'Qwen/Qwen3-32B-AWQ')  # ÏÇ¨Ïö©ÏûêÍ∞Ä ÏÑ†ÌÉùÌïú Î°úÏª¨ Î™®Îç∏
        
        if not query:
            return jsonify({'error': 'ÏßàÎ¨∏ÏùÑ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.'}), 400
        
        # ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îìú ÌôïÏù∏
        try:
            from models.dual_llm import DualLLMManager
            from config import Config
            
            # OpenAI API ÌÇ§ ÏÉÅÌÉú Ï≤¥ÌÅ¨
            api_key = Config.OPENAI_API_KEY
            print(f"[DEBUG] OpenAI API Key ÏÉÅÌÉú: {'ÏÑ§Ï†ïÎê®' if api_key else 'ÎØ∏ÏÑ§Ï†ï'} (Í∏∏Ïù¥: {len(api_key) if api_key else 0})")
            
            dual_llm = DualLLMManager()
            available_models = dual_llm.get_available_models()
            print(f"[DEBUG] Î™®Îç∏ Í∞ÄÏö©ÏÑ±: {available_models}")
            
            # Î°úÏª¨ LLM ÌôúÏÑ±Ìôî (ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠)
            # available_models['local'] = False  # Î°úÏª¨ LLM ÎπÑÌôúÏÑ±Ìôî  
            print(f"[INFO] Î°úÏª¨ LLM ÌôúÏÑ±ÌôîÎê® - kanana8b Î™®Îç∏ ÏÇ¨Ïö©")
            
        except Exception as e:
            print(f"[ERROR] Î™®Îìú ÌôïÏù∏ Ï§ë Ïò§Î•ò: {e}")
            import traceback
            traceback.print_exc()
            # Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ChatGPT Î™®ÎìúÎßå ÏãúÎèÑ
            available_models = {'api': True, 'local': False}
        
        # Phase 1: Í∏∞Î≥∏ Î∞©Î≤ïÎì§ - ÏàúÏ∞® Ïã§Ìñâ
        phase1_modes = []
        if available_models.get('local'):
            phase1_modes.append('local-basic')
        if available_models.get('api'):
            phase1_modes.append('chatgpt-basic')
        
        # Phase 2: Ïª§Ïä§ÌÖÄ Î∞©Î≤ïÎì§ - Phase 1 ÏôÑÎ£å ÌõÑ Ïã§Ìñâ  
        phase2_modes = []
        if available_models.get('local'):
            phase2_modes.append('local-custom')
        if available_models.get('api'):
            phase2_modes.append('chatgpt-custom')
        
        print(f"[INFO] ÏàúÏ∞®Ï†Å Ïã§Ìñâ ÏãúÏûë - Phase 1: {phase1_modes}, Phase 2: {phase2_modes}")
        
        # Ï≤òÎ¶¨ Ìï®Ïàò Ï†ïÏùò
        def process_single_mode(mode, phase_info="", local_model_override=None):
            try:
                start_time = time.time()
                print(f"[{phase_info}] {mode} Î™®Îìú ÏãúÏûë")
                
                # Î°úÏª¨ LLM Î™®ÎìúÏù∏ Í≤ΩÏö∞ Ï∂îÍ∞Ä Ï≤¥ÌÅ¨
                if mode.startswith('local'):
                    try:
                        result = process_query_with_similarity_check(query, mode, summarize, local_model_override or local_model)
                    except Exception as local_error:
                        # Î°úÏª¨ LLM Ïò§Î•ò Ïãú Ïú†ÏÇ¨ÎèÑ Ï†ïÎ≥¥ Ìè¨Ìï®Ìïú Í∏∞Î≥∏ ÏùëÎãµ Ï†úÍ≥µ
                        rag_chain = get_enhanced_rag_chain()
                        search_results = rag_chain._search_documents(query, 3)
                        
                        # Ïú†ÏÇ¨ÎèÑ Ï†ïÎ≥¥ Ï∂îÏ∂ú
                        similarity_info = []
                        if search_results:
                            for i, (doc, score) in enumerate(search_results[:3]):
                                similarity_info.append({
                                    'rank': i + 1,
                                    'score': round(score, 3),
                                    'content': doc.page_content[:100] + '...'
                                })
                        
                        end_time = time.time()
                        
                        return {
                            'mode': mode,
                            'result': {
                                'type': 'single',
                                'answer': f'ÏÇ¨ÎÇ¥ÏÑúÎ≤Ñ vLLM ÏÇ¨Ïö© Î∂àÍ∞Ä: {str(local_error)}\n\nvLLM ÏÑúÎ≤ÑÏóê Ïó∞Í≤∞Ìï† Ïàò ÏóÜÏäµÎãàÎã§.\nÌï¥Í≤∞Î∞©Î≤ï: vLLM ÏÑúÎ≤Ñ(192.168.0.224:8412)Í∞Ä Ïã§Ìñâ Ï§ëÏù∏ÏßÄ ÌôïÏù∏ÌïòÍ≥† ./models/kanana8b Î™®Îç∏Ïù¥ Î°úÎìúÎêòÏóàÎäîÏßÄ ÌôïÏù∏ÌïòÏÑ∏Ïöî.',
                                'response_time': round(end_time - start_time, 3),
                                'model': mode,
                                'success': False,
                                'similarity_scores': similarity_info,
                                'error_reason': 'local_llm_unavailable',
                                'timestamp': datetime.now().isoformat()
                            },
                            'success': False,
                            'error': f'Local LLM Error: {str(local_error)}'
                        }
                else:
                    print(f"[{phase_info}] {mode} ChatGPT API Ìò∏Ï∂ú ÏãúÏûë")
                    result = process_query_with_similarity_check(query, mode, summarize, local_model)
                    print(f"[{phase_info}] {mode} ChatGPT API Ìò∏Ï∂ú ÏôÑÎ£å: {result.get('success', False)}")
                
                end_time = time.time()
                print(f"[{phase_info}] {mode} Î™®Îìú ÏôÑÎ£å ({end_time - start_time:.2f}Ï¥à)")
                
                # Ï≤òÎ¶¨ ÏãúÍ∞Ñ Ï∂îÍ∞Ä
                if 'response_time' not in result:
                    result['response_time'] = end_time - start_time
                
                return {
                    'mode': mode,
                    'result': result,
                    'success': True,
                    'processing_time': end_time - start_time
                }
            except Exception as e:
                error_msg = str(e) if str(e) else type(e).__name__
                print(f"[{phase_info}] {mode} Î™®Îìú Ïò§Î•ò: {error_msg}")
                import traceback
                print(f"[{phase_info}] {mode} ÏÉÅÏÑ∏ Ïò§Î•ò:")
                traceback.print_exc()
                print(f"[{phase_info}] {mode} Ïò§Î•ò ÌÉÄÏûÖ: {type(e).__name__}")
                print(f"[{phase_info}] {mode} Ïò§Î•ò ÏÑ∏Î∂ÄÏÇ¨Ìï≠: {repr(e)}")
                
                # Îçî ÏûêÏÑ∏Ìïú Ïò§Î•ò Î©îÏãúÏßÄ ÏÉùÏÑ±
                if hasattr(e, 'args') and e.args:
                    error_detail = f"{type(e).__name__}: {e.args[0]}"
                else:
                    error_detail = f"{type(e).__name__}: {error_msg}"
                
                return {
                    'mode': mode,
                    'result': {
                        'type': 'single',
                        'answer': f'Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò Î∞úÏÉù: {error_detail}',
                        'response_time': 0,
                        'model': mode,
                        'success': False,
                        'similarity_scores': [],
                        'error_reason': 'processing_error',
                        'error_detail': error_detail,
                        'timestamp': datetime.now().isoformat()
                    },
                    'success': False,
                    'error': error_detail
                }

        # Ïò§Î•ò Í≤∞Í≥º ÏÉùÏÑ± Ìï®Ïàò
        def create_error_result(mode, phase_info=""):
            if mode.startswith('local'):
                error_msg = 'Î°úÏª¨ LLM(vLLM) ÏÑúÎ≤ÑÍ∞Ä Ïó∞Í≤∞ÎêòÏßÄ ÏïäÏäµÎãàÎã§.\n\nÌï¥Í≤∞Î∞©Î≤ï:\n1. vLLM ÏÑúÎ≤ÑÍ∞Ä 192.168.0.224:8412ÏóêÏÑú Ïã§Ìñâ Ï§ëÏù∏ÏßÄ ÌôïÏù∏\n2. ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞ ÏÉÅÌÉú ÌôïÏù∏\n3. ./models/kanana8b Î™®Îç∏Ïù¥ Î°úÎìúÎêòÏóàÎäîÏßÄ ÌôïÏù∏'
                error_reason = 'vllm_not_available'
            else:
                error_msg = 'ChatGPT API ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÍ±∞ÎÇò ÏûòÎ™ªÎêòÏóàÏäµÎãàÎã§.\n\nÌï¥Í≤∞Î∞©Î≤ï: .env ÌååÏùºÏùò OPENAI_API_KEYÎ•º ÌôïÏù∏ÌïòÏÑ∏Ïöî.'
                error_reason = 'api_key_missing'
            
            print(f"[{phase_info}] {mode} Î™®Îìú Ïò§Î•ò ÏÉùÏÑ±: {error_reason}")
            return {
                'mode': mode,
                'result': {
                    'type': 'single',
                    'answer': error_msg,
                    'response_time': 0,
                    'model': mode,
                    'success': False,
                    'similarity_scores': [],
                    'error_reason': error_reason,
                    'timestamp': datetime.now().isoformat()
                },
                'success': False,
                'error': f'{mode} unavailable',
                'processing_time': 0
            }
        
        all_results = []
        
        # === PHASE 1 Ïã§Ìñâ: ChatGPT Í∏∞Î≥∏ (ÏàúÏ∞® Ïã§Ìñâ) ===
        print(f"[PHASE 1] ÏãúÏûë - ChatGPT Í∏∞Î≥∏ ÏàúÏ∞® Ïã§Ìñâ")
        phase1_start_time = time.time()
        
        # Phase 1 ÏàúÏ∞® Ïã§Ìñâ
        for mode in phase1_modes:
            # ChatGPT Î™®Îìú Ïã§Ìñâ Ï°∞Í±¥
            if mode.startswith('chatgpt') and available_models.get('api', False):
                print(f"[PHASE 1] {mode} Ïã§Ìñâ ÏãúÏûë")
                result = process_single_mode(mode, "PHASE 1", local_model)
                all_results.append(result)
                print(f"[PHASE 1] {mode} ÏôÑÎ£å: ÏÑ±Í≥µ={result.get('success', False)}")
            # Î°úÏª¨ Î™®Îìú Ïã§Ìñâ Ï°∞Í±¥
            elif mode.startswith('local') and available_models.get('local', False):
                print(f"[PHASE 1] {mode} Ïã§Ìñâ ÏãúÏûë")
                result = process_single_mode(mode, "PHASE 1", local_model)
                all_results.append(result)
                print(f"[PHASE 1] {mode} ÏôÑÎ£å: ÏÑ±Í≥µ={result.get('success', False)}")
            else:
                # Ïò§Î•ò Í≤∞Í≥º ÏÉùÏÑ±
                error_result = create_error_result(mode, "PHASE 1")
                all_results.append(error_result)
                print(f"[PHASE 1] {mode} Ïä§ÌÇµ: ÏÇ¨Ïö© Î∂àÍ∞ÄÎä•")
        
        phase1_duration = time.time() - phase1_start_time
        print(f"[PHASE 1] Î™®Îì† Î™®Îìú ÏôÑÎ£å ({phase1_duration:.2f}Ï¥à)")
        
        # === PHASE 2 Ïã§Ìñâ: ChatGPT Ïª§Ïä§ÌÖÄ (ÏàúÏ∞® Ïã§Ìñâ) ===
        print(f"[PHASE 2] ÏãúÏûë - ChatGPT Ïª§Ïä§ÌÖÄ ÏàúÏ∞® Ïã§Ìñâ")
        phase2_start_time = time.time()
        
        # Phase 2 ÏàúÏ∞® Ïã§Ìñâ
        for mode in phase2_modes:
            # ChatGPT Î™®Îìú Ïã§Ìñâ Ï°∞Í±¥
            if mode.startswith('chatgpt') and available_models.get('api', False):
                print(f"[PHASE 2] {mode} Ïã§Ìñâ ÏãúÏûë")
                result = process_single_mode(mode, "PHASE 2", local_model)
                all_results.append(result)
                print(f"[PHASE 2] {mode} ÏôÑÎ£å: ÏÑ±Í≥µ={result.get('success', False)}")
            # Î°úÏª¨ Î™®Îìú Ïã§Ìñâ Ï°∞Í±¥
            elif mode.startswith('local') and available_models.get('local', False):
                print(f"[PHASE 2] {mode} Ïã§Ìñâ ÏãúÏûë")
                result = process_single_mode(mode, "PHASE 2", local_model)
                all_results.append(result)
                print(f"[PHASE 2] {mode} ÏôÑÎ£å: ÏÑ±Í≥µ={result.get('success', False)}")
            else:
                # Ïò§Î•ò Í≤∞Í≥º ÏÉùÏÑ±
                error_result = create_error_result(mode, "PHASE 2")
                all_results.append(error_result)
                print(f"[PHASE 2] {mode} Ïä§ÌÇµ: ÏÇ¨Ïö© Î∂àÍ∞ÄÎä•")
        
        phase2_duration = time.time() - phase2_start_time
        print(f"[PHASE 2] Î™®Îì† Î™®Îìú ÏôÑÎ£å ({phase2_duration:.2f}Ï¥à)")
        
        # Ï†ÑÏ≤¥ Ï≤òÎ¶¨ ÌÜµÍ≥Ñ
        total_processing_time = sum(r.get('processing_time', 0) for r in all_results)
        successful_modes = sum(1 for r in all_results if r.get('success', False))
        actual_modes_count = len(all_results)
        
        response = {
            'type': 'multi_benchmark_sequential',
            'query': query,
            'results': all_results,
            'execution_phases': {
                'phase1': {
                    'description': 'Í∏∞Î≥∏ Î∞©Î≤ïÎì§ (ÎèôÏãú Ïã§Ìñâ)',
                    'modes': phase1_modes,
                    'duration': phase1_duration,
                    'emoji': 'üè†ü§ñ'
                },
                'phase2': {
                    'description': 'Ï≤≠ÌÇπ Î∞©Î≤ïÎì§ (ÏàúÏ∞® Ïã§Ìñâ)',
                    'modes': phase2_modes,
                    'duration': phase2_duration,
                    'emoji': 'üîßüß†'
                }
            },
            'summary': {
                'total_modes': actual_modes_count,
                'successful_modes': successful_modes,
                'failed_modes': actual_modes_count - successful_modes,
                'total_processing_time': total_processing_time,
                'average_processing_time': total_processing_time / actual_modes_count if actual_modes_count else 0,
                'phase1_time': phase1_duration,
                'phase2_time': phase2_duration,
                'total_execution_time': phase1_duration + phase2_duration
            },
            'timestamp': datetime.now().isoformat()
        }
        
        print(f"[COMPLETE] ÏàúÏ∞®Ï†Å Î©ÄÌã∞ Î≤§ÏπòÎßàÌÇπ ÏôÑÎ£å - Ï¥ù ÏÜåÏöî ÏãúÍ∞Ñ: {phase1_duration + phase2_duration:.2f}Ï¥à")
        
        return jsonify(response)
        
    except Exception as e:
        return jsonify({
            'error': f'ÏàúÏ∞®Ï†Å Î©ÄÌã∞ Î≤§ÏπòÎßàÌÇπ Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}',
            'type': 'error'
        }), 500

@multi_benchmark_bp.route('/system-status', methods=['GET'])
def get_multi_benchmark_status():
    """Î©ÄÌã∞ Î≤§ÏπòÎßàÌÇπ ÏãúÏä§ÌÖú ÏÉÅÌÉú Ï°∞Ìöå"""
    try:
        rag_chain = get_enhanced_rag_chain()
        
        # Í∞Å LLM ÏÉÅÌÉú ÌôïÏù∏
        api_available = False
        local_available = False
        
        try:
            api_models = rag_chain.dual_llm.get_available_models().get('api', False)
            api_available = bool(api_models)
        except:
            pass
        
        try:
            local_models = rag_chain.dual_llm.get_available_models().get('local', False)
            local_available = bool(local_models)
        except:
            pass
        
        # Î≤°ÌÑ∞DB ÏÉÅÌÉú ÌôïÏù∏
        vectordb_available = False
        doc_count = 0
        try:
            if hasattr(rag_chain, 'vectorstore'):
                doc_count = rag_chain.vectorstore.get_document_count()
                vectordb_available = doc_count > 0
        except:
            pass
        
        status = {
            'system_ready': api_available or local_available,
            'api_llm_available': api_available,
            'local_llm_available': local_available,
            'vectordb_available': vectordb_available,
            'document_count': doc_count,
            'supported_modes': {
                'chatgpt-basic': api_available,
                'chatgpt-custom': api_available,
                'local-basic': local_available,
                'local-custom': local_available
            },
            'executor_info': {
                'max_workers': executor._max_workers,
                'thread_count': len(executor._threads) if hasattr(executor, '_threads') else 0
            }
        }
        
        return jsonify({
            'status': 'success',
            'data': status,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'error': f'ÏÉÅÌÉú Ï°∞Ìöå Ïã§Ìå®: {str(e)}'
        }), 500