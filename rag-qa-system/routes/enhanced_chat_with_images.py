from fastapi import APIRouter, HTTPException, Query
from pydantic import BaseModel
from typing import Optional, List, Dict
import time
from config import Config
from services.enhanced_rag_chain_with_images import create_enhanced_rag_chain_with_images
from services.similarity_suggestions import SimilarityBasedSuggestions
from models.dual_vectorstore import get_dual_vectorstore
import hashlib
import json

router = APIRouter()

# ê¸€ë¡œë²Œ RAG ì²´ì¸ ì¸ìŠ¤í„´ìŠ¤
rag_chain = None
suggestions_service = None

class ChatRequest(BaseModel):
    query: str
    mode: int = 1  # 1: local+basic, 2: local+custom, 3: openai+basic, 4: openai+custom
    benchmarking: bool = False

class ChatResponse(BaseModel):
    answer: str
    mode_description: str
    processing_time: float
    benchmarking_results: Optional[dict] = None
    suggestions: Optional[List[str]] = None
    images: Optional[List[Dict]] = None
    has_images: bool = False

def get_rag_chain():
    """RAG ì²´ì¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì´ë¯¸ì§€ ì²˜ë¦¬ í¬í•¨)"""
    global rag_chain
    if rag_chain is None:
        vectorstore = get_dual_vectorstore()
        rag_chain = create_enhanced_rag_chain_with_images(vectorstore)
    return rag_chain

def get_suggestions_service():
    """ì¶”ì²œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global suggestions_service
    if suggestions_service is None:
        vectorstore = get_dual_vectorstore()
        suggestions_service = SimilarityBasedSuggestions(vectorstore)
    return suggestions_service

def get_mode_info(mode: int) -> tuple:
    """ëª¨ë“œì— ë”°ë¥¸ LLMê³¼ ì²­í‚¹ íƒ€ì… ë°˜í™˜"""
    mode_map = {
        1: ("local", "basic", "ì‚¬ë‚´ì„œë²„ vLLM + s3-ê¸°ë³¸"),
        2: ("local", "custom", "ì‚¬ë‚´ì„œë²„ vLLM + s3-chunking"),
        3: ("openai", "basic", "ChatGPT + s3-ê¸°ë³¸"),
        4: ("openai", "custom", "ChatGPT + s3-chunking")
    }
    return mode_map.get(mode, ("local", "basic", "ì‚¬ë‚´ì„œë²„ vLLM + s3-ê¸°ë³¸"))

@router.post("/chat/with-images", response_model=ChatResponse)
async def chat_with_images(request: ChatRequest):
    """ì´ë¯¸ì§€ ì •ë³´ë¥¼ í¬í•¨í•œ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸"""
    try:
        start_time = time.time()
        
        # ëª¨ë“œ ì •ë³´ ì„¤ì •
        llm_type, chunking_type, mode_description = get_mode_info(request.mode)
        Config.LLM_TYPE = llm_type
        Config.BENCHMARKING_MODE = request.benchmarking
        
        # RAG ì²´ì¸ ê°€ì ¸ì˜¤ê¸°
        chain = get_rag_chain()
        suggestions_service = get_suggestions_service()
        
        print(f"\n[CHAT_WITH_IMAGES] ìš”ì²­ ì²˜ë¦¬ ì‹œì‘")
        print(f"  - ì§ˆë¬¸: {request.query}")
        print(f"  - ëª¨ë“œ: {mode_description}")
        print(f"  - ë²¤ì¹˜ë§ˆí‚¹: {request.benchmarking}")
        
        # ì‘ë‹µ ìƒì„±
        response = chain.process_query(
            query=request.query,
            top_k=5,
            chunking_type=chunking_type
        )
        
        # ì‘ë‹µ ìœ í˜• í™•ì¸
        if response.get('type') == 'error':
            raise HTTPException(status_code=500, detail=response['message'])
        
        # ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ
        images = response.get('images', [])
        has_images = len(images) > 0
        
        # ì´ë¯¸ì§€ ì •ë³´ê°€ ìˆì„ ê²½ìš° ì‘ë‹µì— ì¶”ê°€
        if has_images:
            print(f"[CHAT_WITH_IMAGES] {len(images)}ê°œì˜ ì´ë¯¸ì§€ ë°œê²¬")
            # ì‘ë‹µì— ì´ë¯¸ì§€ ì°¸ì¡° ì¶”ê°€
            if not request.benchmarking:
                answer = response.get('answer', '')
                if images and "ì´ë¯¸ì§€" not in answer:
                    answer += f"\n\nğŸ“· ê´€ë ¨ ì´ë¯¸ì§€ {len(images)}ê°œê°€ ë¬¸ì„œì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
                    response['answer'] = answer
        
        # ë²¤ì¹˜ë§ˆí‚¹ ëª¨ë“œ ì²˜ë¦¬
        if request.benchmarking and response.get('type') == 'benchmark':
            # ê° LLM ê²°ê³¼ ì •ë¦¬
            benchmark_data = {
                'local_llm': response.get('local_llm', {}),
                'openai_llm': response.get('openai_llm', {}),
                'comparison': response.get('comparison', {})
            }
            
            # ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
            if has_images:
                benchmark_data['images_found'] = len(images)
            
            # ìš°ì„  ì‘ë‹µ ê²°ì •
            if llm_type == "local":
                primary_answer = benchmark_data['local_llm'].get('answer', '')
            else:
                primary_answer = benchmark_data['openai_llm'].get('answer', '')
            
            final_response = ChatResponse(
                answer=primary_answer,
                mode_description=mode_description,
                processing_time=time.time() - start_time,
                benchmarking_results=benchmark_data,
                images=images if has_images else None,
                has_images=has_images
            )
        else:
            # ì¼ë°˜ ëª¨ë“œ ì‘ë‹µ
            answer = response.get('answer', '')
            
            # ìœ ì‚¬ ì§ˆë¬¸ ì¶”ì²œ (ì´ë¯¸ì§€ ê´€ë ¨ ì§ˆë¬¸ í¬í•¨)
            suggestions = None
            if chunking_type == 'custom' and has_images:
                # ì´ë¯¸ì§€ ê´€ë ¨ ì¶”ì²œ ì§ˆë¬¸ ì¶”ê°€
                base_suggestions = suggestions_service.get_suggestions(request.query, chunking_type)
                image_suggestions = [
                    "ì´ ë¬¸ì„œì— í¬í•¨ëœ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                    "ê´€ë ¨ ê·¸ë¦¼ì´ë‚˜ ë„í‘œê°€ ìˆë‚˜ìš”?"
                ]
                suggestions = base_suggestions[:3] + image_suggestions[:1]
            else:
                suggestions = suggestions_service.get_suggestions(request.query, chunking_type)
            
            final_response = ChatResponse(
                answer=answer,
                mode_description=mode_description,
                processing_time=time.time() - start_time,
                suggestions=suggestions[:5] if suggestions else None,
                images=images if has_images else None,
                has_images=has_images
            )
        
        print(f"[CHAT_WITH_IMAGES] ì‘ë‹µ ìƒì„± ì™„ë£Œ (ì²˜ë¦¬ì‹œê°„: {final_response.processing_time:.2f}ì´ˆ)")
        if has_images:
            print(f"[CHAT_WITH_IMAGES] ì´ë¯¸ì§€ ì •ë³´ í¬í•¨ë¨: {len(images)}ê°œ")
        
        return final_response
        
    except Exception as e:
        print(f"[CHAT_WITH_IMAGES] ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        
        raise HTTPException(
            status_code=500,
            detail=f"ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

@router.get("/chat/images/search")
async def search_images(query: str = Query(..., description="ê²€ìƒ‰ ì¿¼ë¦¬")):
    """ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ë¬¸ì„œ ê²€ìƒ‰"""
    try:
        vectorstore = get_dual_vectorstore()
        
        # custom ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰ (s3-chunking)
        results = vectorstore.similarity_search_with_score(query, "custom", k=10)
        
        image_documents = []
        for doc, score in results:
            if doc.metadata.get('has_images', False):
                images = doc.metadata.get('images', [])
                image_documents.append({
                    'content_preview': doc.page_content[:200] + '...',
                    'section': doc.metadata.get('section', ''),
                    'filename': doc.metadata.get('filename', ''),
                    'images': images,
                    'image_count': len(images),
                    'score': float(score)
                })
        
        return {
            'query': query,
            'total_results': len(results),
            'image_documents': image_documents,
            'image_document_count': len(image_documents)
        }
        
    except Exception as e:
        print(f"[IMAGE_SEARCH] ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ì´ë¯¸ì§€ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )