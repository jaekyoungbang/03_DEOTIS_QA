# RAG QA 시스템 개선 설계서

## 1. 개요
현재 RAG QA 시스템을 벤치마킹 가능한 고도화된 시스템으로 개선하기 위한 설계서입니다.

## 2. 주요 개선사항

### 2.1 LLM 설정 및 벤치마킹
- **목적**: API와 로컬 모델 선택 가능, 성능 측정
- **구현**:
  - LLM 선택 인터페이스 (OpenAI API, Local LLama, etc.)
  - 벤치마킹 메트릭: 응답시간, 정확도, 비용
  - 설정 파일에 모델별 프롬프트 템플릿 관리

### 2.2 청킹 전략 및 벤치마킹
- **기본 전략**:
  - Character-based splitting (1000자)
  - Overlap: 200자
- **커스텀 전략**:
  - Semantic chunking (의미 단위 분할)
  - Document structure-aware chunking
- **벤치마킹**: 
  - 검색 정확도
  - 청킹 시간
  - 메모리 사용량

### 2.3 질의응답 파이프라인 개선
- **질문 처리**:
  1. 자연어 질문 입력
  2. LLM을 통한 질문 벡터화 (의도 파악 포함)
  3. 벡터 DB 조회
- **답변 생성**:
  1. 관련 문서 검색
  2. LLM을 통한 답변 생성
  3. 답변 요약 및 정리

### 2.4 유사도 기반 다중 답변
- **기능**: 유사도가 비슷한 상위 3개 결과 제시
- **UI**: 
  ```
  찾은 답변들:
  1. [유사도: 0.92] 첫 번째 답변...
  2. [유사도: 0.91] 두 번째 답변...
  3. [유사도: 0.90] 세 번째 답변...
  
  이 중에서 원하시는 답변이 있으신가요?
  ```

### 2.5 API 설정 및 프롬프트 관리
- **창의성 파라미터**: temperature = 0 (일관된 답변)
- **프롬프트 템플릿**:
  ```yaml
  models:
    gpt-4:
      system_prompt: "당신은 정확한 정보를 제공하는 전문가입니다..."
      query_prompt: "다음 문서를 기반으로 질문에 답변하세요..."
    llama3:
      system_prompt: "You are a helpful assistant..."
  ```

### 2.6 시스템 정보 표시
- **표시 정보**:
  - LLM 모델명
  - 임베딩 모델
  - Python 버전 및 포트
  - 벤치마킹 점수

## 3. 시스템 아키텍처

```
┌─────────────────┐     ┌──────────────┐     ┌──────────────┐
│   Web UI        │────▶│  Flask API   │────▶│  RAG Engine  │
└─────────────────┘     └──────────────┘     └──────────────┘
                               │                      │
                               ▼                      ▼
                        ┌──────────────┐     ┌──────────────┐
                        │ Benchmarking │     │  Vector DB   │
                        │   Module     │     │  (ChromaDB)  │
                        └──────────────┘     └──────────────┘
```

## 4. 구현 계획

### Phase 1: 기본 구조 개선
- config.py 확장 (모델 선택, 프롬프트 관리)
- benchmarking.py 모듈 생성
- 다중 LLM 지원 구조

### Phase 2: 청킹 전략
- chunking_strategies.py 모듈
- 벤치마킹 테스트 케이스

### Phase 3: 질의응답 고도화
- 유사도 기반 다중 답변
- 답변 요약 기능

### Phase 4: UI/UX 개선
- 시스템 정보 대시보드
- 벤치마킹 결과 시각화

## 5. 벤치마킹 메트릭

### 5.1 LLM 성능
- 응답 시간 (ms)
- 토큰 사용량
- 비용 (API의 경우)
- 정확도 (ground truth 대비)

### 5.2 청킹 성능
- 청킹 처리 시간
- 청크 품질 (의미 단위 보존)
- 검색 정확도

### 5.3 전체 시스템
- End-to-end 응답 시간
- 메모리 사용량
- 동시 처리 가능 요청 수

## 6. 실행 환경

```yaml
system_info:
  python_version: "3.9+"
  port: 5000
  redis: "선택사항 (캐싱용)"
  
models:
  llm: 
    - "gpt-4-turbo"
    - "gpt-3.5-turbo" 
    - "llama3-8b"
  embedding:
    - "text-embedding-ada-002"
    - "sentence-transformers/all-MiniLM-L6-v2"
```

## 7. 평가 기준 (심사용)
- 자연어 질문 처리 능력
- 답변의 정확성
- 시스템 성능 (속도, 효율성)
- 사용자 경험