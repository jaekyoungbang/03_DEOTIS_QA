# vLLM ì„œë²„ ì—°ë™ ì„¤ì • ê°€ì´ë“œ

## ğŸ“‹ ì„¤ì • ì™„ë£Œëœ ë³€ê²½ì‚¬í•­

### 1. ğŸ  ë¡œì»¬LLM + s3ê¸°ë³¸ ì‹œìŠ¤í…œ ë³€ê²½ì‚¬í•­

#### A. Yanolja LLM ëª¨ë¸ (`models/yanolja_llm.py`)
- **ë³€ê²½ ì „**: Ollama API (`http://192.168.0.224:11434/api/generate`)
- **ë³€ê²½ í›„**: vLLM OpenAI í˜¸í™˜ API (`http://192.168.0.224:8701/v1/chat/completions`)
- **ëª¨ë¸ëª…**: `yanolja/EEVE-Korean-Instruct-10.8B-v1.0`

#### B. ì„¤ì • íŒŒì¼ (`config.py`)
- **local LLM provider**: `ollama` â†’ `vllm`
- **base_url**: `http://192.168.0.224:8701`
- **embedding**: ê¸°ë³¸ê°’ì„ `bge-m3`ë¡œ ë³€ê²½

### 2. ğŸ”§ ë¡œì»¬LLM + s3-chunking ì‹œìŠ¤í…œ ë³€ê²½ì‚¬í•­

#### A. Yanolja íŠ¹í™” ì„¤ì • (`yanolja_config.py`)
- **ëª¨ë“  ëª¨ë¸ íƒ€ì…**ì„ vLLM ì„œë²„ë¡œ ë³€ê²½:
  - Travel AI
  - Hotel AI  
  - Customer Service AI
  - General AI
- **base_url ì¶”ê°€**: ê° ëª¨ë¸ì— `http://192.168.0.224:8701` ì„¤ì •

### 3. ğŸ¯ ì„ë² ë”© ëª¨ë¸ ì„¤ì • (`models/embeddings.py`)
- **BGE-M3 ê¸°ë³¸ ì‚¬ìš©**: `USE_OLLAMA_BGE_M3=true` (ê¸°ë³¸ê°’)
- **ì„œë²„**: `http://192.168.0.224:11434` (Ollama ì„œë²„ ì‚¬ìš©)
- **ëª¨ë¸**: `bge-m3:latest`
- **ì°¨ì›**: 1024

## ğŸš€ ì‚¬ìš©ë²•

### 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```bash
# .env.vllm íŒŒì¼ì„ .envë¡œ ë³µì‚¬í•˜ì—¬ ì‚¬ìš©
cp .env.vllm .env
```

### 2. ì„œë²„ í™•ì¸
vLLM ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸:
```bash
curl http://192.168.0.224:8701/v1/models
```

Ollama ì„œë²„ (ì„ë² ë”©ìš©) í™•ì¸:
```bash
curl http://192.168.0.224:11434/api/tags
```

### 3. ì‹œìŠ¤í…œ ì‹¤í–‰
```bash
# ê¸°ë³¸ ì‹œìŠ¤í…œ
python app.py

# Yanolja íŠ¹í™” ì‹œìŠ¤í…œ  
python yanolja_app.py
```

## ğŸ“Š ëª¨ë¸ ì •ë³´

### LLM ëª¨ë¸
- **ì´ë¦„**: `yanolja/EEVE-Korean-Instruct-10.8B-v1.0`
- **ì„œë²„**: vLLM (192.168.0.224:8701)
- **API**: OpenAI í˜¸í™˜ (`/v1/chat/completions`)

### ì„ë² ë”© ëª¨ë¸  
- **ì´ë¦„**: `bge-m3:latest`
- **ì„œë²„**: Ollama (192.168.0.224:11434)
- **API**: Ollama ì„ë² ë”© API (`/api/embeddings`)
- **ì°¨ì›**: 1024

## ğŸ”§ ì£¼ìš” ì„¤ì • íŒŒì¼ë“¤

| íŒŒì¼ | ì—­í•  | ë³€ê²½ ë‚´ìš© |
|------|------|-----------|
| `models/yanolja_llm.py` | vLLM API ì—°ë™ | Ollama â†’ vLLM API ë³€ê²½ |
| `models/ollama_embeddings.py` | BGE-M3 ì„ë² ë”© | IP ì£¼ì†Œ ì—…ë°ì´íŠ¸ |
| `models/embeddings.py` | ì„ë² ë”© ê´€ë¦¬ì | BGE-M3 ê¸°ë³¸ ì‚¬ìš© |
| `config.py` | ê¸°ë³¸ ì„¤ì • | vLLM provider ì¶”ê°€ |
| `yanolja_config.py` | Yanolja íŠ¹í™” ì„¤ì • | ëª¨ë“  ëª¨ë¸ vLLM ì—°ë™ |
| `.env.vllm` | í™˜ê²½ ë³€ìˆ˜ | vLLM ì„œë²„ ì„¤ì • |

## ğŸ” í…ŒìŠ¤íŠ¸ ë°©ë²•

### 1. LLM ì—°ê²° í…ŒìŠ¤íŠ¸
```python
from models.yanolja_llm import YanoljaLLM

llm = YanoljaLLM()
if llm.test_connection():
    print("âœ… vLLM ì—°ê²° ì„±ê³µ")
    response = llm.invoke("ì•ˆë…•í•˜ì„¸ìš”")
    print(f"ì‘ë‹µ: {response.content}")
```

### 2. ì„ë² ë”© ì—°ê²° í…ŒìŠ¤íŠ¸
```python
from models.embeddings import EmbeddingManager

em = EmbeddingManager()
info = em.get_embedding_info()
print(f"ì„ë² ë”© ì •ë³´: {info}")

# ì„ë² ë”© í…ŒìŠ¤íŠ¸
embedding = em.embed_text("í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸")
print(f"ì„ë² ë”© ì°¨ì›: {len(embedding)}")
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ì„œë²„ ìˆœì„œ**: vLLM ì„œë²„ì™€ Ollama ì„œë²„ ëª¨ë‘ ì‹¤í–‰ë˜ì–´ì•¼ í•¨
2. **í¬íŠ¸ ë¶„ë¦¬**: 
   - vLLM: 8701 (LLM)
   - Ollama: 11434 (ì„ë² ë”©)
3. **ëª¨ë¸ í™•ì¸**: ê° ì„œë²„ì— í•„ìš”í•œ ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
4. **ë°©í™”ë²½**: 192.168.0.224 ì„œë²„ì˜ í¬íŠ¸ê°€ ì—´ë ¤ìˆëŠ”ì§€ í™•ì¸

## ğŸ“ ì ìš©ëœ ìœ„ì¹˜

### ğŸ  ë¡œì»¬LLM + s3ê¸°ë³¸
- `/03_DEOTIS_QA/rag-qa-system/` ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ê´€ë ¨ íŒŒì¼

### ğŸ”§ ë¡œì»¬LLM + s3-chunking  
- ë™ì¼í•œ ë””ë ‰í† ë¦¬ì—ì„œ `yanolja_config.py` ê¸°ë°˜ ì„¤ì •

ëª¨ë“  ì„¤ì •ì´ ì†ŒìŠ¤ì½”ë“œì—ì„œ ë³€ê²½ ê°€ëŠ¥í•œ êµ¬ì¡°ë¡œ ë˜ì–´ ìˆì–´, í•„ìš”ì‹œ í™˜ê²½ë³€ìˆ˜ë‚˜ ì„¤ì • íŒŒì¼ì„ í†µí•´ ì‰½ê²Œ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.