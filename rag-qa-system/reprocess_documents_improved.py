#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê°œì„ ëœ ì²­í‚¹ ì „ëµìœ¼ë¡œ ë¬¸ì„œ ì¬ì²˜ë¦¬
"""

import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from services.document_processor import DocumentProcessor
from services.chunking_strategies import get_chunking_strategy
from models.embeddings import EmbeddingManager
from models.vectorstore import DualVectorStoreManager
import shutil
from config import Config

def reprocess_documents():
    """ê°œì„ ëœ ì²­í‚¹ ì „ëµìœ¼ë¡œ ë¬¸ì„œ ì¬ì²˜ë¦¬"""
    
    print("ğŸ”„ ê°œì„ ëœ ì²­í‚¹ ì „ëµìœ¼ë¡œ ë¬¸ì„œ ì¬ì²˜ë¦¬ ì‹œì‘...")
    
    # VectorDB ë°±ì—…
    persist_dir = Config.CHROMA_PERSIST_DIRECTORY
    backup_dir = persist_dir + "_backup_before_improved_chunking"
    
    if os.path.exists(persist_dir):
        print(f"ğŸ“ ê¸°ì¡´ VectorDB ë°±ì—… ì¤‘... ({backup_dir})")
        if os.path.exists(backup_dir):
            shutil.rmtree(backup_dir)
        shutil.copytree(persist_dir, backup_dir)
        print("âœ… ë°±ì—… ì™„ë£Œ")
        
        # ê¸°ì¡´ VectorDB ì‚­ì œ
        print("ğŸ—‘ï¸ ê¸°ì¡´ VectorDB ì‚­ì œ ì¤‘...")
        shutil.rmtree(persist_dir)
        print("âœ… ì‚­ì œ ì™„ë£Œ")
    
    # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    print("\nğŸ”§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    doc_processor = DocumentProcessor()
    embedding_manager = EmbeddingManager()
    vectorstore_manager = DualVectorStoreManager(embedding_manager.get_embeddings())
    
    # S3 í´ë”ë“¤ ê²½ë¡œ
    s3_folders = {
        "s3": "/mnt/d/99_DEOTIS_QA_SYSTEM/03_DEOTIS_QA/s3",
        "s3-chunking": "/mnt/d/99_DEOTIS_QA_SYSTEM/03_DEOTIS_QA/s3-chunking"
    }
    
    # ì§€ì›ë˜ëŠ” íŒŒì¼ í™•ì¥ì
    supported_extensions = ['.txt', '.docx', '.pdf', '.md']
    
    # ê° í´ë”ë³„ ë¬¸ì„œ ì²˜ë¦¬
    for folder_type, s3_folder in s3_folders.items():
        print(f"\nğŸ“‚ í´ë” ì²˜ë¦¬ ì¤‘: {s3_folder} ({folder_type})")
        
        if not os.path.exists(s3_folder):
            print(f"âš ï¸ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {s3_folder}")
            continue
        
        documents_loaded = 0
        total_chunks = 0
        
        for root, dirs, files in os.walk(s3_folder):
            for file in files:
                file_path = os.path.join(root, file)
                file_extension = os.path.splitext(file)[1].lower()
                
                # ì„ì‹œ íŒŒì¼ ì œì™¸
                if file.startswith('~$') or file.startswith('.'):
                    continue
                
                if file_extension in supported_extensions:
                    print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {file}")
                    
                    try:
                        # ë¬¸ì„œ ì½ê¸°
                        documents = doc_processor.load_document(file_path)
                        
                        if documents:
                            # ê°œì„ ëœ ì²­í‚¹ ì „ëµ ì ìš©
                            if folder_type == "s3":
                                # ê¸°ë³¸ ì²­í‚¹ (ê°œì„ ëœ ë²„ì „)
                                strategy = get_chunking_strategy('basic')
                                chunks = strategy.split_documents(documents)
                                collection_name = "basic"
                            else:
                                # ì»¤ìŠ¤í…€ ì²­í‚¹
                                strategy = get_chunking_strategy('custom_delimiter')
                                chunks = strategy.split_documents(documents)
                                collection_name = "custom"
                            
                            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                            for chunk in chunks:
                                chunk.metadata['source'] = file_path
                                chunk.metadata['filename'] = file
                            
                            # ì²­í¬ ë¶„ì„ (ì‹ ìš©ì¹´ë“œì•Œëœ°ì´ìš©ë²• ê´€ë ¨)
                            problematic_chunks = 0
                            for chunk in chunks:
                                chunk_content = chunk.page_content
                                if "íšŒì›ì œ ì—…ì†Œ" in chunk_content and "ì‹ ìš©ì¹´ë“œì•Œëœ°ì´ìš©ë²•" in chunk_content:
                                    problematic_chunks += 1
                            
                            if problematic_chunks > 0:
                                print(f"   âš ï¸ ê²½ê³ : {problematic_chunks}ê°œ ì²­í¬ì—ì„œ 'íšŒì›ì œ ì—…ì†Œ'ì™€ 'ì‹ ìš©ì¹´ë“œì•Œëœ°ì´ìš©ë²•'ì´ í•¨ê»˜ ë°œê²¬ë¨")
                            
                            # ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€
                            vectorstore_manager.add_documents(chunks, collection_name)
                            
                            print(f"   âœ… {len(chunks)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ ({collection_name})")
                            documents_loaded += 1
                            total_chunks += len(chunks)
                            
                    except Exception as e:
                        print(f"   âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        print(f"\nğŸ“Š {folder_type} í´ë” ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"   - ë¬¸ì„œ ìˆ˜: {documents_loaded}ê°œ")
        print(f"   - ì²­í¬ ìˆ˜: {total_chunks}ê°œ")
    
    # ì „ì²´ í†µê³„
    counts = vectorstore_manager.get_document_count()
    print(f"\nğŸ“ˆ ì „ì²´ ì €ì¥ í†µê³„:")
    print(f"   - ê¸°ë³¸ ì²­í‚¹: {counts.get('basic', 0)}ê°œ")
    print(f"   - ì»¤ìŠ¤í…€ ì²­í‚¹: {counts.get('custom', 0)}ê°œ")
    print(f"   - ì „ì²´: {counts.get('total', 0)}ê°œ")
    
    # ì‹ ìš©ì¹´ë“œì•Œëœ°ì´ìš©ë²• ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\nğŸ” 'ì‹ ìš©ì¹´ë“œì•Œëœ°ì´ìš©ë²•' ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
    test_results = vectorstore_manager.similarity_search_with_score("ì‹ ìš©ì¹´ë“œì•Œëœ°ì´ìš©ë²•", "basic", k=3)
    
    for i, (doc, score) in enumerate(test_results, 1):
        content = doc.page_content
        print(f"\nìˆœìœ„ {i}: ì ìˆ˜ {score:.2%}")
        print(f"íŒŒì¼: {doc.metadata.get('source', 'Unknown')}")
        
        # ë¬¸ì œê°€ ìˆëŠ”ì§€ í™•ì¸
        if "íšŒì›ì œ ì—…ì†Œ" in content and "ì‹ ìš©ì¹´ë“œì•Œëœ°ì´ìš©ë²•" in content:
            print("âš ï¸ ê²½ê³ : ì—¬ì „íˆ 'íšŒì›ì œ ì—…ì†Œ'ì™€ 'ì‹ ìš©ì¹´ë“œì•Œëœ°ì´ìš©ë²•'ì´ ê°™ì€ ì²­í¬ì— ìˆìŠµë‹ˆë‹¤!")
            idx1 = content.find("íšŒì›ì œ ì—…ì†Œ")
            idx2 = content.find("ì‹ ìš©ì¹´ë“œì•Œëœ°ì´ìš©ë²•")
            print(f"   ê±°ë¦¬: {abs(idx2 - idx1)}ì")
        else:
            print("âœ… ì •ìƒ: ì„¹ì…˜ì´ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        print(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content[:200]}...")

if __name__ == "__main__":
    reprocess_documents()