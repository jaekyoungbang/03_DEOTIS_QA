#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ ë¡œì»¬ LLM ì„œë²„ (Transformers ê¸°ë°˜)
Ollama ëŒ€ì‹  ì‘ì€ ëª¨ë¸ë¡œ ë¡œì»¬ LLMì„ ì œê³µí•©ë‹ˆë‹¤.
"""
import asyncio
from flask import Flask, request, jsonify
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
import threading
import time

app = Flask(__name__)

# ì „ì—­ ë³€ìˆ˜
llm_pipeline = None
model_name = "microsoft/DialoGPT-small"  # ì‘ê³  ë¹ ë¥¸ ëŒ€í™”í˜• ëª¨ë¸

def initialize_model():
    """ëª¨ë¸ ì´ˆê¸°í™”"""
    global llm_pipeline
    try:
        print("ğŸ¤– ë¡œì»¬ LLM ëª¨ë¸ ë¡œë”© ì¤‘...")
        print(f"ğŸ“¦ ëª¨ë¸: {model_name}")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        device = 0 if torch.cuda.is_available() else -1
        device_name = "GPU" if device == 0 else "CPU"
        print(f"ğŸ’» ë””ë°”ì´ìŠ¤: {device_name}")
        
        # íŒŒì´í”„ë¼ì¸ ìƒì„± (ì‘ì€ ëª¨ë¸ ì‚¬ìš©)
        llm_pipeline = pipeline(
            "text-generation",
            model=model_name,
            device=device,
            max_length=512,
            do_sample=True,
            temperature=0.7,
            pad_token_id=50256
        )
        
        print("âœ… ë¡œì»¬ LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return False

@app.route('/api/tags', methods=['GET'])
def get_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
    return jsonify({
        "models": [
            {
                "name": "llama3.2",
                "modified_at": "2024-01-01T00:00:00Z",
                "size": 1234567890,
                "digest": "local_llm",
                "details": {
                    "family": "llama",
                    "format": "transformers",
                    "parameter_size": "124M"
                }
            }
        ]
    })

@app.route('/api/version', methods=['GET'])
def get_version():
    """API ë²„ì „ ì •ë³´"""
    return jsonify({
        "version": "0.1.0-local"
    })

@app.route('/api/generate', methods=['POST'])
def generate():
    """í…ìŠ¤íŠ¸ ìƒì„±"""
    global llm_pipeline
    
    if llm_pipeline is None:
        return jsonify({"error": "Model not loaded"}), 500
    
    try:
        data = request.json
        prompt = data.get('prompt', '')
        model = data.get('model', 'llama3.2')
        
        if not prompt:
            return jsonify({"error": "No prompt provided"}), 400
        
        print(f"ğŸ” ì§ˆë¬¸: {prompt[:100]}...")
        
        # í•œêµ­ì–´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        system_prompt = "You are a helpful assistant that provides accurate and concise answers in Korean. Answer based on the given context."
        full_prompt = f"{system_prompt}\n\nQuestion: {prompt}\nAnswer:"
        
        # í…ìŠ¤íŠ¸ ìƒì„±
        start_time = time.time()
        outputs = llm_pipeline(
            full_prompt,
            max_new_tokens=200,
            num_return_sequences=1,
            temperature=0.7,
            do_sample=True,
            pad_token_id=llm_pipeline.tokenizer.eos_token_id
        )
        
        generated_text = outputs[0]['generated_text']
        # ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        answer = generated_text.split("Answer:")[-1].strip()
        
        processing_time = time.time() - start_time
        print(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ ({processing_time:.2f}ì´ˆ)")
        print(f"ğŸ’¬ ë‹µë³€: {answer[:100]}...")
        
        return jsonify({
            "model": model,
            "created_at": "2024-01-01T00:00:00Z",
            "response": answer,
            "done": True,
            "total_duration": int(processing_time * 1000000000),  # ë‚˜ë…¸ì´ˆ
            "load_duration": 0,
            "prompt_eval_count": len(prompt.split()),
            "eval_count": len(answer.split()),
            "eval_duration": int(processing_time * 1000000000)
        })
        
    except Exception as e:
        print(f"âŒ ìƒì„± ì˜¤ë¥˜: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/chat', methods=['POST'])
def chat():
    """ì±„íŒ… ì™„ë£Œ"""
    global llm_pipeline
    
    if llm_pipeline is None:
        return jsonify({"error": "Model not loaded"}), 500
    
    try:
        data = request.json
        messages = data.get('messages', [])
        model = data.get('model', 'llama3.2')
        
        if not messages:
            return jsonify({"error": "No messages provided"}), 400
        
        # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
        user_message = None
        for msg in reversed(messages):
            if msg.get('role') == 'user':
                user_message = msg.get('content', '')
                break
        
        if not user_message:
            return jsonify({"error": "No user message found"}), 400
        
        print(f"ğŸ’¬ ì±„íŒ… ì§ˆë¬¸: {user_message[:100]}...")
        
        # ê°„ë‹¨í•œ ë‹µë³€ ìƒì„±
        context_prompt = "You are a helpful AI assistant for BC Card customer service. Answer in Korean and be concise and helpful."
        full_prompt = f"{context_prompt}\n\nUser: {user_message}\nAssistant:"
        
        start_time = time.time()
        
        # ì…ë ¥ ê¸¸ì´ ì œí•œ (í† í° ì˜¤ë²„í”Œë¡œìš° ë°©ì§€)
        max_input_length = 200
        if len(full_prompt) > max_input_length:
            full_prompt = full_prompt[:max_input_length]
        
        try:
            outputs = llm_pipeline(
                full_prompt,
                max_new_tokens=100,  # ë” ì‘ì€ ì¶œë ¥ ê¸¸ì´
                num_return_sequences=1,
                temperature=0.3,  # ë” ë³´ìˆ˜ì ì¸ ìƒì„±
                do_sample=True,
                pad_token_id=llm_pipeline.tokenizer.eos_token_id,
                truncation=True  # ìë™ ê¸¸ì´ ì œí•œ
            )
            
            generated_text = outputs[0]['generated_text']
            answer = generated_text.split("Assistant:")[-1].strip()
            
        except Exception as inner_e:
            print(f"âš ï¸ ëª¨ë¸ ìƒì„± ì˜¤ë¥˜, ê¸°ë³¸ ì‘ë‹µ ì‚¬ìš©: {inner_e}")
            answer = f"ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì§ˆë¬¸: {user_message[:50]}... ì— ëŒ€í•œ ì •ë³´ë¥¼ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤."
        
        processing_time = time.time() - start_time
        print(f"âœ… ì±„íŒ… ì‘ë‹µ ì™„ë£Œ ({processing_time:.2f}ì´ˆ)")
        
        return jsonify({
            "model": model,
            "created_at": "2024-01-01T00:00:00Z",
            "message": {
                "role": "assistant",
                "content": answer
            },
            "done": True,
            "total_duration": int(processing_time * 1000000000),
            "load_duration": 0
        })
        
    except Exception as e:
        print(f"âŒ ì±„íŒ… ì˜¤ë¥˜: {e}")
        return jsonify({"error": str(e)}), 500

def start_server():
    """ì„œë²„ ì‹œì‘"""
    print("ğŸš€ ë¡œì»¬ LLM ì„œë²„ ì‹œì‘...")
    print("ğŸ“ ì£¼ì†Œ: http://localhost:11434")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    if not initialize_model():
        print("âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨. ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # Flask ì„œë²„ ì‹œì‘
    app.run(host='0.0.0.0', port=11434, debug=False, threaded=True)

if __name__ == '__main__':
    start_server()