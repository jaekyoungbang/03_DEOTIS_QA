"""
ì¤‘ë³µ ì²­í¬ ë¬¸ì œ í•´ê²° ìŠ¤í¬ë¦½íŠ¸
"""
import os
import sys
from pathlib import Path

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_dir = Path(__file__).parent
sys.path.append(str(current_dir))

def check_and_fix_duplicates():
    """ì¤‘ë³µ ì²­í¬ í™•ì¸ ë° ìˆ˜ì •"""
    try:
        from models.embeddings import EmbeddingManager
        from models.dual_vectorstore import DualVectorStoreManager
        
        print("=" * 60)
        print("ì¤‘ë³µ ì²­í¬ ë¬¸ì œ ì§„ë‹¨ ë° í•´ê²°")
        print("=" * 60)
        
        # ì„ë² ë”© ë§¤ë‹ˆì € ì´ˆê¸°í™”
        embedding_manager = EmbeddingManager()
        embedding_info = embedding_manager.get_embedding_info()
        print(f"ì„ë² ë”© ëª¨ë¸: {embedding_info['type']} ({embedding_info['dimension']}ì°¨ì›)")
        
        # ë“€ì–¼ ë²¡í„°ìŠ¤í† ì–´ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        dual_manager = DualVectorStoreManager(embedding_manager.get_embeddings())
        
        # Basic ë²¡í„°ìŠ¤í† ì–´ ê²€ì‚¬
        print("\nğŸ“‚ Basic ë²¡í„°ìŠ¤í† ì–´ ì¤‘ë³µ ê²€ì‚¬")
        print("-" * 40)
        basic_store = dual_manager.get_vectorstore("basic")
        if basic_store and hasattr(basic_store, '_collection'):
            try:
                results = basic_store._collection.get()
                total_docs = len(results['ids'])
                print(f"ì „ì²´ ë¬¸ì„œ ìˆ˜: {total_docs}ê°œ")
                
                # ë‚´ìš©ë³„ ê·¸ë£¹í™”ë¡œ ì¤‘ë³µ í™•ì¸
                content_groups = {}
                for i, content in enumerate(results['documents']):
                    content_hash = hash(content[:100])  # ì• 100ìë¡œ í•´ì‹œ
                    if content_hash not in content_groups:
                        content_groups[content_hash] = []
                    content_groups[content_hash].append({
                        'id': results['ids'][i],
                        'content': content[:200] + "...",
                        'metadata': results['metadatas'][i] if results['metadatas'] else {}
                    })
                
                # ì¤‘ë³µ ê·¸ë£¹ ì°¾ê¸°
                duplicate_groups = {k: v for k, v in content_groups.items() if len(v) > 1}
                print(f"ê³ ìœ  ë‚´ìš© ê·¸ë£¹: {len(content_groups)}ê°œ")
                print(f"ì¤‘ë³µ ê·¸ë£¹: {len(duplicate_groups)}ê°œ")
                
                if duplicate_groups:
                    print("\nğŸ” ì¤‘ë³µ ê·¸ë£¹ ìƒì„¸:")
                    for i, (hash_key, group) in enumerate(duplicate_groups.items()):
                        if i >= 3:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                            break
                        print(f"\n{i+1}. ì¤‘ë³µ {len(group)}ê°œ:")
                        print(f"   ë‚´ìš©: {group[0]['content']}")
                        for doc in group:
                            source = doc['metadata'].get('source', 'Unknown')
                            print(f"   - ID: {doc['id'][:20]}... ì†ŒìŠ¤: {source}")
                
            except Exception as e:
                print(f"Basic ë²¡í„°ìŠ¤í† ì–´ ê²€ì‚¬ ì˜¤ë¥˜: {e}")
        
        # Custom ë²¡í„°ìŠ¤í† ì–´ ê²€ì‚¬
        print(f"\nğŸ“‚ Custom ë²¡í„°ìŠ¤í† ì–´ ì¤‘ë³µ ê²€ì‚¬")
        print("-" * 40)
        custom_store = dual_manager.get_vectorstore("custom")
        if custom_store and hasattr(custom_store, '_collection'):
            try:
                results = custom_store._collection.get()
                total_docs = len(results['ids'])
                print(f"ì „ì²´ ë¬¸ì„œ ìˆ˜: {total_docs}ê°œ")
                
                # ë‚´ìš©ë³„ ê·¸ë£¹í™”ë¡œ ì¤‘ë³µ í™•ì¸
                content_groups = {}
                for i, content in enumerate(results['documents']):
                    content_hash = hash(content[:100])
                    if content_hash not in content_groups:
                        content_groups[content_hash] = []
                    content_groups[content_hash].append({
                        'id': results['ids'][i],
                        'content': content[:200] + "...",
                        'metadata': results['metadatas'][i] if results['metadatas'] else {}
                    })
                
                # ì¤‘ë³µ ê·¸ë£¹ ì°¾ê¸°
                duplicate_groups = {k: v for k, v in content_groups.items() if len(v) > 1}
                print(f"ê³ ìœ  ë‚´ìš© ê·¸ë£¹: {len(content_groups)}ê°œ")
                print(f"ì¤‘ë³µ ê·¸ë£¹: {len(duplicate_groups)}ê°œ")
                
                if duplicate_groups:
                    print("\nğŸ” ì¤‘ë³µ ê·¸ë£¹ ìƒì„¸:")
                    for i, (hash_key, group) in enumerate(duplicate_groups.items()):
                        if i >= 3:
                            break
                        print(f"\n{i+1}. ì¤‘ë³µ {len(group)}ê°œ:")
                        print(f"   ë‚´ìš©: {group[0]['content']}")
                        for doc in group:
                            source = doc['metadata'].get('source', 'Unknown')
                            print(f"   - ID: {doc['id'][:20]}... ì†ŒìŠ¤: {source}")
                            
            except Exception as e:
                print(f"Custom ë²¡í„°ìŠ¤í† ì–´ ê²€ì‚¬ ì˜¤ë¥˜: {e}")
        
        print("\n" + "=" * 60)
        print("ğŸ’¡ í•´ê²° ë°©ë²•:")
        print("1. ê´€ë¦¬ì í˜ì´ì§€ì—ì„œ 'ğŸ”„ BGE-M3ë¡œ ë²¡í„°DB ì´ˆê¸°í™”' í´ë¦­")
        print("2. ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ: rd /s /q data\\vectordb")
        print("3. ì„œë²„ ì¬ì‹œì‘ í›„ ë¬¸ì„œ ì¬ë¡œë”©")
        print("4. ì¤‘ë³µ ì œê±° ë¡œì§ ê°œì„  í•„ìš”")
                
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    check_and_fix_duplicates()