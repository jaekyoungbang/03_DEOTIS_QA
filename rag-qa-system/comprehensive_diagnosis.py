"""
ì¢…í•© ì§„ë‹¨ ìŠ¤í¬ë¦½íŠ¸ - ë²¡í„°DB, ë¬¸ì„œ, ì²­í‚¹ ìƒíƒœ ì „ì²´ í™•ì¸
"""
import os
import sys
from pathlib import Path
import json

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_dir = Path(__file__).parent
sys.path.append(str(current_dir))

def check_source_documents():
    """ì›ë³¸ ë¬¸ì„œ í™•ì¸"""
    print("=" * 60)
    print("ğŸ“ ì›ë³¸ ë¬¸ì„œ ìƒíƒœ í™•ì¸")
    print("=" * 60)
    
    # s3 í´ë” í™•ì¸
    s3_path = Path("../s3")
    print(f"\nğŸ“‚ s3 í´ë”: {s3_path.absolute()}")
    if s3_path.exists():
        files = list(s3_path.glob("*.docx"))
        print(f"   DOCX íŒŒì¼: {len(files)}ê°œ")
        for file in files:
            size = file.stat().st_size / 1024  # KB
            print(f"   - {file.name}: {size:.1f}KB")
    else:
        print("   âŒ s3 í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤!")
    
    # s3-chunking í´ë” í™•ì¸
    s3_chunking_path = Path("../s3-chunking")
    print(f"\nğŸ“‚ s3-chunking í´ë”: {s3_chunking_path.absolute()}")
    if s3_chunking_path.exists():
        files = list(s3_chunking_path.glob("*.docx"))
        print(f"   DOCX íŒŒì¼: {len(files)}ê°œ")
        for file in files:
            size = file.stat().st_size / 1024  # KB
            print(f"   - {file.name}: {size:.1f}KB")
    else:
        print("   âŒ s3-chunking í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤!")

def check_vectordb_structure():
    """ë²¡í„°DB êµ¬ì¡° í™•ì¸"""
    print("\n" + "=" * 60)
    print("ğŸ—„ï¸ ë²¡í„°DB êµ¬ì¡° í™•ì¸")
    print("=" * 60)
    
    vectordb_path = Path("./data/vectordb")
    if not vectordb_path.exists():
        print("âŒ ë²¡í„°DB í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # SQLite íŒŒì¼ í™•ì¸
    sqlite_file = vectordb_path / "chroma.sqlite3"
    if sqlite_file.exists():
        size = sqlite_file.stat().st_size / 1024 / 1024  # MB
        print(f"ğŸ“‹ SQLite DB: {size:.2f}MB")
    else:
        print("âŒ SQLite DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
    
    # ì»¬ë ‰ì…˜ í´ë” í™•ì¸
    collection_dirs = [d for d in vectordb_path.iterdir() if d.is_dir()]
    print(f"ğŸ“ ì»¬ë ‰ì…˜ í´ë”: {len(collection_dirs)}ê°œ")
    
    for i, collection_dir in enumerate(collection_dirs):
        if i >= 3:  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
            break
        files = list(collection_dir.glob("*"))
        total_size = sum(f.stat().st_size for f in files if f.is_file()) / 1024 / 1024  # MB
        print(f"   - {collection_dir.name}: {len(files)}ê°œ íŒŒì¼, {total_size:.2f}MB")

def check_vectordb_content():
    """ë²¡í„°DB ë‚´ìš© í™•ì¸"""
    print("\n" + "=" * 60)
    print("ğŸ“Š ë²¡í„°DB ë‚´ìš© ë¶„ì„")
    print("=" * 60)
    
    try:
        from models.embeddings import EmbeddingManager
        from models.dual_vectorstore import DualVectorStoreManager
        
        # ì„ë² ë”© ë§¤ë‹ˆì € ì´ˆê¸°í™”
        embedding_manager = EmbeddingManager()
        embedding_info = embedding_manager.get_embedding_info()
        print(f"ğŸ”§ ì„ë² ë”© ëª¨ë¸: {embedding_info['type']} ({embedding_info['dimension']}ì°¨ì›)")
        print(f"   ìƒíƒœ: {embedding_info['status']}")
        if 'server' in embedding_info:
            print(f"   ì„œë²„: {embedding_info['server']}")
        
        # ë“€ì–¼ ë²¡í„°ìŠ¤í† ì–´ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        dual_manager = DualVectorStoreManager(embedding_manager.get_embeddings())
        
        # Basic ì»¬ë ‰ì…˜ ë¶„ì„
        print(f"\nğŸ“‚ Basic ì»¬ë ‰ì…˜ (s3 í´ë”)")
        print("-" * 40)
        basic_store = dual_manager.get_vectorstore("basic")
        if basic_store and hasattr(basic_store, '_collection'):
            try:
                results = basic_store._collection.get()
                total_docs = len(results['ids'])
                print(f"   ğŸ“„ ì´ ë¬¸ì„œ: {total_docs}ê°œ")
                
                # ê³ ìœ  ë‚´ìš© ë¶„ì„
                unique_contents = set()
                source_files = {}
                
                for i, content in enumerate(results['documents']):
                    # ë‚´ìš© ê³ ìœ ì„± ì²´í¬
                    content_preview = content[:100]
                    unique_contents.add(content_preview)
                    
                    # ì†ŒìŠ¤ íŒŒì¼ ë¶„ì„
                    metadata = results['metadatas'][i] if results['metadatas'] else {}
                    source = metadata.get('source_file', 'Unknown')
                    source_files[source] = source_files.get(source, 0) + 1
                
                print(f"   ğŸ” ê³ ìœ  ë‚´ìš©: {len(unique_contents)}ê°œ")
                print(f"   ğŸ“ ì†ŒìŠ¤ íŒŒì¼ë³„ ë¶„í¬:")
                for source, count in sorted(source_files.items()):
                    print(f"      - {source}: {count}ê°œ")
                
                # ìƒ˜í”Œ ë‚´ìš© í‘œì‹œ
                if total_docs > 0:
                    print(f"\n   ğŸ“‹ ìƒ˜í”Œ ë‚´ìš© (ìµœëŒ€ 3ê°œ):")
                    for i in range(min(3, total_docs)):
                        metadata = results['metadatas'][i] if results['metadatas'] else {}
                        content = results['documents'][i][:150] + "..."
                        source = metadata.get('source_file', 'Unknown')
                        print(f"      {i+1}. [{source}] {content}")
                        
            except Exception as e:
                print(f"   âŒ Basic ì»¬ë ‰ì…˜ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        else:
            print("   âŒ Basic ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤!")
        
        # Custom ì»¬ë ‰ì…˜ ë¶„ì„
        print(f"\nğŸ“‚ Custom ì»¬ë ‰ì…˜ (s3-chunking í´ë”)")
        print("-" * 40)
        custom_store = dual_manager.get_vectorstore("custom")
        if custom_store and hasattr(custom_store, '_collection'):
            try:
                results = custom_store._collection.get()
                total_docs = len(results['ids'])
                print(f"   ğŸ“„ ì´ ë¬¸ì„œ: {total_docs}ê°œ")
                
                # ê³ ìœ  ë‚´ìš© ë¶„ì„
                unique_contents = set()
                source_files = {}
                
                for i, content in enumerate(results['documents']):
                    content_preview = content[:100]
                    unique_contents.add(content_preview)
                    
                    metadata = results['metadatas'][i] if results['metadatas'] else {}
                    source = metadata.get('source_file', 'Unknown')
                    source_files[source] = source_files.get(source, 0) + 1
                
                print(f"   ğŸ” ê³ ìœ  ë‚´ìš©: {len(unique_contents)}ê°œ")
                print(f"   ğŸ“ ì†ŒìŠ¤ íŒŒì¼ë³„ ë¶„í¬:")
                for source, count in sorted(source_files.items()):
                    print(f"      - {source}: {count}ê°œ")
                
                # ìƒ˜í”Œ ë‚´ìš© í‘œì‹œ
                if total_docs > 0:
                    print(f"\n   ğŸ“‹ ìƒ˜í”Œ ë‚´ìš© (ìµœëŒ€ 3ê°œ):")
                    for i in range(min(3, total_docs)):
                        metadata = results['metadatas'][i] if results['metadatas'] else {}
                        content = results['documents'][i][:150] + "..."
                        source = metadata.get('source_file', 'Unknown')
                        print(f"      {i+1}. [{source}] {content}")
                        
            except Exception as e:
                print(f"   âŒ Custom ì»¬ë ‰ì…˜ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        else:
            print("   âŒ Custom ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤!")
            
    except Exception as e:
        print(f"âŒ ë²¡í„°DB ë‚´ìš© í™•ì¸ ì˜¤ë¥˜: {e}")

def check_search_quality():
    """ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ğŸ” ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    try:
        from models.embeddings import EmbeddingManager
        from models.dual_vectorstore import DualVectorStoreManager
        
        embedding_manager = EmbeddingManager()
        dual_manager = DualVectorStoreManager(embedding_manager.get_embeddings())
        
        test_queries = [
            "ì¹´ë“œë°œê¸‰",
            "BCì¹´ë“œ",
            "ì‹ ìš©ì¹´ë“œ",
            "í¬ì¸íŠ¸",
            "í• ë¶€"
        ]
        
        for query in test_queries:
            print(f"\nğŸ” ê²€ìƒ‰ì–´: '{query}'")
            print("-" * 30)
            
            # Basic ê²€ìƒ‰
            try:
                basic_results = dual_manager.similarity_search_with_score(query, "basic", k=3)
                print(f"   Basic: {len(basic_results)}ê°œ ê²°ê³¼")
                if basic_results:
                    max_score = max(score for _, score in basic_results)
                    print(f"   ìµœê³  ìœ ì‚¬ë„: {max_score:.1%}")
                    # ì¤‘ë³µ ì²´í¬
                    contents = [doc.page_content[:50] for doc, _ in basic_results]
                    unique_contents = set(contents)
                    if len(unique_contents) < len(contents):
                        print(f"   âš ï¸ ì¤‘ë³µ ë°œê²¬: {len(contents) - len(unique_contents)}ê°œ")
            except Exception as e:
                print(f"   Basic ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            
            # Custom ê²€ìƒ‰
            try:
                custom_results = dual_manager.similarity_search_with_score(query, "custom", k=3)
                print(f"   Custom: {len(custom_results)}ê°œ ê²°ê³¼")
                if custom_results:
                    max_score = max(score for _, score in custom_results)
                    print(f"   ìµœê³  ìœ ì‚¬ë„: {max_score:.1%}")
                    # ì¤‘ë³µ ì²´í¬
                    contents = [doc.page_content[:50] for doc, _ in custom_results]
                    unique_contents = set(contents)
                    if len(unique_contents) < len(contents):
                        print(f"   âš ï¸ ì¤‘ë³µ ë°œê²¬: {len(contents) - len(unique_contents)}ê°œ")
            except Exception as e:
                print(f"   Custom ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")

def check_logs():
    """ë¡œê·¸ íŒŒì¼ í™•ì¸"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ ìµœê·¼ ë¡œê·¸ í™•ì¸")
    print("=" * 60)
    
    log_files = [
        "app.log",
        "server.log", 
        "server_debug.log",
        "app_latest.log"
    ]
    
    for log_file in log_files:
        log_path = Path(log_file)
        if log_path.exists():
            size = log_path.stat().st_size / 1024  # KB
            print(f"\nğŸ“„ {log_file}: {size:.1f}KB")
            
            # ìµœê·¼ 20ì¤„ ì½ê¸°
            try:
                with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:
                    lines = f.readlines()
                    recent_lines = lines[-20:] if len(lines) > 20 else lines
                    
                print("   ìµœê·¼ ë¡œê·¸:")
                for line in recent_lines:
                    line = line.strip()
                    if line and ('error' in line.lower() or 'fail' in line.lower() or 'exception' in line.lower()):
                        print(f"   âŒ {line[:100]}")
                    elif line and ('success' in line.lower() or 'complete' in line.lower()):
                        print(f"   âœ… {line[:100]}")
                    elif line and len(line) > 10:
                        print(f"   â„¹ï¸  {line[:100]}")
                        
            except Exception as e:
                print(f"   ë¡œê·¸ ì½ê¸° ì˜¤ë¥˜: {e}")
        else:
            print(f"âŒ {log_file}: íŒŒì¼ ì—†ìŒ")

def main():
    """ë©”ì¸ ì§„ë‹¨ í•¨ìˆ˜"""
    print("ğŸ” RAG ì‹œìŠ¤í…œ ì¢…í•© ì§„ë‹¨ ì‹œì‘")
    print("í˜„ì¬ ì‹œê°„:", __import__('datetime').datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    
    try:
        check_source_documents()
        check_vectordb_structure()
        check_vectordb_content()
        check_search_quality()
        check_logs()
        
        print("\n" + "=" * 60)
        print("âœ… ì§„ë‹¨ ì™„ë£Œ!")
        print("=" * 60)
        print("\nğŸ’¡ ë¬¸ì œ í•´ê²° ë°©ë²•:")
        print("1. ë²¡í„°DB ì´ˆê¸°í™”: rd /s /q data\\vectordb")
        print("2. ì„œë²„ ì¬ì‹œì‘: python app.py")
        print("3. ê´€ë¦¬ì í˜ì´ì§€ì—ì„œ ë¬¸ì„œ ì¬ë¡œë”©")
        print("4. ì´ ìŠ¤í¬ë¦½íŠ¸ë¡œ ë‹¤ì‹œ í™•ì¸")
        
    except Exception as e:
        print(f"\nâŒ ì§„ë‹¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()