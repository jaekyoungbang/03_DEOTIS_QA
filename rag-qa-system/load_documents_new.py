#!/usr/bin/env python3
"""
ê°œì„ ëœ ë¬¸ì„œ ë¡œë”© ì‹œìŠ¤í…œ
- s3 í´ë”: ê¸°ë³¸ ì²­í‚¹ (BasicChunkingStrategy)
- s3-chunking í´ë”: /$$/ êµ¬ë¶„ì ì²­í‚¹ (CustomDelimiterChunkingStrategy)
"""

import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from services.document_processor import DocumentProcessor
from services.chunking_strategies import get_chunking_strategy
from models.embeddings import EmbeddingManager
from models.vectorstore import DualVectorStoreManager
from config import Config

def clear_all_data():
    """ëª¨ë“  ë²¡í„° DB ë°ì´í„° ì‚­ì œ"""
    try:
        print("ğŸ—‘ï¸ ëª¨ë“  ë°ì´í„° ì‚­ì œ ì¤‘...")
        vector_manager = DualVectorStoreManager()
        vector_manager.clear_all_collections()
        print("âœ… ëª¨ë“  ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        return True
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {e}")
        return False

def load_s3_documents(clear_before_load=True):
    """S3 í´ë”ì™€ S3-chunking í´ë”ì˜ ë¬¸ì„œë¥¼ ë¶„ë¦¬í•˜ì—¬ ë²¡í„° DBì— ì €ì¥"""
    
    print("ğŸš€ ê°œì„ ëœ ë¬¸ì„œ ë¡œë”© ì‹œìŠ¤í…œ ì‹œì‘...")
    
    # S3 í´ë”ë“¤ ê²½ë¡œ - í™˜ê²½ë³„ ìë™ ê°ì§€
    import platform
    if platform.system() == "Windows" or os.name == "nt":
        # Windows CMD í™˜ê²½
        s3_folders = {
            "s3": r"D:\99_DEOTIS_QA_SYSTEM\03_DEOTIS_QA\s3",
            "s3-chunking": r"D:\99_DEOTIS_QA_SYSTEM\03_DEOTIS_QA\s3-chunking"
        }
        print("ğŸªŸ Windows í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘")
    else:
        # WSL/Linux í™˜ê²½
        s3_folders = {
            "s3": "/mnt/d/99_DEOTIS_QA_SYSTEM/03_DEOTIS_QA/s3",
            "s3-chunking": "/mnt/d/99_DEOTIS_QA_SYSTEM/03_DEOTIS_QA/s3-chunking"
        }
        print("ğŸ§ WSL/Linux í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘")
    
    # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    print("ğŸ”§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    doc_processor = DocumentProcessor()
    embedding_manager = EmbeddingManager()
    vectorstore_manager = DualVectorStoreManager(embedding_manager.get_embeddings())
    
    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì˜µì…˜)
    if clear_before_load:
        print("ğŸ—‘ï¸ ê¸°ì¡´ ë²¡í„° DB ë°ì´í„° ì‚­ì œ ì¤‘...")
        try:
            from models.vectorstore import reset_vectorstore
            reset_vectorstore()
            print("âœ… ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {e}")
    
    # ì§€ì›ë˜ëŠ” íŒŒì¼ í™•ì¥ì
    supported_extensions = ['.txt', '.docx', '.pdf', '.md']
    
    # ê° í´ë”ë³„ íŒŒì¼ ì°¾ê¸°
    total_documents_loaded = 0
    total_chunks = 0
    
    # ì „ì—­ ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì»¬ë ‰ì…˜ë³„ ì¤‘ë³µ ì²´í¬
    global_seen_content = {
        'basic': set(),
        'custom': set()
    }
    
    # ì „ì²´ ì‹œìŠ¤í…œì—ì„œ ì ˆëŒ€ ì¤‘ë³µ ì œê±° (ì»¬ë ‰ì…˜ ë¬´ê´€)
    global_absolute_content = set()
    
    for folder_type, s3_folder in s3_folders.items():
        print(f"\nğŸ“‚ í´ë” ì²˜ë¦¬ ì¤‘: {s3_folder} ({folder_type})")
        
        if not os.path.exists(s3_folder):
            print(f"âš ï¸ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {s3_folder}")
            continue
        
        folder_documents_loaded = 0
        folder_chunks = 0
        
        for root, dirs, files in os.walk(s3_folder):
            for file in files:
                file_path = os.path.join(root, file)
                file_extension = os.path.splitext(file)[1].lower()
                
                # ì„ì‹œ íŒŒì¼ ì œì™¸
                if file.startswith('~$') or file.startswith('.'):
                    print(f"   â­ï¸ ì„ì‹œ íŒŒì¼ ê±´ë„ˆë›°ê¸°: {file}")
                    continue
                
                # s3-chunking í´ë”ëŠ” MD íŒŒì¼ë§Œ ì²˜ë¦¬
                if folder_type == "s3-chunking" and file_extension != '.md':
                    continue
                
                # s3-chunking í´ë”ëŠ” íŠ¹ì • MD íŒŒì¼ë§Œ ì²˜ë¦¬ (ì™„ì „íŒ, ìµœì í™” íŒŒì¼)
                if folder_type == "s3-chunking" and file_extension == '.md':
                    if not ('ì™„ì „íŒ' in file or 'ìµœì í™”' in file):
                        print(f"   â­ï¸ s3-chunking: ëŒ€ìƒ ì™¸ MD íŒŒì¼ ê±´ë„ˆë›°ê¸°: {file}")
                        continue
                
                if file_extension in supported_extensions:
                    print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {file}")
                    
                    try:
                        # ë¬¸ì„œ ë¡œë“œ
                        documents = doc_processor.load_document(file_path)
                        
                        if not documents:
                            print(f"   âš ï¸ ë¬¸ì„œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {file}")
                            continue
                        
                        # ì²­í‚¹ ì „ëµ ì„ íƒ
                        if folder_type == "s3":
                            # s3 í´ë”: ê¸°ë³¸ ì²­í‚¹ (ê°œì„ ëœ ë²„ì „)
                            strategy = get_chunking_strategy('basic')
                            collection_name = "basic"
                            print(f"   ğŸ”§ ê¸°ë³¸ ì²­í‚¹ ì ìš©")
                        else:
                            # s3-chunking í´ë”: /$$/ êµ¬ë¶„ì ì²­í‚¹
                            strategy = get_chunking_strategy('custom_delimiter')
                            collection_name = "custom"
                            print(f"   ğŸ”§ /$$/ êµ¬ë¶„ì ì²­í‚¹ ì ìš©")
                        
                        # ì²­í‚¹ ìˆ˜í–‰
                        chunks = strategy.split_documents(documents)
                        
                        # ë©”íƒ€ë°ì´í„° ë³´ê°•
                        for chunk in chunks:
                            chunk.metadata.update({
                                'source': file_path,
                                'filename': file,
                                'folder_type': folder_type,
                                'processing_timestamp': os.path.getmtime(file_path)
                            })
                        
                        # ì¤‘ë³µ ì²­í¬ ì œê±° - ì „ì—­ ë° ë¡œì»¬ ì¤‘ë³µ ì²´í¬
                        unique_chunks = []
                        seen_content = set()
                        
                        for chunk in chunks:
                            content_hash = hash(chunk.page_content.strip())
                            
                            # ë¡œì»¬ íŒŒì¼ ë‚´ ì¤‘ë³µ ì²´í¬
                            if content_hash in seen_content:
                                continue
                            
                            # ì ˆëŒ€ ì¤‘ë³µ ì²´í¬ (ì „ì²´ ì‹œìŠ¤í…œì—ì„œ ìœ ì¼)
                            if content_hash in global_absolute_content:
                                print(f"   ğŸš« ì ˆëŒ€ ì¤‘ë³µ ì œê±°: {repr(chunk.page_content[:50])}...")
                                continue
                                
                            # ì»¬ë ‰ì…˜ë³„ ì¤‘ë³µ ì²´í¬
                            if content_hash in global_seen_content[collection_name]:
                                print(f"   ğŸ”„ ì»¬ë ‰ì…˜ ì¤‘ë³µ ì œê±°: {repr(chunk.page_content[:50])}...")
                                continue
                            
                            # ì¤‘ë³µì´ ì•„ë‹ˆë©´ ì¶”ê°€
                            seen_content.add(content_hash)
                            global_seen_content[collection_name].add(content_hash)
                            global_absolute_content.add(content_hash)
                            unique_chunks.append(chunk)
                        
                        # ì¤‘ë³µ ì œê±° ê²°ê³¼ ì¶œë ¥
                        if len(chunks) != len(unique_chunks):
                            print(f"   ğŸ”„ ì¤‘ë³µ ì œê±°: {len(chunks)}ê°œ â†’ {len(unique_chunks)}ê°œ")
                        
                        # ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€
                        vectorstore_manager.add_documents(unique_chunks, collection_name)
                        
                        print(f"   âœ… {len(unique_chunks)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ ({collection_name} ì»¬ë ‰ì…˜)")
                        
                        folder_documents_loaded += 1
                        folder_chunks += len(unique_chunks)
                        
                        # íŠ¹ì • í‚¤ì›Œë“œ ë¶„ì„ (ì‹ ìš©ì¹´ë“œì•Œëœ°ì´ìš©ë²• ë“±)
                        problematic_chunks = 0
                        smart_usage_chunks = 0
                        
                        for chunk in chunks:
                            content = chunk.page_content
                            if "íšŒì›ì œ ì—…ì†Œ" in content and "ì‹ ìš©ì¹´ë“œì•Œëœ°ì´ìš©ë²•" in content:
                                problematic_chunks += 1
                            if "ì‹ ìš©ì¹´ë“œì•Œëœ°ì´ìš©ë²•" in content:
                                smart_usage_chunks += 1
                        
                        if problematic_chunks > 0:
                            print(f"   âš ï¸ ê²½ê³ : {problematic_chunks}ê°œ ì²­í¬ì—ì„œ ì„¹ì…˜ í˜¼ì¬ ë°œê²¬")
                        if smart_usage_chunks > 0:
                            print(f"   ğŸ“Š 'ì‹ ìš©ì¹´ë“œì•Œëœ°ì´ìš©ë²•' í¬í•¨ ì²­í¬: {smart_usage_chunks}ê°œ")
                            
                    except Exception as e:
                        print(f"   âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        print(f"\nğŸ“Š {folder_type} í´ë” ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"   - ì²˜ë¦¬ëœ ë¬¸ì„œ: {folder_documents_loaded}ê°œ")
        print(f"   - ìƒì„±ëœ ì²­í¬: {folder_chunks}ê°œ")
        
        total_documents_loaded += folder_documents_loaded
        total_chunks += folder_chunks
    
    # ìµœì¢… í†µê³„
    try:
        counts = vectorstore_manager.get_document_count()
        print(f"\nğŸ“ˆ ìµœì¢… ì €ì¥ í†µê³„:")
        print(f"   - ê¸°ë³¸ ì²­í‚¹ (s3): {counts.get('basic', 0)}ê°œ")
        print(f"   - ì»¤ìŠ¤í…€ ì²­í‚¹ (s3-chunking): {counts.get('custom', 0)}ê°œ")
        print(f"   - ì „ì²´: {counts.get('total', 0)}ê°œ")
    except Exception as e:
        print(f"âš ï¸ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    # ì¤‘ë³µ ì œê±° í†µê³„
    print(f"\nğŸ”„ ì „ì—­ ì¤‘ë³µ ì œê±° í†µê³„:")
    print(f"   - ê¸°ë³¸ ì²­í‚¹ ê³ ìœ  ì½˜í…ì¸ : {len(global_seen_content['basic'])}ê°œ")
    print(f"   - ì»¤ìŠ¤í…€ ì²­í‚¹ ê³ ìœ  ì½˜í…ì¸ : {len(global_seen_content['custom'])}ê°œ")
    print(f"   - ì „ì²´ ì ˆëŒ€ ê³ ìœ  ì½˜í…ì¸ : {len(global_absolute_content)}ê°œ")
    print(f"   - ì»¬ë ‰ì…˜ë³„ í•©ê³„: {len(global_seen_content['basic']) + len(global_seen_content['custom'])}ê°œ")
    
    print(f"\nğŸ‰ ë¬¸ì„œ ë¡œë”© ì™„ë£Œ!")
    print(f"   - ì´ ì²˜ë¦¬ëœ ë¬¸ì„œ: {total_documents_loaded}ê°œ")
    print(f"   - ì´ ìƒì„±ëœ ì²­í¬: {total_chunks}ê°œ")
    print(f"   - ì¤‘ë³µ ì œê±° í›„ ì ˆëŒ€ ê³ ìœ  ì²­í¬: {len(global_absolute_content)}ê°œ")
    
    return total_documents_loaded, total_chunks

if __name__ == "__main__":
    # ë¬¸ì„œ ë¡œë”© ì‹¤í–‰
    docs_loaded, chunks_created = load_s3_documents(clear_before_load=True)