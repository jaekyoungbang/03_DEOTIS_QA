from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    TextLoader,
    UnstructuredMarkdownLoader
)
try:
    from langchain_community.document_loaders import PyPDFLoader
except ImportError:
    PyPDFLoader = None
try:
    from langchain_community.document_loaders import Docx2txtLoader
except ImportError:
    Docx2txtLoader = None
from langchain.schema import Document
from config import Config
import os
from typing import List

class DocumentProcessor:
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        self.loaders = {
            '.txt': TextLoader,
            '.md': UnstructuredMarkdownLoader
        }
        if PyPDFLoader:
            self.loaders['.pdf'] = PyPDFLoader
        if Docx2txtLoader:
            self.loaders['.docx'] = Docx2txtLoader
    
    def load_document(self, file_path: str) -> List[Document]:
        """Load a document based on its file extension"""
        file_extension = os.path.splitext(file_path)[1].lower()
        
        if file_extension not in self.loaders:
            raise ValueError(f"Unsupported file type: {file_extension}")
        
        loader_class = self.loaders[file_extension]
        loader = loader_class(file_path)
        
        try:
            documents = loader.load()
            return documents
        except Exception as e:
            raise Exception(f"Error loading document: {str(e)}")
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """Split documents into chunks"""
        return self.text_splitter.split_documents(documents)
    
    def process_file(self, file_path: str, metadata: dict = None) -> List[Document]:
        """Process a file: load and split into chunks"""
        # Load the document
        documents = self.load_document(file_path)
        
        # Add custom metadata if provided
        if metadata:
            for doc in documents:
                doc.metadata.update(metadata)
        
        # Split into chunks
        chunks = self.split_documents(documents)
        
        # Add chunk index to metadata
        for i, chunk in enumerate(chunks):
            chunk.metadata['chunk_index'] = i
            chunk.metadata['source_file'] = os.path.basename(file_path)
        
        return chunks
    
    def process_text(self, text: str, metadata: dict = None) -> List[Document]:
        """Process raw text into chunks"""
        # Create a document from text
        doc = Document(page_content=text, metadata=metadata or {})
        
        # Split into chunks
        chunks = self.text_splitter.split_documents([doc])
        
        # Add chunk index to metadata
        for i, chunk in enumerate(chunks):
            chunk.metadata['chunk_index'] = i
        
        return chunks
    
    def validate_file(self, file_path: str) -> bool:
        """Validate if a file can be processed"""
        # Check if file exists
        if not os.path.exists(file_path):
            return False
        
        # Check file size
        file_size = os.path.getsize(file_path)
        if file_size > Config.MAX_FILE_SIZE:
            return False
        
        # Check file extension
        file_extension = os.path.splitext(file_path)[1].lower()
        if file_extension not in self.loaders:
            return False
        
        return True