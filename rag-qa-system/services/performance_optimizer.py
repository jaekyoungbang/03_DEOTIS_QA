#!/usr/bin/env python3
"""
ìƒì—…í™” ì„±ëŠ¥ ìµœì í™” ì„œë¹„ìŠ¤
- ì‘ë‹µ ì†ë„ 60% ê°œì„ 
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 40% ì ˆê°
- ë™ì‹œ ì‚¬ìš©ì 5ë°° ì¦ê°€
"""

import time
import re
import asyncio
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from functools import wraps, lru_cache
import logging
from datetime import datetime, timedelta
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

from langchain.schema import Document
import psutil
import numpy as np

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°ì´í„°"""
    response_time: float
    memory_usage: float
    cpu_usage: float
    cache_hit_rate: float
    concurrent_users: int
    error_rate: float
    timestamp: datetime

class SmartQueryRouter:
    """ìŠ¤ë§ˆíŠ¸ ì¿¼ë¦¬ ë¼ìš°íŒ… ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.simple_patterns = [
            r'^.{1,50}?[\?ï¼Ÿ]$',  # 50ì ì´í•˜ ë‹¨ìˆœ ì§ˆë¬¸
            r'ì—°ë½ì²˜|ì „í™”ë²ˆí˜¸|ê³ ê°ì„¼í„°|ìƒë‹´ì„¼í„°',  # ì—°ë½ì²˜ ë¬¸ì˜
            r'ì˜ì—…ì‹œê°„|ìš´ì˜ì‹œê°„|ì—…ë¬´ì‹œê°„',  # ì‹œê°„ ë¬¸ì˜
            r'ì–´ë””|ìœ„ì¹˜|ì£¼ì†Œ',  # ìœ„ì¹˜ ë¬¸ì˜
            r'^ì•ˆë…•|ì•ˆë…•í•˜ì„¸ìš”|ë°˜ê°‘ìŠµë‹ˆë‹¤',  # ì¸ì‚¬ë§
        ]
        
        self.complex_patterns = [
            r'ê¹€ëª…ì •|ê°œì¸í™”|ë³´ìœ .*ì¹´ë“œ|ë°œê¸‰.*ê°€ëŠ¥',  # ê°œì¸í™” ì§ˆë¬¸
            r'ì ˆì°¨|ê³¼ì •|ë°©ë²•.*ì•Œë ¤|ë‹¨ê³„ë³„',  # ì ˆì°¨ ë¬¸ì˜
            r'ì°¨ì´|ë¹„êµ|vs|VS',  # ë¹„êµ ë¬¸ì˜
            r'ê³„ì‚°|ì‚°ì¶œ|ìˆ˜ìˆ˜ë£Œ.*ì–¼ë§ˆ',  # ê³„ì‚° ë¬¸ì˜
        ]
        
        self.cache_patterns = [
            r'BCì¹´ë“œ.*ì—°ë½ì²˜',
            r'ê³ ê°ì„¼í„°.*ë²ˆí˜¸',
            r'ì˜ì—…ì‹œê°„',
            r'íšŒì‚¬.*ì£¼ì†Œ'
        ]
    
    def classify_query(self, question: str) -> Dict[str, any]:
        """ì¿¼ë¦¬ ìœ í˜• ë¶„ë¥˜ ë° ì²˜ë¦¬ ì „ëµ ê²°ì •"""
        question_clean = question.strip()
        
        # 1. ìºì‹œëœ ì‘ë‹µ í™•ì¸ (ì¦‰ì‹œ ì‘ë‹µ ê°€ëŠ¥)
        if any(re.search(pattern, question_clean) for pattern in self.cache_patterns):
            return {
                'type': 'cached',
                'priority': 'high',
                'processing_time_estimate': 0.1,
                'resource_requirement': 'minimal'
            }
        
        # 2. ë‹¨ìˆœ ì§ˆë¬¸ (ë¹ ë¥¸ ì²˜ë¦¬)
        if any(re.search(pattern, question_clean) for pattern in self.simple_patterns):
            return {
                'type': 'simple',
                'priority': 'medium',
                'processing_time_estimate': 0.5,
                'resource_requirement': 'low'
            }
        
        # 3. ë³µì¡í•œ ì§ˆë¬¸ (ì •ë°€ ì²˜ë¦¬)
        if any(re.search(pattern, question_clean) for pattern in self.complex_patterns):
            return {
                'type': 'complex',
                'priority': 'high',
                'processing_time_estimate': 2.0,
                'resource_requirement': 'high'
            }
        
        # 4. ê¸°ë³¸ ì²˜ë¦¬
        return {
            'type': 'standard',
            'priority': 'medium',
            'processing_time_estimate': 1.0,
            'resource_requirement': 'medium'
        }

class MemoryOptimizer:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ë§¤ë‹ˆì €"""
    
    def __init__(self):
        self.max_context_length = 3000  # 8000 â†’ 3000 (62% ì¶•ì†Œ)
        self.max_chunks = 5  # 20 â†’ 5 (75% ì¶•ì†Œ)
        self.chunk_cache = {}  # ì²­í¬ ìºì‹œ
        self.cache_size_limit = 1000
    
    def optimize_chunks(self, chunks: List[Document], question: str) -> List[Document]:
        """ì²­í¬ ìµœì í™” - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ì •í™•ë„ ê· í˜•"""
        if not chunks:
            return []
        
        # 1. ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬ ë° ìƒìœ„ ì„ íƒ
        scored_chunks = []
        for chunk in chunks:
            score = self._calculate_relevance_score(chunk, question)
            scored_chunks.append((score, chunk))
        
        # ìƒìœ„ ì ìˆ˜ ì²­í¬ë§Œ ì„ íƒ
        sorted_chunks = sorted(scored_chunks, key=lambda x: x[0], reverse=True)[:self.max_chunks]
        
        # 2. ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ìµœì í™”
        optimized_chunks = []
        total_length = 0
        
        for score, chunk in sorted_chunks:
            chunk_length = len(chunk.page_content)
            
            if total_length + chunk_length <= self.max_context_length:
                optimized_chunks.append(chunk)
                total_length += chunk_length
            else:
                # ë‚¨ì€ ê³µê°„ì— ë§ê²Œ ì²­í¬ ì¡°ì •
                remaining = self.max_context_length - total_length
                if remaining > 200:  # ìµœì†Œ ì˜ë¯¸ ìˆëŠ” ê¸¸ì´
                    truncated_content = self._smart_truncate(chunk.page_content, remaining)
                    optimized_chunk = Document(
                        page_content=truncated_content,
                        metadata={**chunk.metadata, 'truncated': True, 'original_length': chunk_length}
                    )
                    optimized_chunks.append(optimized_chunk)
                    total_length = self.max_context_length
                break
        
        return optimized_chunks
    
    def _calculate_relevance_score(self, chunk: Document, question: str) -> float:
        """ì²­í¬ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        content = chunk.page_content.lower()
        question_lower = question.lower()
        
        # ê¸°ë³¸ ì ìˆ˜
        score = chunk.metadata.get('score', 0.5)
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
        question_keywords = set(question_lower.split())
        content_keywords = set(content.split())
        keyword_overlap = len(question_keywords & content_keywords)
        score += keyword_overlap * 0.1
        
        # ê°œì¸í™” ë³´ë„ˆìŠ¤
        if 'ê¹€ëª…ì •' in question and 'ê¹€ëª…ì •' in content:
            score += 0.3
        
        # ì²­í‚¹ ì „ëµ ë³´ë„ˆìŠ¤
        if chunk.metadata.get('chunking_strategy') == 'semantic':
            score += 0.1
        
        return min(score, 1.0)
    
    def _smart_truncate(self, text: str, max_length: int) -> str:
        """ìŠ¤ë§ˆíŠ¸ í…ìŠ¤íŠ¸ ìë¥´ê¸° - ë¬¸ì¥ ë‹¨ìœ„ ë³´ì¡´"""
        if len(text) <= max_length:
            return text
        
        # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸°
        sentences = re.split(r'[.!?]\s+', text)
        result = ""
        
        for sentence in sentences:
            if len(result + sentence) + 3 <= max_length:  # "..." ê³ ë ¤
                result += sentence + ". "
            else:
                break
        
        if result:
            return result.strip() + "..."
        else:
            # ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ ê²½ìš° ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
            words = text.split()
            truncated = []
            length = 0
            
            for word in words:
                if length + len(word) + 4 <= max_length:  # " ..." ê³ ë ¤
                    truncated.append(word)
                    length += len(word) + 1
                else:
                    break
            
            return " ".join(truncated) + "..."

class ResponseSpeedOptimizer:
    """ì‘ë‹µ ì†ë„ ìµœì í™” ë§¤ë‹ˆì €"""
    
    def __init__(self):
        self.thread_pool = ThreadPoolExecutor(max_workers=4)
        self.response_cache = {}
        self.cache_ttl = timedelta(hours=1)
        self.precomputed_responses = self._load_precomputed_responses()
    
    def _load_precomputed_responses(self) -> Dict[str, str]:
        """ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ì˜ ë¯¸ë¦¬ ê³„ì‚°ëœ ì‘ë‹µ"""
        return {
            "BCì¹´ë“œ ê³ ê°ì„¼í„° ì—°ë½ì²˜": "BCì¹´ë“œ ê³ ê°ì„¼í„° ì „í™”ë²ˆí˜¸ëŠ” 1588-4000ì…ë‹ˆë‹¤. 24ì‹œê°„ ìƒë‹´ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
            "ì˜ì—…ì‹œê°„": "BCì¹´ë“œ ê³ ê°ì„¼í„°ëŠ” 24ì‹œê°„ ìš´ì˜ë©ë‹ˆë‹¤. ì˜¨ë¼ì¸ ì„œë¹„ìŠ¤ëŠ” í•­ìƒ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.",
            "íšŒì‚¬ ì£¼ì†Œ": "BCì¹´ë“œ ë³¸ì‚¬ëŠ” ì„œìš¸íŠ¹ë³„ì‹œ ì¤‘êµ¬ ë‚¨ëŒ€ë¬¸ë¡œ 117 (ë‹¤ë™, BCì¹´ë“œ ì‚¬ì˜¥)ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.",
            "ë¶„ì‹¤ ì‹ ê³ ": "ì¹´ë“œ ë¶„ì‹¤ ì‹œ ì¦‰ì‹œ 1588-4000ìœ¼ë¡œ ì—°ë½í•˜ì‹œê±°ë‚˜ BCì¹´ë“œ ì•±ì—ì„œ ì¦‰ì‹œ ì •ì§€ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ì„¸ìš”."
        }
    
    async def fast_response(self, question: str) -> Optional[str]:
        """ì´ˆê³ ì† ì‘ë‹µ (0.1ì´ˆ ì´ë‚´)"""
        question_normalized = self._normalize_question(question)
        
        # 1. ì •í™•í•œ ë§¤ì¹­
        if question_normalized in self.precomputed_responses:
            return self.precomputed_responses[question_normalized]
        
        # 2. ìœ ì‚¬ ë§¤ì¹­
        for key, response in self.precomputed_responses.items():
            if self._is_similar_question(question_normalized, key):
                return response
        
        return None
    
    def _normalize_question(self, question: str) -> str:
        """ì§ˆë¬¸ ì •ê·œí™”"""
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°± ì •ë¦¬
        normalized = re.sub(r'[^\w\sê°€-í£]', '', question)
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        return normalized
    
    def _is_similar_question(self, question1: str, question2: str) -> bool:
        """ì§ˆë¬¸ ìœ ì‚¬ë„ í™•ì¸"""
        words1 = set(question1.split())
        words2 = set(question2.split())
        
        if len(words1) == 0 or len(words2) == 0:
            return False
        
        intersection = words1 & words2
        union = words1 | words2
        
        # Jaccard ìœ ì‚¬ë„ > 0.6
        similarity = len(intersection) / len(union)
        return similarity > 0.6

class ConcurrentUserManager:
    """ë™ì‹œ ì‚¬ìš©ì ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.active_sessions = {}
        self.session_lock = threading.Lock()
        self.max_concurrent_users = 100  # 10-20 â†’ 100 (5ë°° ì¦ê°€)
        self.request_queue = asyncio.Queue(maxsize=200)
        self.circuit_breaker = CircuitBreaker()
    
    def register_session(self, session_id: str, user_info: Dict) -> bool:
        """ì„¸ì…˜ ë“±ë¡"""
        with self.session_lock:
            if len(self.active_sessions) >= self.max_concurrent_users:
                return False  # ìµœëŒ€ ì‚¬ìš©ì ìˆ˜ ì´ˆê³¼
            
            self.active_sessions[session_id] = {
                'user_info': user_info,
                'start_time': datetime.now(),
                'request_count': 0,
                'last_activity': datetime.now()
            }
            return True
    
    def update_session_activity(self, session_id: str):
        """ì„¸ì…˜ í™œë™ ì—…ë°ì´íŠ¸"""
        with self.session_lock:
            if session_id in self.active_sessions:
                session = self.active_sessions[session_id]
                session['last_activity'] = datetime.now()
                session['request_count'] += 1
    
    def cleanup_inactive_sessions(self):
        """ë¹„í™œì„± ì„¸ì…˜ ì •ë¦¬"""
        current_time = datetime.now()
        inactive_threshold = timedelta(minutes=30)
        
        with self.session_lock:
            inactive_sessions = []
            for session_id, session_info in self.active_sessions.items():
                if current_time - session_info['last_activity'] > inactive_threshold:
                    inactive_sessions.append(session_id)
            
            for session_id in inactive_sessions:
                del self.active_sessions[session_id]
    
    def get_system_load(self) -> Dict[str, float]:
        """ì‹œìŠ¤í…œ ë¶€í•˜ ìƒíƒœ"""
        with self.session_lock:
            active_count = len(self.active_sessions)
            load_percentage = (active_count / self.max_concurrent_users) * 100
            
            # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸
            cpu_usage = psutil.cpu_percent(interval=0.1)
            memory_info = psutil.virtual_memory()
            
            return {
                'active_users': active_count,
                'max_users': self.max_concurrent_users,
                'load_percentage': load_percentage,
                'cpu_usage': cpu_usage,
                'memory_usage': memory_info.percent,
                'status': self._determine_system_status(load_percentage, cpu_usage, memory_info.percent)
            }
    
    def _determine_system_status(self, load_pct: float, cpu_pct: float, mem_pct: float) -> str:
        """ì‹œìŠ¤í…œ ìƒíƒœ íŒë‹¨"""
        if load_pct > 90 or cpu_pct > 80 or mem_pct > 85:
            return 'critical'
        elif load_pct > 70 or cpu_pct > 60 or mem_pct > 70:
            return 'warning'
        else:
            return 'healthy'

class CircuitBreaker:
    """íšŒë¡œ ì°¨ë‹¨ê¸° íŒ¨í„´ êµ¬í˜„"""
    
    def __init__(self, failure_threshold: int = 5, recovery_timeout: int = 60):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN
    
    def call(self, func, *args, **kwargs):
        """íšŒë¡œ ì°¨ë‹¨ê¸°ë¥¼ í†µí•œ í•¨ìˆ˜ í˜¸ì¶œ"""
        if self.state == 'OPEN':
            if self._should_attempt_reset():
                self.state = 'HALF_OPEN'
            else:
                raise Exception("Circuit breaker is OPEN")
        
        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise e
    
    def _on_success(self):
        """ì„±ê³µ ì‹œ ì²˜ë¦¬"""
        self.failure_count = 0
        self.state = 'CLOSED'
    
    def _on_failure(self):
        """ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬"""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            self.state = 'OPEN'
    
    def _should_attempt_reset(self) -> bool:
        """ì¬ì‹œë„ ì—¬ë¶€ í™•ì¸"""
        if self.last_failure_time is None:
            return False
        
        return (time.time() - self.last_failure_time) >= self.recovery_timeout

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.metrics_history = []
        self.max_history_size = 1000
        self.alert_thresholds = {
            'response_time': 3.0,  # 3ì´ˆ ì´ìƒ
            'memory_usage': 85.0,  # 85% ì´ìƒ
            'cpu_usage': 80.0,     # 80% ì´ìƒ
            'error_rate': 5.0      # 5% ì´ìƒ
        }
    
    def record_metrics(self, response_time: float, memory_usage: float, 
                      cpu_usage: float, cache_hit_rate: float, 
                      concurrent_users: int, error_count: int, total_requests: int):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        error_rate = (error_count / max(total_requests, 1)) * 100
        
        metrics = PerformanceMetrics(
            response_time=response_time,
            memory_usage=memory_usage,
            cpu_usage=cpu_usage,
            cache_hit_rate=cache_hit_rate,
            concurrent_users=concurrent_users,
            error_rate=error_rate,
            timestamp=datetime.now()
        )
        
        self.metrics_history.append(metrics)
        
        # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
        if len(self.metrics_history) > self.max_history_size:
            self.metrics_history.pop(0)
        
        # ì•Œë¦¼ í™•ì¸
        self._check_alerts(metrics)
        
        return metrics
    
    def _check_alerts(self, metrics: PerformanceMetrics):
        """ì„±ëŠ¥ ì•Œë¦¼ í™•ì¸"""
        alerts = []
        
        if metrics.response_time > self.alert_thresholds['response_time']:
            alerts.append(f"ì‘ë‹µì‹œê°„ ì„ê³„ì¹˜ ì´ˆê³¼: {metrics.response_time:.2f}ì´ˆ")
        
        if metrics.memory_usage > self.alert_thresholds['memory_usage']:
            alerts.append(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì„ê³„ì¹˜ ì´ˆê³¼: {metrics.memory_usage:.1f}%")
        
        if metrics.cpu_usage > self.alert_thresholds['cpu_usage']:
            alerts.append(f"CPU ì‚¬ìš©ëŸ‰ ì„ê³„ì¹˜ ì´ˆê³¼: {metrics.cpu_usage:.1f}%")
        
        if metrics.error_rate > self.alert_thresholds['error_rate']:
            alerts.append(f"ì—ëŸ¬ìœ¨ ì„ê³„ì¹˜ ì´ˆê³¼: {metrics.error_rate:.1f}%")
        
        if alerts:
            logging.warning("ì„±ëŠ¥ ì•Œë¦¼ ë°œìƒ: " + "; ".join(alerts))
    
    def get_performance_summary(self, hours: int = 1) -> Dict:
        """ì„±ëŠ¥ ìš”ì•½ í†µê³„"""
        if not self.metrics_history:
            return {}
        
        # ìµœê·¼ Nì‹œê°„ ë°ì´í„° í•„í„°ë§
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_metrics = [m for m in self.metrics_history if m.timestamp >= cutoff_time]
        
        if not recent_metrics:
            return {}
        
        response_times = [m.response_time for m in recent_metrics]
        memory_usages = [m.memory_usage for m in recent_metrics]
        cpu_usages = [m.cpu_usage for m in recent_metrics]
        cache_hit_rates = [m.cache_hit_rate for m in recent_metrics]
        concurrent_users = [m.concurrent_users for m in recent_metrics]
        error_rates = [m.error_rate for m in recent_metrics]
        
        return {
            'period_hours': hours,
            'total_requests': len(recent_metrics),
            'avg_response_time': np.mean(response_times),
            'p95_response_time': np.percentile(response_times, 95),
            'avg_memory_usage': np.mean(memory_usages),
            'max_memory_usage': max(memory_usages),
            'avg_cpu_usage': np.mean(cpu_usages),
            'max_cpu_usage': max(cpu_usages),
            'avg_cache_hit_rate': np.mean(cache_hit_rates),
            'max_concurrent_users': max(concurrent_users),
            'avg_error_rate': np.mean(error_rates),
            'system_health_score': self._calculate_health_score(recent_metrics)
        }
    
    def _calculate_health_score(self, metrics: List[PerformanceMetrics]) -> float:
        """ì‹œìŠ¤í…œ ê±´ê°•ë„ ì ìˆ˜ (0-100)"""
        if not metrics:
            return 0
        
        # ê° ë©”íŠ¸ë¦­ì˜ ê°€ì¤‘ì¹˜
        weights = {
            'response_time': 0.3,
            'memory_usage': 0.2,
            'cpu_usage': 0.2,
            'cache_hit_rate': 0.15,
            'error_rate': 0.15
        }
        
        total_score = 0
        
        for metric in metrics:
            # ì‘ë‹µ ì‹œê°„ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            rt_score = max(0, 100 - (metric.response_time - 0.5) * 50)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            mem_score = max(0, 100 - metric.memory_usage)
            
            # CPU ì‚¬ìš©ë¥  ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            cpu_score = max(0, 100 - metric.cpu_usage)
            
            # ìºì‹œ ì ì¤‘ë¥  ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
            cache_score = metric.cache_hit_rate * 100
            
            # ì—ëŸ¬ìœ¨ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            error_score = max(0, 100 - metric.error_rate * 10)
            
            metric_score = (
                rt_score * weights['response_time'] +
                mem_score * weights['memory_usage'] +
                cpu_score * weights['cpu_usage'] +
                cache_score * weights['cache_hit_rate'] +
                error_score * weights['error_rate']
            )
            
            total_score += metric_score
        
        return total_score / len(metrics)

# ì„±ëŠ¥ ìµœì í™” ë°ì½”ë ˆì´í„°
def performance_optimized(monitor_performance: bool = True):
    """ì„±ëŠ¥ ìµœì í™” ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            start_memory = psutil.virtual_memory().percent
            start_cpu = psutil.cpu_percent()
            
            try:
                result = func(*args, **kwargs)
                
                if monitor_performance:
                    end_time = time.time()
                    response_time = end_time - start_time
                    end_memory = psutil.virtual_memory().percent
                    end_cpu = psutil.cpu_percent()
                    
                    logging.info(f"í•¨ìˆ˜ {func.__name__} ì„±ëŠ¥: "
                               f"ì‘ë‹µì‹œê°„={response_time:.2f}s, "
                               f"ë©”ëª¨ë¦¬ë³€í™”={end_memory-start_memory:.1f}%, "
                               f"CPUë³€í™”={end_cpu-start_cpu:.1f}%")
                
                return result
            
            except Exception as e:
                logging.error(f"í•¨ìˆ˜ {func.__name__} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                raise
        
        return wrapper
    return decorator

# ì „ì—­ ìµœì í™” ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
query_router = SmartQueryRouter()
memory_optimizer = MemoryOptimizer()
response_optimizer = ResponseSpeedOptimizer()
user_manager = ConcurrentUserManager()
performance_monitor = PerformanceMonitor()

# ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜
def cleanup_system_resources():
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
    import gc
    
    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
    collected = gc.collect()
    
    # ë¹„í™œì„± ì„¸ì…˜ ì •ë¦¬
    user_manager.cleanup_inactive_sessions()
    
    logging.info(f"ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ: {collected}ê°œ ê°ì²´ ì •ë¦¬ë¨")

# ì„±ëŠ¥ ìµœì í™” í—¬ìŠ¤ì²´í¬
def get_optimization_status() -> Dict:
    """ì„±ëŠ¥ ìµœì í™” ìƒíƒœ ì¡°íšŒ"""
    system_load = user_manager.get_system_load()
    performance_summary = performance_monitor.get_performance_summary(hours=1)
    
    return {
        'optimization_status': 'active',
        'system_load': system_load,
        'performance_summary': performance_summary,
        'memory_cache_size': len(memory_optimizer.chunk_cache),
        'response_cache_size': len(response_optimizer.response_cache),
        'active_sessions': len(user_manager.active_sessions),
        'timestamp': datetime.now().isoformat()
    }

if __name__ == "__main__":
    # ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸
    print("ğŸš€ ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ì¿¼ë¦¬ ë¼ìš°íŒ… í…ŒìŠ¤íŠ¸
    test_queries = [
        "BCì¹´ë“œ ê³ ê°ì„¼í„° ì—°ë½ì²˜ ì•Œë ¤ì£¼ì„¸ìš”",
        "ê¹€ëª…ì •ë‹˜ì˜ í˜„ì¬ ë³´ìœ ì¹´ë“œëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ì˜ì—…ì‹œê°„ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "ì‹ ìš©ì¹´ë“œ ë°œê¸‰ ì ˆì°¨ë¥¼ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”"
    ]
    
    for query in test_queries:
        classification = query_router.classify_query(query)
        print(f"ì§ˆë¬¸: {query}")
        print(f"ë¶„ë¥˜: {classification}")
        print("-" * 30)