from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from models.llm import LLMManager
from models.embeddings import EmbeddingManager
from models.vectorstore import VectorStoreManager
from models.dual_vectorstore import DualVectorStoreManager, get_dual_vectorstore
from utils.error_handler import detect_error_type, format_error_response
from services.cache_factory import CacheFactory
# from services.query_analyzer import QueryAnalyzer
# from services.reranker import SearchReranker
import time
import sqlite3
import os
from datetime import datetime
from typing import List

class RAGChain:
    def __init__(self):
        self.llm_manager = LLMManager()
        self.embedding_manager = EmbeddingManager()
        self.vectorstore_manager = None
        self.dual_vectorstore_manager = None
        # Expose vectorstore for external access
        self.vectorstore = None
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
    
    def _initialize_vectorstore(self):
        """VectorStore ì§€ì—° ì´ˆê¸°í™”"""
        if self.vectorstore_manager is None:
            self.vectorstore_manager = VectorStoreManager(
                self.embedding_manager.get_embeddings()
            )
            # ì´ì¤‘ ë²¡í„°ìŠ¤í† ì–´ ë§¤ë‹ˆì € ì¶”ê°€
            self.dual_vectorstore_manager = get_dual_vectorstore()
            # Expose vectorstore for external access
            self.vectorstore = self.vectorstore_manager
            
            # ì´ˆê¸°í™” í›„ ì¶”ê°€ êµ¬ì„± ìš”ì†Œë“¤ ì„¤ì •
            self.qa_chain = None
            self.cache_manager = CacheFactory.get_cache_manager()
            # self.query_analyzer = QueryAnalyzer()  # ì§ˆë¬¸ ë¶„ì„ê¸°
            # self.reranker = SearchReranker()  # ì¬ìˆœìœ„ ì‹œìŠ¤í…œ
            self.stats_db_path = "./data/search_stats.db"
            self.initialize_stats_db()
            self.initialize_chain()
    
    def initialize_stats_db(self):
        """ê²€ìƒ‰ í†µê³„ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        os.makedirs(os.path.dirname(self.stats_db_path), exist_ok=True)
        
        with sqlite3.connect(self.stats_db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS search_stats (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    question TEXT NOT NULL,
                    query_time REAL NOT NULL,
                    cache_hit BOOLEAN NOT NULL,
                    cache_time REAL,
                    total_time REAL NOT NULL,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS global_stats (
                    id INTEGER PRIMARY KEY,
                    total_searches INTEGER DEFAULT 0,
                    total_cache_hits INTEGER DEFAULT 0,
                    avg_query_time REAL DEFAULT 0.0,
                    last_updated DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # Initialize global stats if empty
            cursor.execute('SELECT COUNT(*) FROM global_stats')
            if cursor.fetchone()[0] == 0:
                cursor.execute('''
                    INSERT INTO global_stats (id, total_searches, total_cache_hits, avg_query_time)
                    VALUES (1, 0, 0, 0.0)
                ''')
            
            conn.commit()
    
    def update_search_stats(self, question, query_time, cache_hit, cache_time=None, total_time=None):
        """ê²€ìƒ‰ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            with sqlite3.connect(self.stats_db_path) as conn:
                cursor = conn.cursor()
                
                # Insert individual search record
                cursor.execute('''
                    INSERT INTO search_stats (question, query_time, cache_hit, cache_time, total_time)
                    VALUES (?, ?, ?, ?, ?)
                ''', (question, query_time, cache_hit, cache_time, total_time or query_time))
                
                # Update global stats
                cursor.execute('''
                    UPDATE global_stats SET 
                        total_searches = total_searches + 1,
                        total_cache_hits = total_cache_hits + ?,
                        avg_query_time = (
                            SELECT AVG(query_time) FROM search_stats 
                            WHERE cache_hit = 0
                        ),
                        last_updated = CURRENT_TIMESTAMP
                    WHERE id = 1
                ''', (1 if cache_hit else 0,))
                
                conn.commit()
        except Exception as e:
            print(f"âš ï¸ ê²€ìƒ‰ í†µê³„ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
    
    def get_search_stats(self):
        """ê²€ìƒ‰ í†µê³„ ì¡°íšŒ"""
        try:
            with sqlite3.connect(self.stats_db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                    SELECT total_searches, total_cache_hits, avg_query_time, last_updated
                    FROM global_stats WHERE id = 1
                ''')
                result = cursor.fetchone()
                
                if result:
                    total_searches, total_cache_hits, avg_query_time, last_updated = result
                    cache_hit_rate = (total_cache_hits / total_searches * 100) if total_searches > 0 else 0
                    
                    return {
                        "total_searches": total_searches,
                        "total_cache_hits": total_cache_hits,
                        "cache_hit_rate": round(cache_hit_rate, 1),
                        "avg_query_time": round(avg_query_time or 0, 3),
                        "last_updated": last_updated
                    }
                
        except Exception as e:
            print(f"âš ï¸ ê²€ìƒ‰ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            
        return {
            "total_searches": 0,
            "total_cache_hits": 0,
            "cache_hit_rate": 0.0,
            "avg_query_time": 0.0,
            "last_updated": None
        }
    
    def initialize_chain(self):
        """Initialize the RAG chain"""
        # Custom prompt template
        prompt_template = """ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
        ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

        ## í•µì‹¬ ì›ì¹™
        1. **ì •í™•ì„± ìš°ì„ **: ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€
        2. **í˜•ì‹ ì¼ì¹˜**: ì›ë³¸ ë¬¸ì„œì˜ í˜•ì‹(íŠ¹íˆ í‘œ)ì„ ìµœëŒ€í•œ ìœ ì‚¬í•˜ê²Œ ìœ ì§€
        3. **ê¹”ë”í•œ êµ¬ì„±**: ì²´ê³„ì ì´ê³  ê°€ë…ì„± ìˆëŠ” ë‹µë³€ êµ¬ì¡°

        ## ë‹µë³€ í˜•ì‹ ê·œì¹™

        ### í‘œ í˜•ì‹ ë°ì´í„° ì²˜ë¦¬
        - ë¬¸ì„œì˜ í‘œ í˜•ì‹ ë°ì´í„°ëŠ” **ë°˜ë“œì‹œ** markdown í‘œë¡œ ì •í™•íˆ ì¬í˜„
        - ì›ë³¸ í‘œì˜ ì—´ êµ¬ì„±, ì •ë ¬, êµ¬ì¡°ë¥¼ ìµœëŒ€í•œ ìœ ì‚¬í•˜ê²Œ ìœ ì§€
        - í‘œ ì œëª©ì´ë‚˜ ì„¤ëª…ì´ ìˆë‹¤ë©´ í‘œ ìœ„ì— ëª…ì‹œ

        ## ë‹µë³€ êµ¬ì¡°
        1. **í•µì‹¬ ë‹µë³€**: ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µ (í‘œ í˜•ì‹ì´ë¼ë©´ í‘œë¡œ)
        2. **ì¶œì²˜ ëª…í™•ì„±**: ì–´ë–¤ ë¬¸ì„œì˜ ì–´ë–¤ ë¶€ë¶„ì—ì„œ ì •ë³´ë¥¼ ì–»ì—ˆëŠ”ì§€ ìì„¸íˆ ì–¸ê¸‰

        ## ì‘ì„± ìŠ¤íƒ€ì¼
        - ê°„ê²°í•˜ê³  ëª…í™•í•œ í•œêµ­ì–´ ì‚¬ìš©
        - ë¶ˆí•„ìš”í•œ ë‹µë³€ì€ ìƒëµ
        - í•µì‹¬ ì •ë³´ë¥¼ ì•ìª½ì— ë°°ì¹˜
        - ëª©ë¡ì´ë‚˜ ë‹¨ê³„ë³„ ì„¤ëª… ì‹œ ì ì ˆí•œ ë²ˆí˜¸ë‚˜ ë¶ˆë¦¿ í¬ì¸íŠ¸ í™œìš©

        ## ì¤‘ì²©ëœ ë‚´ìš©
        - ë‚´ìš©ì¤‘ì— ì¤‘ì²©ëœ ë‚´ìš©ì€ 1ê°œë¡œ í†µí•© ì¶œë ¥ 

         ## ì™¸ë¶€ ê²€ìƒ‰ ë¶ˆê°€ 
        - í˜„ì¬ ë²¡í„°DBì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ë§Œì„ ê°€ì§€ê³  í™”ë©´ ì„¤ê³„ í•„ìš” 
        - ì™¸ë¶€ ê²€ìƒ‰ì„ í†µí•œ ë°ì´í„° ì¡°íšŒ ë¶ˆê°€ 

        
        ì»¨í…ìŠ¤íŠ¸: {context}
        
        ì§ˆë¬¸: {question}
        
        ë‹µë³€:"""
        
        self.prompt_template = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # Create retrieval QA chain with improved retrieval
        # Use dual_vectorstore_manager with basic collection as default
        try:
            basic_retriever = self.dual_vectorstore_manager.get_retriever("basic", k=20)
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm_manager.get_llm(),
                chain_type="stuff",
                retriever=basic_retriever,
                return_source_documents=True,
                chain_type_kwargs={"prompt": self.prompt_template}
            )
        except Exception as e:
            print(f"âš ï¸ Dual vectorstore not available, falling back to single vectorstore: {e}")
            # Fallback to single vectorstore
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm_manager.get_llm(),
                chain_type="stuff",
                retriever=self.vectorstore_manager.get_retriever(k=20),
                return_source_documents=True,
                chain_type_kwargs={"prompt": self.prompt_template}
            )
    
    def search_with_strategy(self, question, search_mode="basic", k=5):  # Reduced k for optimization
        """ì²­í‚¹ ì „ëµë³„ ë¬¸ì„œ ê²€ìƒ‰"""
        # ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” í™•ì¸
        self._initialize_vectorstore()
        
        try:
            if search_mode == "dual":
                # ì´ì¤‘ ê²€ìƒ‰: ê¸°ë³¸ + ì»¤ìŠ¤í…€
                results = self.dual_vectorstore_manager.dual_search(question, k)
                documents = [doc for doc, score in results]
                return documents
            elif search_mode == "custom":
                # ì»¤ìŠ¤í…€ ì²­í‚¹ë§Œ ê²€ìƒ‰
                return self.dual_vectorstore_manager.similarity_search(question, "custom", k)
            else:
                # ê¸°ë³¸ ì²­í‚¹ë§Œ ê²€ìƒ‰ (ê¸°ì¡´ ë°©ì‹)
                return self.vectorstore_manager.similarity_search(question, k)
        except Exception as e:
            print(f"âš ï¸ ì „ëµë³„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            # í´ë°±: ê¸°ë³¸ ê²€ìƒ‰
            return self.vectorstore_manager.similarity_search(question, k)
    
    def get_conversational_chain(self):
        """Create a conversational retrieval chain with memory"""
        # ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” í™•ì¸
        self._initialize_vectorstore()
        
        return ConversationalRetrievalChain.from_llm(
            llm=self.llm_manager.get_llm(),
            retriever=self.vectorstore_manager.get_retriever(k=5),
            memory=self.memory,
            return_source_documents=True,
            verbose=True
        )
    
    def query(self, question, use_memory=False, llm_model=None, use_cache=True, search_mode="basic"):
        """Query the RAG system with caching support and performance tracking"""
        # ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” í™•ì¸
        self._initialize_vectorstore()
        
        start_time = time.time()
        cache_start_time = None
        cache_end_time = None
        query_start_time = None
        query_end_time = None
        
        try:
            # Check if vectorstore has documents before querying
            try:
                doc_count_info = self.dual_vectorstore_manager.get_document_count()
                doc_count = doc_count_info.get('total', 0)
            except Exception:
                # Fallback to single vectorstore
                doc_count = self.vectorstore_manager.get_document_count()
            
            if doc_count == 0:
                from utils.error_handler import ErrorCodes, format_error_response
                error_response = format_error_response(ErrorCodes.VDB_NO_COLLECTION)
                return {
                    "error": True,
                    "error_code": error_response["error_code"],
                    "title": error_response["title"],
                    "message": error_response["message"],
                    "solution": error_response["solution"],
                    "answer": f"âŒ {error_response['title']}: {error_response['message']}"
                }
            
            # Check cache first (only for non-memory queries)
            cache_hit = False
            if use_cache and not use_memory:
                cache_start_time = time.time()
                cached_response = self.cache_manager.get(question, llm_model)
                cache_end_time = time.time()
                if cached_response:
                    # Add cache indicator and timing info
                    cache_time = cache_end_time - cache_start_time
                    total_time = time.time() - start_time
                    cache_hit = True
                    
                    # Get global search statistics
                    global_stats = self.get_search_stats()
                    current_search_number = global_stats["total_searches"] + 1
                    
                    # Update statistics
                    self.update_search_stats(question, 0.0, cache_hit, cache_time, total_time)
                    
                    # Add enhanced performance info to cached response
                    # ì›ë³¸ ìºì‹œëœ ì‘ë‹µì˜ ì„±ëŠ¥ ì •ë³´ ë³´ì¡´
                    original_performance = cached_response.get('performance', {})
                    cached_response.update({
                        '_from_cache': True,
                        'performance': {
                            'cache_hit': True,
                            'cache_lookup_time_ms': round(cache_time * 1000, 2),
                            'rag_search_time_ms': original_performance.get('rag_search_time_ms', 0),  # ì›ë³¸ RAG ì‹œê°„
                            'llm_response_time_ms': original_performance.get('llm_response_time_ms', 0),  # ì›ë³¸ LLM ì‹œê°„
                            'total_time_ms': round(total_time * 1000, 2),  # í˜„ì¬ ìºì‹œ ì¡°íšŒ í¬í•¨ ì‹œê°„
                            'original_total_time_ms': original_performance.get('total_time_ms', 0),  # ì›ë³¸ ì´ ì²˜ë¦¬ ì‹œê°„
                            'timestamp': datetime.now().isoformat()
                        },
                        'system_info': {
                            'llm_model': llm_model or "gpt-4o-mini",
                            'embedding_model': "text-embedding-3-small",
                            'vector_db': "ChromaDB",
                            'cache_system': "Hybrid (Redis + SQLite)"
                        },
                        'search_metadata': {
                            'current_search_number': current_search_number,
                            'total_searches_today': global_stats["total_searches"],
                            'cache_hit_rate': round((global_stats["total_cache_hits"] + 1) / current_search_number * 100, 1),
                            'avg_query_time_ms': round(global_stats["avg_query_time"] * 1000, 2),
                            'documents_searched': 0,  # No search needed
                            'vector_db_size': doc_count
                        }
                    })
                    return cached_response
            
            # Start actual query timing
            query_start_time = time.time()
            
            # ì§ˆë¬¸ ë¶„ì„ (DEOTISì²˜ëŸ¼)
            try:
                query_analysis = self.query_analyzer.analyze_query(question)
                analyzed_keywords = query_analysis.get('keywords', [])
                print(f"[ì§ˆë¬¸ ë¶„ì„] í‚¤ì›Œë“œ: {analyzed_keywords}")
            except:
                analyzed_keywords = []
            
            # ì¹´ë“œ ê´€ë ¨ ì§ˆì˜ ê°ì§€
            card_keywords = ["ì¹´ë“œ", "ë°œê¸‰", "íšŒì›", "ì€í–‰", "ì¹´ë“œë°œê¸‰", "ê¹€ëª…ì •"]
            is_card_query = any(keyword in question for keyword in card_keywords)
            
            # Get similarity search results with scores first using strategy (increased k for better recall)
            if is_card_query and search_mode == "custom":
                # ì¹´ë“œ ê´€ë ¨ ì§ˆì˜ì— ëŒ€í•´ ê°•í™”ëœ ê²€ìƒ‰ ì‚¬ìš©
                similarity_results = self.dual_vectorstore_manager.enhanced_card_search(question, k=15)
                documents = [doc for doc, score in similarity_results]
                print(f"ğŸ” [ì¹´ë“œ ì „ìš© ê²€ìƒ‰] ê°•í™”ëœ ê²€ìƒ‰ìœ¼ë¡œ {len(documents)}ê°œ ë¬¸ì„œ ê²€ìƒ‰")
            elif search_mode == "dual":
                similarity_results = self.dual_vectorstore_manager.dual_search(question, k=20)
                documents = [doc for doc, score in similarity_results]
            elif search_mode == "custom":
                documents = self.dual_vectorstore_manager.similarity_search(question, "custom", k=20)
                similarity_results = [(doc, 0.0) for doc in documents]  # ì ìˆ˜ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’
            else:
                # basic ê²€ìƒ‰ ëª¨ë“œì—ì„œë„ dual_vectorstore_manager ì‚¬ìš©
                similarity_results = self.dual_vectorstore_manager.similarity_search_with_score(question, "basic", k=20)
                documents = [doc for doc, score in similarity_results]
                
            # í‚¤ì›Œë“œ ì •í™• ë§¤ì¹­ ë¶€ìŠ¤íŒ…: ì§ˆë¬¸ì— í¬í•¨ëœ í‚¤ì›Œë“œê°€ ë¬¸ì„œì— ì§ì ‘ í¬í•¨ëœ ê²½ìš° ì ìˆ˜ ìƒìŠ¹
            boosted_results = []
            for doc, score in similarity_results:
                boost_factor = 1.0
                
                # ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ë¶€ìŠ¤íŒ…
                if question in doc.page_content:
                    boost_factor = 1.5  # 50% ë¶€ìŠ¤íŠ¸
                elif any(word in doc.page_content for word in question.split() if len(word) > 2):
                    boost_factor = 1.2  # 20% ë¶€ìŠ¤íŠ¸
                    
                boosted_score = min(score * boost_factor, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
                boosted_results.append((doc, boosted_score))
            
            # ë¶€ìŠ¤íŠ¸ëœ ì ìˆ˜ë¡œ ì¬ì •ë ¬
            boosted_results.sort(key=lambda x: x[1], reverse=True)
            
            # DEOTISì²˜ëŸ¼ AI ì¬ìˆœìœ„ ì ìš©
            try:
                similarity_results = self.reranker.rerank_results(question, boosted_results, strategy="hybrid")
                print(f"[ì¬ìˆœìœ„] ì ìš© ì™„ë£Œ")
            except:
                similarity_results = boosted_results
                
            documents = [doc for doc, score in similarity_results]
            
            # Create optimized context from retrieved documents (chunking optimization)
            # Instead of concatenating all documents, use only the most relevant ones
            # and limit the total context length to reduce LLM input size
            max_context_length = 8000  # Further increased for better content inclusion
            context, used_documents = self._create_optimized_context_with_docs(documents, max_context_length)
            
            if use_memory:
                chain = self.get_conversational_chain()
                result = chain({"question": question})
            else:
                # ìˆ˜ë™ìœ¼ë¡œ LLM í˜¸ì¶œí•˜ì—¬ ê²€ìƒ‰ ëª¨ë“œ ë°˜ì˜
                if search_mode != "basic":
                    # ì»¤ìŠ¤í…€ ê²€ìƒ‰ ëª¨ë“œì˜ ê²½ìš° ì§ì ‘ LLM í˜¸ì¶œ
                    llm = self.llm_manager.get_llm(model_name=llm_model)
                    prompt = self.prompt_template.format(context=context, question=question)
                    answer = llm.invoke(prompt)
                    result = {
                        "result": answer,
                        "source_documents": documents
                    }
                else:
                    result = self.qa_chain({"query": question})
            
            query_end_time = time.time()
            query_time = query_end_time - query_start_time
            total_time = time.time() - start_time
            cache_time = (cache_end_time - cache_start_time) if cache_start_time else 0
            
            # Get global search statistics
            global_stats = self.get_search_stats()
            current_search_number = global_stats["total_searches"] + 1
            
            # ìµœê³  ìœ ì‚¬ë„ í™•ì¸ ë° ì„ê³„ê°’ ì²´í¬
            max_similarity = similarity_results[0][1] if similarity_results else 0
            similarity_threshold_met = max_similarity >= 0.8  # 80% ê¸°ì¤€
            
            # Format response with similarity scores and enhanced performance info
            response = {
                "answer": result.get("result") or result.get("answer"),
                "source_documents": [],
                "similarity_search": {
                    "query": question,
                    "total_results": len(similarity_results),
                    "top_matches": [],
                    "max_similarity": max_similarity,
                    "similarity_threshold_met": similarity_threshold_met
                },
                "suggested_questions": [],  # ë‚®ì€ ìœ ì‚¬ë„ì‹œ ì¶”ì²œ ì§ˆë¬¸
                "performance": {
                    "cache_hit": False,
                    "cache_lookup_time_ms": round(cache_time * 1000, 2) if cache_time > 0 else 0,
                    "rag_search_time_ms": round(query_time * 1000, 2),
                    "llm_response_time_ms": round(query_time * 1000, 2),  # Same as RAG for now
                    "total_time_ms": round(total_time * 1000, 2),
                    "timestamp": datetime.now().isoformat()
                },
                "system_info": {
                    "llm_model": llm_model or "gpt-4o-mini",
                    "embedding_model": "text-embedding-3-small",
                    "vector_db": "ChromaDB",
                    "cache_system": "Hybrid (Redis + SQLite)"
                },
                "search_metadata": {
                    "current_search_number": current_search_number,
                    "total_searches_today": global_stats["total_searches"],
                    "cache_hit_rate": global_stats["cache_hit_rate"],
                    "avg_query_time_ms": round(global_stats["avg_query_time"] * 1000, 2),
                    "documents_searched": len(similarity_results),
                    "vector_db_size": doc_count
                },
                "chunking_type": search_mode  # Add chunking_type to response
            }
            
            # Add source documents if available
            if "source_documents" in result:
                for doc in result["source_documents"]:
                    response["source_documents"].append({
                        "content": doc.page_content[:500],  # Limit content length
                        "metadata": doc.metadata
                    })
            
            # Add similarity search results with scores - ONLY show documents that were actually used by LLM
            # This ensures consistency between what the LLM sees and what the user sees
            used_similarity_results = []
            for doc in used_documents:
                # Find the corresponding score from original similarity_results
                for orig_doc, score in similarity_results:
                    if doc.page_content == orig_doc.page_content:
                        used_similarity_results.append((doc, score))
                        break
                else:
                    # If not found in similarity_results, assign a default score
                    used_similarity_results.append((doc, 0.5))
            
            for i, (doc, score) in enumerate(used_similarity_results[:3], 1):  # Top 3 results that were actually used
                # ì „ì²´ ë‚´ìš© í‘œì‹œ (2000ìë¡œ ë” ì¦ê°€)
                content_full = doc.page_content
                if len(content_full) > 2000:
                    content_preview = content_full[:2000] + "..."
                else:
                    content_preview = content_full
                    
                similarity_info = {
                    "rank": i,
                    "similarity_score": round(score, 4),
                    "similarity_percentage": round(score * 100, 2),
                    "content_preview": content_preview,
                    "metadata": doc.metadata,
                    "document_source": doc.metadata.get("source", "Unknown"),
                    "document_title": doc.metadata.get("title", doc.metadata.get("filename", "Unknown"))
                }
                response["similarity_search"]["top_matches"].append(similarity_info)
            
            # ìœ ì‚¬ë„ ì„ê³„ê°’ ë¯¸ë‹¬ì‹œ ë‹µë³€ ìˆ˜ì • ë° ì¶”ì²œ ì§ˆë¬¸ ìƒì„±
            # ChatGPT ëª¨ë¸ì€ 80% ë¯¸ë§Œì‹œì—ë§Œ ì ìš© (ë¡œì»¬LLMì€ ìì²´ ë¡œì§ ì‚¬ìš©)
            if not similarity_threshold_met and 'gpt' in llm_model.lower():
                # ë‚®ì€ ìœ ì‚¬ë„ì‹œ ë‹µë³€ì„ ì•ˆë‚´ ë©”ì‹œì§€ë¡œ ë³€ê²½
                response["answer"] = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (ìµœê³  ìœ ì‚¬ë„: {max_similarity*100:.1f}%).\n\nì•„ë˜ì™€ ê°™ì€ ì§ˆë¬¸ë“¤ì€ ì–´ë– ì‹ ê°€ìš”?"
                
                # ì¶”ì²œ ì§ˆë¬¸ ìƒì„± (ìƒìœ„ 3ê°œ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜)
                suggested_questions = self._generate_suggested_questions(similarity_results[:3], question)
                response["suggested_questions"] = suggested_questions
            
            # Cache the response (only for non-memory queries)
            if use_cache and not use_memory:
                self.cache_manager.set(question, response, llm_model)
            
            # Update search statistics
            self.update_search_stats(question, query_time, cache_hit, cache_time, total_time)
            
            response['_from_cache'] = False
            return response
        except Exception as e:
            # Import locally to avoid scope issues
            from utils.error_handler import detect_error_type, format_error_response
            
            # Detect error type and return user-friendly message
            error_code = detect_error_type(str(e))
            error_response = format_error_response(error_code, str(e))
            
            # Return formatted error with user-friendly message
            return {
                "error": True,
                "error_code": error_response["error_code"],
                "title": error_response["title"],
                "message": error_response["message"],
                "solution": error_response["solution"],
                "answer": f"âŒ {error_response['title']}: {error_response['message']}"
            }
    
    def _create_optimized_context(self, documents: List, max_length: int) -> str:
        """Create optimized context by selecting most relevant content within length limit"""
        context, _ = self._create_optimized_context_with_docs(documents, max_length)
        return context
    
    def _create_optimized_context_with_docs(self, documents: List, max_length: int) -> tuple:
        """Create optimized context and return both context string and used documents"""
        if not documents:
            return "", []
        
        context_parts = []
        used_documents = []
        current_length = 0
        
        # Sort documents by relevance (assume first documents are most relevant)
        # Use only the most relevant chunks and truncate if necessary
        for doc in documents:
            doc_content = doc.page_content
            
            # Skip very short or empty documents
            if len(doc_content.strip()) < 50:
                continue
            
            # If adding this document would exceed the limit
            if current_length + len(doc_content) > max_length:
                # Calculate how much space is left
                remaining_space = max_length - current_length
                
                if remaining_space > 200:  # Only add if there's meaningful space left
                    # Truncate the document content to fit
                    truncated_content = doc_content[:remaining_space-3] + "..."
                    context_parts.append(truncated_content)
                    
                    # Create a copy of the document with truncated content for consistency
                    from langchain.schema import Document
                    truncated_doc = Document(
                        page_content=truncated_content,
                        metadata=doc.metadata.copy()
                    )
                    used_documents.append(truncated_doc)
                break
            else:
                # Add the full document
                context_parts.append(doc_content)
                used_documents.append(doc)
                current_length += len(doc_content)
        
        # Join with double newlines for clear separation
        optimized_context = "\n\n".join(context_parts)
        
        # Log optimization info for debugging
        print(f"[CHUNKING OPTIMIZATION] Original docs: {len(documents)}, Used docs: {len(context_parts)}, "
              f"Context length: {len(optimized_context)} chars (limit: {max_length})")
        
        return optimized_context, used_documents
    
    def _generate_suggested_questions(self, similarity_results, original_question):
        """ìœ ì‚¬ë„ê°€ ë‚®ì„ ë•Œ ì¶”ì²œ ì§ˆë¬¸ ìƒì„±"""
        suggestions = []
        
        for doc, score in similarity_results:
            content = doc.page_content[:200]
            
            # ë¬¸ì„œ ë‚´ìš© ê¸°ë°˜ ì¶”ì²œ ì§ˆë¬¸ ìƒì„±
            if 'BCì¹´ë“œ' in content or 'ì‹ ìš©ì¹´ë“œ' in content:
                if 'í• ë¶€' in content:
                    suggestions.append("BCì¹´ë“œ í• ë¶€ ì´ìš© ë°©ë²•ì´ ê¶ê¸ˆí•˜ì‹œë‚˜ìš”?")
                elif 'ì¼ì‹œë¶ˆ' in content:
                    suggestions.append("ì‹ ìš©ì¹´ë“œ ì¼ì‹œë¶ˆ ê²°ì œ ì¥ì ì´ ê¶ê¸ˆí•˜ì‹œë‚˜ìš”?")
                elif 'ëŒ€ì¶œ' in content or 'í˜„ê¸ˆì„œë¹„ìŠ¤' in content:
                    suggestions.append("BCì¹´ë“œ ëŒ€ì¶œ ì„œë¹„ìŠ¤ì— ëŒ€í•´ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?")
                elif 'ìˆ˜ìˆ˜ë£Œ' in content:
                    suggestions.append("BCì¹´ë“œ ìˆ˜ìˆ˜ë£Œ ì²´ê³„ì— ëŒ€í•´ ë¬¸ì˜í•˜ì‹œë‚˜ìš”?")
                elif 'í¬ì¸íŠ¸' in content:
                    suggestions.append("BCì¹´ë“œ í¬ì¸íŠ¸ ì ë¦½ í˜œíƒì´ ê¶ê¸ˆí•˜ì‹œë‚˜ìš”?")
                else:
                    suggestions.append("BCì¹´ë“œ ì¼ë°˜ì ì¸ ì´ìš© ë°©ë²•ì´ ê¶ê¸ˆí•˜ì‹œë‚˜ìš”?")
        
        # ì¤‘ë³µ ì œê±° í›„ ìµœëŒ€ 3ê°œ ë°˜í™˜
        unique_suggestions = list(dict.fromkeys(suggestions))[:3]
        
        # ê¸°ë³¸ ì¶”ì²œ ì§ˆë¬¸ì´ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ì§ˆë¬¸ ì¶”ê°€
        if len(unique_suggestions) < 3:
            default_suggestions = [
                "BCì¹´ë“œ ì‹ ìš©ì¹´ë“œ ì¢…ë¥˜ì— ëŒ€í•´ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
                "BCì¹´ë“œ ë¶€ê°€ ì„œë¹„ìŠ¤ì— ëŒ€í•´ ë¬¸ì˜í•˜ì‹œë‚˜ìš”?",
                "BCì¹´ë“œ ê³ ê°ì„¼í„° ì—°ë½ì²˜ë¥¼ ì›í•˜ì‹œë‚˜ìš”?"
            ]
            for default in default_suggestions:
                if default not in unique_suggestions and len(unique_suggestions) < 3:
                    unique_suggestions.append(default)
        
        return unique_suggestions

    def clear_memory(self):
        """Clear conversation memory"""
        self.memory.clear()