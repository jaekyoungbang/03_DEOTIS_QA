import sqlite3
import redis
import json
import hashlib
from datetime import datetime, timedelta
import os
import threading
import time
from services.cache_manager import CacheManager as SQLiteCacheManager
# from services.redis_cache_manager import RedisCacheManager

class HybridCacheManager:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ìºì‹œ ì‹œìŠ¤í…œ:
    1. Redis: ì„ì‹œ ìºì‹œ (24ì‹œê°„)
    2. RDB: ì¸ê¸° ì§ˆë¬¸ ì˜êµ¬ ì €ì¥ (5íšŒ ì´ìƒ ì¡°íšŒ)
    3. ë¬¸ì„œ ë³€ê²½ ê°ì§€: ë§¤ì¼ RDB ë‚´ìš© ê²€ì¦
    """
    
    def __init__(self, popular_threshold=5):
        self.popular_threshold = popular_threshold
        
        # Redis cache (temporary) - gracefully handle connection failures
        # Redis disabled - using dummy cache
        print(f"âš ï¸ Redis disabled - using SQLite only")
        # Create a dummy cache manager to avoid errors
        class DummyCache:
            def __init__(self):
                self.connected = False
            def get(self, *args, **kwargs):
                return None
            def set(self, *args, **kwargs):
                return False
            def clear_all(self):
                return 0
            def get_stats(self):
                return {'connected': False, 'error': 'Redis disabled'}
        self.redis_cache = DummyCache()
        
        # SQLite cache for popular queries (permanent)
        self.popular_cache_db_path = 'data/cache/popular_cache.db'
        self.init_popular_cache_db()
        
        # ê¸°ì¡´ SQLiteCacheManagerëŠ” query_cache í…Œì´ë¸”ì„ ìƒì„±í•˜ë¯€ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        self.popular_cache = SQLiteCacheManager(
            cache_db_path='data/cache/popular_cache.db',
            ttl_hours=24*365  # 1ë…„ (ì‹¤ì§ˆì ìœ¼ë¡œ ì˜êµ¬)
        )
        
        # Document validation DB
        self.validation_db_path = 'data/cache/document_validation.db'
        self.init_validation_db()
        
        # Start background validation thread
        self.start_validation_thread()
        
        # Start daily cleanup thread
        self.start_daily_cleanup_thread()
    
    def init_popular_cache_db(self):
        """ì¸ê¸° ì§ˆë¬¸ DB ì´ˆê¸°í™” - popular_questions í…Œì´ë¸” ìƒì„±"""
        os.makedirs(os.path.dirname(self.popular_cache_db_path), exist_ok=True)
        
        conn = sqlite3.connect(self.popular_cache_db_path)
        cursor = conn.cursor()
        
        # popular_questions í…Œì´ë¸” ìƒì„±
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS popular_questions (
                query_hash TEXT PRIMARY KEY,
                question TEXT NOT NULL,
                answer TEXT NOT NULL,
                similarity_data TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                last_accessed TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                hit_count INTEGER DEFAULT 5,
                llm_model TEXT,
                vector_count INTEGER
            )
        ''')
        
        # ì¸ë±ìŠ¤ ìƒì„±
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_hit_count 
            ON popular_questions(hit_count DESC)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_last_accessed 
            ON popular_questions(last_accessed)
        ''')
        
        conn.commit()
        conn.close()
        print("âœ… popular_questions í…Œì´ë¸” ì´ˆê¸°í™” ì™„ë£Œ")
    
    def init_validation_db(self):
        """ë¬¸ì„œ ê²€ì¦ìš© DB ì´ˆê¸°í™”"""
        os.makedirs(os.path.dirname(self.validation_db_path), exist_ok=True)
        
        conn = sqlite3.connect(self.validation_db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS document_checksums (
                document_id TEXT PRIMARY KEY,
                document_path TEXT,
                checksum TEXT,
                last_checked TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS validation_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                validation_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                documents_checked INTEGER,
                changes_detected INTEGER,
                cache_cleared INTEGER
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def get(self, query, llm_model=None):
        """
        ìºì‹œì—ì„œ ì‘ë‹µ ì¡°íšŒ - ê²€ìƒ‰ íšŸìˆ˜ ì¶”ì 
        1. ê²€ìƒ‰ íšŸìˆ˜ ì¦ê°€ (ì „ì²´ ì¹´ìš´íŒ…)
        2. RDB (ì¸ê¸° ì§ˆë¬¸) í™•ì¸ - 5íšŒ ì´ìƒ
        3. Redis í™•ì¸ - 1~4íšŒ
        4. ë‘˜ ë‹¤ ì—†ìœ¼ë©´ None ë°˜í™˜ (RAG ê²€ìƒ‰ í•„ìš”)
        """
        # ê²€ìƒ‰ íšŸìˆ˜ ì¦ê°€ (ì „ì²´ ì¹´ìš´íŒ…)
        search_count = self._increment_search_count(query, llm_model)
        
        # 1. RDB ì¸ê¸° ì§ˆë¬¸ í™•ì¸ (5íšŒ ì´ìƒ)
        popular_result = self._get_from_popular_db(query, llm_model)
        if popular_result:
            self._increment_popular_hit_count(query, llm_model)
            popular_result['_from_cache'] = True
            popular_result['_cache_source'] = 'RDB'
            popular_result['_search_count'] = search_count
            popular_result['_note'] = f"RDBì—ì„œ ì‘ë‹µ (ì´ {search_count}ë²ˆì§¸ ê²€ìƒ‰)"
            print(f"ğŸ¯ RDBì—ì„œ ì‘ë‹µ: {query[:30]}... (ì´ {search_count}ë²ˆì§¸ ê²€ìƒ‰)")
            return popular_result
        
        # 2. Redis í™•ì¸ (1~4íšŒ)
        if hasattr(self.redis_cache, 'connected') and self.redis_cache.connected:
            redis_result = self.redis_cache.get(query, llm_model)
            if redis_result:
                # Redis ì¡°íšŒìˆ˜ ì¦ê°€
                redis_hit_count = self._increment_redis_hit_count(query, llm_model)
                
                # 5íšŒê°€ ë˜ë©´ RDBë¡œ ì´ë™
                if search_count >= self.popular_threshold:
                    self._promote_to_popular(query, redis_result, llm_model, search_count)
                    self._remove_from_redis(query, llm_model)
                    print(f"ğŸ”„ Redis â†’ RDB ì´ë™ ì™„ë£Œ: {query[:30]}... (5íšŒ ë„ë‹¬)")
                    
                    # RDBì—ì„œ ë‹¤ì‹œ ì¡°íšŒí•˜ì—¬ ë°˜í™˜
                    return self.get(query, llm_model)
                
                redis_result['_from_cache'] = True
                redis_result['_cache_source'] = 'Redis'
                redis_result['_search_count'] = search_count
                redis_result['_redis_hit_count'] = redis_hit_count
                redis_result['_note'] = f"Redisì—ì„œ ì‘ë‹µ (ì´ {search_count}ë²ˆì§¸ ê²€ìƒ‰)"
                print(f"âš¡ Redisì—ì„œ ì‘ë‹µ: {query[:30]}... (ì´ {search_count}ë²ˆì§¸ ê²€ìƒ‰)")
                return redis_result
        
        # 3. ìºì‹œ ì—†ìŒ - RAG ê²€ìƒ‰ í•„ìš”
        print(f"ğŸ” ìƒˆë¡œìš´ ì§ˆë¬¸: {query[:30]}... ({search_count}ë²ˆì§¸ ê²€ìƒ‰ - RAGì—ì„œ ê²€ìƒ‰)")
        return None
    
    def _increment_search_count(self, query, llm_model):
        """ì§ˆë¬¸ë³„ ì „ì²´ ê²€ìƒ‰ íšŸìˆ˜ ì¦ê°€"""
        try:
            cache_key = f"search_count:{self._generate_cache_key(query, llm_model)}"
            
            if hasattr(self.redis_cache, 'redis_client'):
                # Redisì—ì„œ ì¹´ìš´í„° ì¦ê°€
                current_count = self.redis_cache.redis_client.incr(cache_key)
                # ì¹´ìš´í„°ì— TTL ì„¤ì • (30ì¼)
                self.redis_cache.redis_client.expire(cache_key, 30 * 24 * 3600)
                return current_count
            else:
                # Redis ì—†ìœ¼ë©´ SQLiteë¡œ ëŒ€ì²´
                return self._increment_search_count_sqlite(query, llm_model)
                
        except Exception as e:
            print(f"âš ï¸ ê²€ìƒ‰ íšŸìˆ˜ ì¦ê°€ ì˜¤ë¥˜: {e}")
            return 1
    
    def _increment_search_count_sqlite(self, query, llm_model):
        """SQLiteë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰ íšŸìˆ˜ ê´€ë¦¬ (Redis ë°±ì—…)"""
        try:
            cache_key = self._generate_cache_key(query, llm_model)
            
            conn = sqlite3.connect(self.popular_cache_db_path)
            cursor = conn.cursor()
            
            # search_counts í…Œì´ë¸” ìƒì„± (ì—†ìœ¼ë©´)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS search_counts (
                    query_hash TEXT PRIMARY KEY,
                    question TEXT NOT NULL,
                    search_count INTEGER DEFAULT 1,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    last_searched TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ê²€ìƒ‰ íšŸìˆ˜ ì¦ê°€ ë˜ëŠ” ìƒˆë¡œ ìƒì„±
            cursor.execute('''
                INSERT INTO search_counts (query_hash, question, search_count, last_searched)
                VALUES (?, ?, 1, CURRENT_TIMESTAMP)
                ON CONFLICT(query_hash) DO UPDATE SET
                    search_count = search_count + 1,
                    last_searched = CURRENT_TIMESTAMP
            ''', (cache_key, query))
            
            # í˜„ì¬ ì¹´ìš´íŠ¸ ì¡°íšŒ
            cursor.execute('SELECT search_count FROM search_counts WHERE query_hash = ?', (cache_key,))
            result = cursor.fetchone()
            current_count = result[0] if result else 1
            
            conn.commit()
            conn.close()
            
            return current_count
            
        except Exception as e:
            print(f"âš ï¸ SQLite ê²€ìƒ‰ íšŸìˆ˜ ê´€ë¦¬ ì˜¤ë¥˜: {e}")
            return 1
    
    def set(self, query, response, llm_model=None):
        """
        RAG ê²€ìƒ‰ ê²°ê³¼ ìºì‹œì— ì €ì¥ (ì²« ê²€ìƒ‰ ì‹œ)
        - ì‘ë‹µì— RAG ê²€ìƒ‰ ì •ë³´ ì¶”ê°€
        - Redisì— ì €ì¥ (1~4íšŒì°¨ ê²€ìƒ‰ìš©)
        """
        try:
            # í˜„ì¬ ê²€ìƒ‰ íšŸìˆ˜ ì¡°íšŒ
            search_count = self._get_current_search_count(query, llm_model)
            
            # ì‘ë‹µì— RAG ê²€ìƒ‰ ì •ë³´ ì¶”ê°€
            if isinstance(response, dict):
                response['_from_cache'] = False
                response['_cache_source'] = 'RAG'
                response['_search_count'] = search_count
                response['_note'] = f"RAGì—ì„œ ê²€ìƒ‰ (ì´ {search_count}ë²ˆì§¸ ê²€ìƒ‰)"
            
            # ë¨¼ì € RDBì— ìˆëŠ”ì§€ í™•ì¸ (ì´ë¯¸ ì¸ê¸° ì§ˆë¬¸ì¸ì§€)
            if self._get_from_popular_db(query, llm_model):
                print(f"â„¹ï¸ ì´ë¯¸ ì¸ê¸° ì§ˆë¬¸ RDBì— ì¡´ì¬: {query[:30]}...")
                return True
            
            # Redisì— ì €ì¥ (ì²« ì €ì¥ì€ í•­ìƒ Redis)
            if hasattr(self.redis_cache, 'connected') and self.redis_cache.connected:
                success = self.redis_cache.set(query, response, llm_model)
                if success:
                    # Redis ì¡°íšŒìˆ˜ 1ë¡œ ì´ˆê¸°í™”
                    cache_key = self._generate_redis_hit_key(query, llm_model)
                    self.redis_cache.redis_client.setex(cache_key, 7*24*3600, 1)
                    print(f"ğŸ“¥ RAG ê²°ê³¼ Redis ì €ì¥: {query[:30]}... ({search_count}ë²ˆì§¸ ê²€ìƒ‰)")
                return success
            else:
                # Redisê°€ ì—†ìœ¼ë©´ SQLite ìºì‹œì— ì €ì¥ (Fallback)
                from services.cache_manager import CacheManager as SQLiteCacheManager
                sqlite_cache = SQLiteCacheManager(ttl_hours=24)
                success = sqlite_cache.set(query, response, llm_model)
                print(f"âš ï¸ Redis ë¶ˆê°€ëŠ¥ - SQLiteì— ì €ì¥: {query[:30]}... ({search_count}ë²ˆì§¸ ê²€ìƒ‰)")
                return success
                
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    def _get_current_search_count(self, query, llm_model):
        """í˜„ì¬ ê²€ìƒ‰ íšŸìˆ˜ ì¡°íšŒ (ì¦ê°€ ì—†ì´)"""
        try:
            cache_key = f"search_count:{self._generate_cache_key(query, llm_model)}"
            
            if hasattr(self.redis_cache, 'redis_client'):
                count = self.redis_cache.redis_client.get(cache_key)
                return int(count) if count else 1
            else:
                # SQLiteì—ì„œ ì¡°íšŒ
                cache_key_hash = self._generate_cache_key(query, llm_model)
                conn = sqlite3.connect(self.popular_cache_db_path)
                cursor = conn.cursor()
                cursor.execute('SELECT search_count FROM search_counts WHERE query_hash = ?', (cache_key_hash,))
                result = cursor.fetchone()
                conn.close()
                return result[0] if result else 1
                
        except Exception as e:
            print(f"âš ï¸ ê²€ìƒ‰ íšŸìˆ˜ ì¡°íšŒ ì˜¤ë£Œ: {e}")
            return 1
    
    def _increment_redis_hit_count(self, query, llm_model):
        """Redisì—ì„œ ì¡°íšŒìˆ˜ ì¦ê°€ ë° ìƒˆë¡œìš´ ì¡°íšŒìˆ˜ ë°˜í™˜"""
        try:
            cache_key = self._generate_redis_hit_key(query, llm_model)
            
            if hasattr(self.redis_cache, 'connected') and self.redis_cache.connected:
                # Redisì—ì„œ í˜„ì¬ ì¡°íšŒìˆ˜ ê°€ì ¸ì˜¤ê¸°
                current_hits = self.redis_cache.redis_client.get(cache_key)
                current_hits = int(current_hits) if current_hits else 0
                new_hits = current_hits + 1
                
                # ì¡°íšŒìˆ˜ ì—…ë°ì´íŠ¸ (7ì¼ TTL)
                self.redis_cache.redis_client.setex(cache_key, 7*24*3600, new_hits)
                
                print(f"ğŸ”„ Redis ì¡°íšŒìˆ˜ ì¦ê°€: {query[:30]}... ({new_hits}íšŒ)")
                return new_hits
                    
        except Exception as e:
            print(f"âš ï¸ ì¡°íšŒìˆ˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return 1
    
    def _get_from_popular_db(self, query, llm_model):
        """popular_questions í…Œì´ë¸”ì—ì„œ ì§ì ‘ ì¡°íšŒ"""
        try:
            cache_key = self._generate_cache_key(query, llm_model)
            
            conn = sqlite3.connect(self.popular_cache_db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT answer, similarity_data, hit_count 
                FROM popular_questions 
                WHERE query_hash = ?
            ''', (cache_key,))
            
            result = cursor.fetchone()
            conn.close()
            
            if result:
                answer, similarity_data, hit_count = result
                response = {'answer': answer}
                
                if similarity_data:
                    try:
                        response['similarity_search'] = json.loads(similarity_data)
                    except:
                        pass
                
                response['_hit_count'] = hit_count
                return response
                
        except Exception as e:
            print(f"âš ï¸ popular_questions ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return None
    
    def _increment_popular_hit_count(self, query, llm_model):
        """ì¸ê¸° ì§ˆë¬¸ DB ì¡°íšŒìˆ˜ ì¦ê°€"""
        try:
            cache_key = self._generate_cache_key(query, llm_model)
            
            conn = sqlite3.connect(self.popular_cache_db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                UPDATE popular_questions 
                SET hit_count = hit_count + 1,
                    last_accessed = CURRENT_TIMESTAMP
                WHERE query_hash = ?
            ''', (cache_key,))
            
            conn.commit()
            conn.close()
        except Exception as e:
            print(f"âš ï¸ ì¸ê¸° ì§ˆë¬¸ ì¡°íšŒìˆ˜ ì¦ê°€ ì˜¤ë¥˜: {e}")
    
    def _generate_cache_key(self, query, llm_model):
        """ìºì‹œ í‚¤ ìƒì„±"""
        normalized_query = query.lower().strip()
        cache_input = f"{normalized_query}:{llm_model}" if llm_model else normalized_query
        hash_key = hashlib.md5(cache_input.encode()).hexdigest()
        return hash_key
    
    def _generate_redis_hit_key(self, query, llm_model):
        """Redis ì¡°íšŒìˆ˜ í‚¤ ìƒì„±"""
        hash_key = self._generate_cache_key(query, llm_model)
        return f"rag:hits:{hash_key}"
    
    def _promote_to_popular(self, query, response, llm_model, hit_count):
        """Redisì—ì„œ ì¸ê¸° ì§ˆë¬¸ DBë¡œ ìŠ¹ê²©"""
        try:
            # popular_questions í…Œì´ë¸”ì— ì§ì ‘ ì €ì¥
            cache_key = self._generate_cache_key(query, llm_model)
            
            conn = sqlite3.connect(self.popular_cache_db_path)
            cursor = conn.cursor()
            
            # responseê°€ dictì¸ ê²½ìš° ì²˜ë¦¬
            if isinstance(response, dict):
                answer = response.get('answer', '')
                similarity_data = json.dumps(response.get('similarity_search', {}))
                vector_count = response.get('similarity_search', {}).get('total_results', 0)
            else:
                answer = str(response)
                similarity_data = None
                vector_count = 0
            
            cursor.execute('''
                INSERT OR REPLACE INTO popular_questions 
                (query_hash, question, answer, similarity_data, hit_count, llm_model, vector_count, last_accessed)
                VALUES (?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            ''', (cache_key, query, answer, similarity_data, hit_count, llm_model, vector_count))
            
            conn.commit()
            conn.close()
            
            print(f"â­ ì¸ê¸° ì§ˆë¬¸ ìŠ¹ê²©: {query[:30]}... ({hit_count}íšŒ)")
            
        except Exception as e:
            print(f"âš ï¸ ì¸ê¸° ì§ˆë¬¸ ìŠ¹ê²© ì˜¤ë¥˜: {e}")
    
    def _remove_from_redis(self, query, llm_model):
        """Redisì—ì„œ ì§ˆë¬¸ê³¼ ì¡°íšŒìˆ˜ ëª¨ë‘ ì œê±° (SQLiteë¡œ ì´ë™ í›„)"""
        try:
            if hasattr(self.redis_cache, 'connected') and self.redis_cache.connected:
                # ì§ˆë¬¸ ìºì‹œ ì‚­ì œ
                query_removed = self.redis_cache.delete(query, llm_model)
                
                # ì¡°íšŒìˆ˜ ì‚­ì œ
                hit_key = self._generate_redis_hit_key(query, llm_model)
                hit_removed = self.redis_cache.redis_client.delete(hit_key)
                
                print(f"ğŸ—‘ï¸ Redisì—ì„œ ì‚­ì œ ì™„ë£Œ: ì§ˆë¬¸={query_removed}, ì¡°íšŒìˆ˜={hit_removed}")
                return query_removed or hit_removed
                
        except Exception as e:
            print(f"âš ï¸ Redis ì œê±° ì˜¤ë¥˜: {e}")
        return False
    
    def _old_increment_and_check_popularity(self, query, llm_model=None):
        """ê¸°ì¡´ ì¡°íšŒìˆ˜ ì¦ê°€ í•¨ìˆ˜ (í˜¸í™˜ì„±)"""
        try:
            # Redisì—ì„œ ì¡°íšŒìˆ˜ ì¦ê°€ (Redisê°€ ì—°ê²°ëœ ê²½ìš°ë§Œ)
            if self.redis_cache.connected:
                cache_key = self._generate_cache_key(query, llm_model)
                hit_key = f"{cache_key}:hits"
                
                hit_count = self.redis_cache.redis_client.get(hit_key)
                if hit_count and int(hit_count) >= self.popular_threshold:
                    # ì¸ê¸° ì§ˆë¬¸ì´ ëœ ê²½ìš° RDBì— ì €ì¥
                    self._promote_to_popular(query, llm_model)
                    
        except Exception as e:
            print(f"Error checking popularity: {e}")
    
    
    def _store_document_info(self, response):
        """ì‘ë‹µì— í¬í•¨ëœ ë¬¸ì„œ ì •ë³´ ì €ì¥"""
        try:
            source_docs = response.get('source_documents', [])
            
            conn = sqlite3.connect(self.validation_db_path)
            cursor = conn.cursor()
            
            for doc in source_docs:
                metadata = doc.get('metadata', {})
                doc_id = metadata.get('filename', metadata.get('title', 'unknown'))
                
                # ë¬¸ì„œ ì²´í¬ì„¬ ê³„ì‚° (ë‚´ìš© ê¸°ë°˜)
                content = doc.get('content', '')
                checksum = hashlib.md5(content.encode()).hexdigest()
                
                cursor.execute('''
                    INSERT OR REPLACE INTO document_checksums 
                    (document_id, checksum, last_checked)
                    VALUES (?, ?, CURRENT_TIMESTAMP)
                ''', (doc_id, checksum))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            print(f"Error storing document info: {e}")
    
    def validate_documents(self):
        """
        ë¬¸ì„œ ë³€ê²½ ê°ì§€ ë° ìºì‹œ ë¬´íš¨í™”
        ë§¤ì¼ ì‹¤í–‰ë˜ì–´ ë¬¸ì„œê°€ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
        """
        try:
            print("ğŸ” ë¬¸ì„œ ë³€ê²½ ê°ì§€ ì‹œì‘...")
            
            from models.embeddings import EmbeddingManager
            from models.vectorstore import VectorStoreManager
            
            # í˜„ì¬ ë²¡í„°DBì˜ ë¬¸ì„œë“¤ ê°€ì ¸ì˜¤ê¸°
            embedding_manager = EmbeddingManager()
            vectorstore_manager = VectorStoreManager(embedding_manager.get_embeddings())
            
            # ìƒ˜í”Œ ê²€ìƒ‰ìœ¼ë¡œ í˜„ì¬ ë¬¸ì„œë“¤ í™•ì¸
            sample_docs = vectorstore_manager.similarity_search("test", k=50)
            
            conn = sqlite3.connect(self.validation_db_path)
            cursor = conn.cursor()
            
            changes_detected = 0
            documents_checked = 0
            
            for doc in sample_docs:
                metadata = doc.metadata
                doc_id = metadata.get('filename', metadata.get('title', 'unknown'))
                content = doc.page_content
                current_checksum = hashlib.md5(content.encode()).hexdigest()
                
                # ì´ì „ ì²´í¬ì„¬ê³¼ ë¹„êµ
                cursor.execute('''
                    SELECT checksum FROM document_checksums 
                    WHERE document_id = ?
                ''', (doc_id,))
                
                result = cursor.fetchone()
                if result and result[0] != current_checksum:
                    changes_detected += 1
                    print(f"ğŸ“ ë¬¸ì„œ ë³€ê²½ ê°ì§€: {doc_id}")
                    
                    # ì²´í¬ì„¬ ì—…ë°ì´íŠ¸
                    cursor.execute('''
                        UPDATE document_checksums 
                        SET checksum = ?, last_checked = CURRENT_TIMESTAMP
                        WHERE document_id = ?
                    ''', (current_checksum, doc_id))
                
                documents_checked += 1
            
            # ë³€ê²½ì´ ê°ì§€ë˜ë©´ ê´€ë ¨ ìºì‹œ ë¬´íš¨í™”
            cache_cleared = 0
            if changes_detected > 0:
                print(f"âš ï¸ {changes_detected}ê°œ ë¬¸ì„œ ë³€ê²½ ê°ì§€ - ìºì‹œ ì •ë¦¬ ì¤‘...")
                
                # Redis ìºì‹œ ì „ì²´ ì‚­ì œ
                self.redis_cache.clear_all()
                
                # ì¸ê¸° ì§ˆë¬¸ ìºì‹œë„ ë¬´íš¨í™” (ì„ íƒì )
                cache_cleared = self.popular_cache.clear_all()
                
                print(f"ğŸ§¹ {cache_cleared}ê°œ ìºì‹œ í•­ëª© ì‚­ì œ ì™„ë£Œ")
            
            # ê²€ì¦ ë¡œê·¸ ì €ì¥
            cursor.execute('''
                INSERT INTO validation_log 
                (documents_checked, changes_detected, cache_cleared)
                VALUES (?, ?, ?)
            ''', (documents_checked, changes_detected, cache_cleared))
            
            conn.commit()
            conn.close()
            
            print(f"âœ… ë¬¸ì„œ ê²€ì¦ ì™„ë£Œ: {documents_checked}ê°œ í™•ì¸, {changes_detected}ê°œ ë³€ê²½")
            
            return {
                'documents_checked': documents_checked,
                'changes_detected': changes_detected,
                'cache_cleared': cache_cleared
            }
            
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ê²€ì¦ ì˜¤ë¥˜: {e}")
            return {'error': str(e)}
    
    def start_validation_thread(self):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë§¤ì¼ ë¬¸ì„œ ê²€ì¦ ì‹¤í–‰"""
        def validation_worker():
            while True:
                try:
                    # 24ì‹œê°„ ëŒ€ê¸°
                    time.sleep(24 * 60 * 60)  # 24 hours
                    
                    # ë¬¸ì„œ ê²€ì¦ ì‹¤í–‰
                    self.validate_documents()
                    
                except Exception as e:
                    print(f"Validation thread error: {e}")
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ 1ì‹œê°„ í›„ ì¬ì‹œë„
                    time.sleep(60 * 60)
        
        validation_thread = threading.Thread(target=validation_worker, daemon=True)
        validation_thread.start()
        print("ğŸ•’ ë¬¸ì„œ ê²€ì¦ ìŠ¤ë ˆë“œ ì‹œì‘ (24ì‹œê°„ ê°„ê²©)")
    
    def get_stats(self):
        """ìºì‹œ í†µê³„ ì •ë³´"""
        redis_stats = self.redis_cache.get_stats()
        popular_stats = self.popular_cache.get_stats()
        
        # ê²€ì¦ ë¡œê·¸ ì¡°íšŒ
        try:
            conn = sqlite3.connect(self.validation_db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT validation_date, documents_checked, changes_detected, cache_cleared
                FROM validation_log 
                ORDER BY validation_date DESC 
                LIMIT 5
            ''')
            
            validation_history = cursor.fetchall()
            conn.close()
            
        except:
            validation_history = []
        
        return {
            'redis_cache': redis_stats,
            'popular_cache': popular_stats,
            'validation_history': [
                {
                    'date': row[0],
                    'checked': row[1],
                    'changes': row[2],
                    'cleared': row[3]
                } for row in validation_history
            ],
            'cache_strategy': {
                'redis_ttl': '24ì‹œê°„',
                'popular_threshold': f'{self.popular_threshold}íšŒ ì´ìƒ',
                'validation_interval': '24ì‹œê°„'
            }
        }
    
    def clear_all(self):
        """ëª¨ë“  ìºì‹œ ì‚­ì œ (Redis + RDB ì „ì²´)"""
        # Redis ìºì‹œ ì‚­ì œ
        redis_cleared = self.redis_cache.clear_all()
        
        # RDB ìºì‹œ ì‚­ì œ (popular_questions í¬í•¨)
        popular_cleared = 0
        try:
            conn = sqlite3.connect(self.popular_cache_db_path)
            cursor = conn.cursor()
            
            # popular_questions í…Œì´ë¸” ì™„ì „ ì‚­ì œ
            cursor.execute('DELETE FROM popular_questions')
            popular_cleared = cursor.rowcount
            
            # query_cache í…Œì´ë¸”ë„ ì‚­ì œ (ìˆë‹¤ë©´)
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='query_cache'")
            if cursor.fetchone():
                cursor.execute('DELETE FROM query_cache')
                popular_cleared += cursor.rowcount
            
            conn.commit()
            conn.close()
            
            print(f"âœ… RDB ì „ì²´ ì‚­ì œ ì™„ë£Œ: {popular_cleared}ê°œ í•­ëª©")
            
        except Exception as e:
            print(f"âš ï¸ RDB ì‚­ì œ ì˜¤ë¥˜: {e}")
        
        # ê²€ìƒ‰ íšŸìˆ˜ ì¹´ìš´í„°ë„ ì´ˆê¸°í™”
        self._clear_search_counters()
        
        return {
            'redis_cleared': redis_cleared,
            'rdb_cleared': popular_cleared,
            'total': redis_cleared + popular_cleared,
            'message': 'Redis + RDB ì „ì²´ ìºì‹œ ì‚­ì œ ì™„ë£Œ'
        }
    
    def _clear_search_counters(self):
        """ê²€ìƒ‰ íšŸìˆ˜ ì¹´ìš´í„° ì´ˆê¸°í™”"""
        try:
            # Redisì—ì„œ ê²€ìƒ‰ íšŸìˆ˜ í‚¤ë“¤ ì‚­ì œ
            if hasattr(self.redis_cache, 'redis_client'):
                pattern = "search_count:*"
                keys = self.redis_cache.redis_client.keys(pattern)
                if keys:
                    self.redis_cache.redis_client.delete(*keys)
                    print(f"âœ… ê²€ìƒ‰ íšŸìˆ˜ ì¹´ìš´í„° {len(keys)}ê°œ ì´ˆê¸°í™”")
        except Exception as e:
            print(f"âš ï¸ ê²€ìƒ‰ ì¹´ìš´í„° ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    
    def start_daily_cleanup_thread(self):
        """ë§¤ì¼ 00ì‹œ ìºì‹œ ì •ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘"""
        def daily_cleanup():
            while True:
                # ë‹¤ìŒ 00ì‹œê¹Œì§€ ëŒ€ê¸°
                now = datetime.now()
                tomorrow_midnight = (now + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
                seconds_until_midnight = (tomorrow_midnight - now).total_seconds()
                
                time.sleep(seconds_until_midnight)
                
                try:
                    print("ğŸ•› ë§¤ì¼ 00ì‹œ ìºì‹œ ì •ë¦¬ ì‹œì‘...")
                    self.daily_cache_cleanup()
                except Exception as e:
                    print(f"âŒ ë§¤ì¼ ìºì‹œ ì •ë¦¬ ì˜¤ë¥˜: {e}")
        
        cleanup_thread = threading.Thread(target=daily_cleanup, daemon=True)
        cleanup_thread.start()
        print("âœ… ë§¤ì¼ ìºì‹œ ì •ë¦¬ ìŠ¤ì¼€ì¤„ ì‹œì‘ë¨")
    
    def daily_cache_cleanup(self):
        """ë§¤ì¼ 00ì‹œ ì‹¤í–‰ë˜ëŠ” ìºì‹œ ì •ë¦¬"""
        print("ğŸ§¹ Redis ìºì‹œ ì •ë¦¬ ì¤‘...")
        
        # 1. ë¬¸ì„œ ë³€ê²½ ê²€ì¦
        self.validate_documents()
        
        # 2. Redisì—ì„œ ì¡°íšŒìˆ˜ 5íšŒ ë¯¸ë§Œ ë°ì´í„°ëŠ” TTLë¡œ ìë™ ì‚­ì œë¨
        redis_cleared = 0
        if hasattr(self.redis_cache, 'connected') and self.redis_cache.connected:
            try:
                stats = self.redis_cache.get_stats()
                print(f"ğŸ“Š Redis ìƒíƒœ: {stats.get('keys_count', 0)}ê°œ í‚¤ ì¡´ì¬")
            except Exception as e:
                print(f"âš ï¸ Redis í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # 3. ì¸ê¸° ì§ˆë¬¸ DB ê²€ì¦ (5íšŒ ì´ìƒ ì¡°íšŒëœ í•­ëª©ë“¤ ìœ ì§€)
        popular_verified = self._verify_popular_cache()
        
        print(f"âœ… ë§¤ì¼ ì •ë¦¬ ì™„ë£Œ - ì¸ê¸°ì§ˆë¬¸ {popular_verified}ê°œ ê²€ì¦ë¨")
        
        # ì •ë¦¬ ë¡œê·¸ ì €ì¥
        self._log_daily_cleanup(redis_cleared, popular_verified)
    
    def _verify_popular_cache(self):
        """ì¸ê¸° ì§ˆë¬¸ ìºì‹œ ê²€ì¦ - 5íšŒ ë¯¸ë§Œ í•­ëª© ì‚­ì œ"""
        try:
            conn = sqlite3.connect(self.popular_cache.cache_db_path)
            cursor = conn.cursor()
            
            # ì¡°íšŒìˆ˜ê°€ threshold ë¯¸ë§Œì¸ í•­ëª©ë“¤ ì°¾ê¸°
            cursor.execute('''
                SELECT id, hit_count FROM cache 
                WHERE hit_count < ? 
            ''', (self.popular_threshold,))
            
            low_hit_items = cursor.fetchall()
            
            # threshold ë¯¸ë§Œ í•­ëª©ë“¤ ì‚­ì œ
            if low_hit_items:
                item_ids = [str(item[0]) for item in low_hit_items]
                cursor.execute(f'''
                    DELETE FROM cache 
                    WHERE id IN ({','.join(['?' for _ in item_ids])})
                ''', item_ids)
                print(f"ğŸ—‘ï¸ ì¡°íšŒìˆ˜ {self.popular_threshold}íšŒ ë¯¸ë§Œ {len(low_hit_items)}ê°œ í•­ëª© ì‚­ì œ")
            
            # í˜„ì¬ ë‚¨ì€ í•­ëª© ìˆ˜ í™•ì¸
            cursor.execute('SELECT COUNT(*) FROM cache')
            remaining_count = cursor.fetchone()[0]
            
            conn.commit()
            conn.close()
            
            return remaining_count
            
        except Exception as e:
            print(f"âš ï¸ ì¸ê¸° ìºì‹œ ê²€ì¦ ì˜¤ë¥˜: {e}")
            return 0
    
    def _log_daily_cleanup(self, redis_cleared, popular_verified):
        """ë§¤ì¼ ì •ë¦¬ ë¡œê·¸ ê¸°ë¡"""
        try:
            conn = sqlite3.connect(self.validation_db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO validation_log (
                    validation_date, documents_checked, changes_detected, cache_cleared
                ) VALUES (?, ?, ?, ?)
            ''', (datetime.now(), 0, 0, popular_verified))
            
            conn.commit()
            conn.close()
        except Exception as e:
            print(f"âš ï¸ ì •ë¦¬ ë¡œê·¸ ì €ì¥ ì˜¤ë¥˜: {e}")