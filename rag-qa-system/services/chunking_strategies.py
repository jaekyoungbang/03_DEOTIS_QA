from typing import List, Dict, Any
import re
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.schema import Document
import time
import os
from docx import Document as DocxDocument

class ChunkingBenchmarker:
    """ì²­í‚¹ ì „ëµ ë²¤ì¹˜ë§ˆí‚¹ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.strategies = {
            'basic': BasicChunkingStrategy(),
            'semantic': SemanticChunkingStrategy(),
            'hybrid': HybridChunkingStrategy()
        }
        self.results = []
    
    def benchmark_strategy(self, strategy_name: str, documents: List[Document]) -> Dict:
        """íŠ¹ì • ì²­í‚¹ ì „ëµ ë²¤ì¹˜ë§ˆí‚¹"""
        if strategy_name not in self.strategies:
            raise ValueError(f"Unknown strategy: {strategy_name}")
        
        strategy = self.strategies[strategy_name]
        start_time = time.time()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (ê°„ì†Œí™”)
        start_memory = 0
        
        # ì²­í‚¹ ì‹¤í–‰
        chunks = strategy.split_documents(documents)
        
        end_time = time.time()
        end_memory = 0
        
        # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
        quality_metrics = self._calculate_quality_metrics(chunks)
        
        result = {
            'strategy': strategy_name,
            'processing_time': end_time - start_time,
            'memory_used': end_memory - start_memory,
            'total_chunks': len(chunks),
            'avg_chunk_size': sum(len(chunk.page_content) for chunk in chunks) / len(chunks) if chunks else 0,
            'std_chunk_size': 0,  # ê°„ì†Œí™”
            'quality_metrics': quality_metrics,
            'timestamp': time.time()
        }
        
        self.results.append(result)
        return result
    
    def _calculate_quality_metrics(self, chunks: List[Document]) -> Dict:
        """ì²­í‚¹ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if not chunks:
            return {'coherence_score': 0, 'completeness_score': 0}
        
        chunk_sizes = [len(chunk.page_content) for chunk in chunks]
        
        # ì¼ê´€ì„± ì ìˆ˜ (ê°„ì†Œí™”ëœ ë²„ì „)
        if chunk_sizes:
            avg_size = sum(chunk_sizes) / len(chunk_sizes)
            max_deviation = max(abs(size - avg_size) for size in chunk_sizes)
            coherence_score = max(0, 1 - (max_deviation / avg_size)) if avg_size > 0 else 1
        else:
            coherence_score = 0
        
        # ì™„ì„±ë„ ì ìˆ˜ (ë¹ˆ ì²­í¬ë‚˜ ë„ˆë¬´ ì‘ì€ ì²­í¬ê°€ ì ì„ìˆ˜ë¡ ì¢‹ìŒ)
        min_size_threshold = 100
        valid_chunks = sum(1 for size in chunk_sizes if size >= min_size_threshold)
        completeness_score = valid_chunks / len(chunks) if chunks else 0
        
        return {
            'coherence_score': coherence_score,
            'completeness_score': completeness_score,
            'avg_size': sum(chunk_sizes) / len(chunk_sizes) if chunk_sizes else 0,
            'size_variance': 0  # ê°„ì†Œí™”
        }
    
    def compare_strategies(self, documents: List[Document]) -> Dict:
        """ëª¨ë“  ì „ëµ ë¹„êµ"""
        comparison_results = {}
        
        for strategy_name in self.strategies.keys():
            try:
                result = self.benchmark_strategy(strategy_name, documents)
                comparison_results[strategy_name] = result
            except Exception as e:
                comparison_results[strategy_name] = {
                    'error': str(e),
                    'strategy': strategy_name
                }
        
        # ìµœì  ì „ëµ ì„ íƒ
        best_strategy = self._select_best_strategy(comparison_results)
        
        return {
            'results': comparison_results,
            'best_strategy': best_strategy,
            'comparison_summary': self._generate_comparison_summary(comparison_results)
        }
    
    def _select_best_strategy(self, results: Dict) -> str:
        """ìµœì  ì „ëµ ì„ íƒ (ì¢…í•© ì ìˆ˜ ê¸°ë°˜)"""
        scores = {}
        
        for strategy, result in results.items():
            if 'error' in result:
                scores[strategy] = 0
                continue
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ì†ë„ + í’ˆì§ˆ)
            speed_score = 1 / (result['processing_time'] + 0.1)  # ë¹ ë¥¼ìˆ˜ë¡ ì¢‹ìŒ
            quality_score = (
                result['quality_metrics']['coherence_score'] + 
                result['quality_metrics']['completeness_score']
            ) / 2
            
            scores[strategy] = (speed_score * 0.3) + (quality_score * 0.7)
        
        return max(scores, key=scores.get) if scores else 'basic'
    
    def _generate_comparison_summary(self, results: Dict) -> Dict:
        """ë¹„êµ ìš”ì•½ ìƒì„±"""
        summary = {
            'fastest': None,
            'highest_quality': None,
            'most_balanced': None
        }
        
        valid_results = {k: v for k, v in results.items() if 'error' not in v}
        
        if not valid_results:
            return summary
        
        # ê°€ì¥ ë¹ ë¥¸ ì „ëµ
        fastest = min(valid_results.items(), 
                     key=lambda x: x[1]['processing_time'])
        summary['fastest'] = fastest[0]
        
        # ê°€ì¥ ë†’ì€ í’ˆì§ˆ
        highest_quality = max(valid_results.items(),
                            key=lambda x: (x[1]['quality_metrics']['coherence_score'] + 
                                         x[1]['quality_metrics']['completeness_score']) / 2)
        summary['highest_quality'] = highest_quality[0]
        
        # ê°€ì¥ ê· í˜•ì¡íŒ ì „ëµ (ì´ë¯¸ ê³„ì‚°ëœ best_strategyì™€ ë™ì¼)
        summary['most_balanced'] = self._select_best_strategy(results)
        
        return summary

class BasicChunkingStrategy:
    """ê¸°ë³¸ ì²­í‚¹ ì „ëµ - ë¬¸ì ê¸°ë°˜"""
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        # Optimized for reduced LLM input length - smaller chunks work better
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        # ì„¹ì…˜ êµ¬ë¶„ìë¥¼ ì¶”ê°€í•˜ì—¬ ë” ë‚˜ì€ ì²­í‚¹
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=[
                "\n\n\n",  # ì„¹ì…˜ ê°„ êµ¬ë¶„
                "\n\n",    # ë‹¨ë½ êµ¬ë¶„
                "\n",      # ì¤„ êµ¬ë¶„
                ". ",      # ë¬¸ì¥ êµ¬ë¶„
                " ",       # ë‹¨ì–´ êµ¬ë¶„
                ""
            ]
        )
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• """
        # ì„¹ì…˜ ê¸°ë°˜ ì‚¬ì „ ì²˜ë¦¬
        enhanced_documents = self._preprocess_sections(documents)
        chunks = self.splitter.split_documents(enhanced_documents)
        
        # ì œëª© ì „ìš© ì²­í¬ í•„í„°ë§ - ë” ì—„ê²©í•œ ê¸°ì¤€ ì ìš©
        filtered_chunks = []
        for chunk in chunks:
            content = chunk.page_content.strip()
            
            # ì œëª© ì „ìš© ì²­í¬ì¸ì§€ í™•ì¸
            if self._is_title_only_chunk(content):
                print(f"   âŒ ì œëª© ì „ìš© ì²­í¬ í•„í„°ë§: {repr(content[:50])}...")
                continue
                
            # ìµœì†Œ ê¸¸ì´ ì²´í¬ (100ì ì´ìƒ)
            if len(content) >= 100:
                filtered_chunks.append(chunk)
            else:
                print(f"   âŒ ì§§ì€ ì²­í¬ í•„í„°ë§ ({len(content)}ì): {repr(content[:30])}...")
        
        print(f"   ğŸ“Š ì²­í‚¹ ê²°ê³¼: {len(chunks)}ê°œ â†’ {len(filtered_chunks)}ê°œ (í•„í„°ë§: {len(chunks) - len(filtered_chunks)}ê°œ)")
        return filtered_chunks if filtered_chunks else chunks  # ëª¨ë“  ì²­í¬ê°€ í•„í„°ë§ë˜ë©´ ì›ë³¸ ë°˜í™˜
    
    def _preprocess_sections(self, documents: List[Document]) -> List[Document]:
        """ì„¹ì…˜ êµ¬ë¶„ì„ ëª…í™•íˆ í•˜ê¸° ìœ„í•œ ì‚¬ì „ ì²˜ë¦¬"""
        processed_docs = []
        
        for doc in documents:
            content = doc.page_content
            
            # ìˆ«ì) ë˜ëŠ” ìˆ«ì-ìˆ«ì) íŒ¨í„´ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì„¹ì…˜ êµ¬ë¶„
            # ì˜ˆ: "10) íšŒì›ì œ ì—…ì†Œ", "1-1) ì¼ì‹œë¶ˆ êµ¬ë§¤" ë“±
            section_pattern = r'(\n)(\d+[-]?\d*\))'
            content = re.sub(section_pattern, r'\n\n\n\2', content)
            
            # ëŒ€ì œëª© êµ¬ë¶„ (ì˜ˆ: [ì¹´ë“œìƒí™œ TIP], /. ì‹ ìš©ì¹´ë“œ TIP)
            # ì œëª©ì´ ë‹¨ë…ìœ¼ë¡œ ë¶„ë¦¬ë˜ì§€ ì•Šë„ë¡ êµ¬ë¶„ìë¥¼ ìµœì†Œí™”
            title_pattern = r'(\n)(\[.*?\]|/\..*?)(\n)'
            # ì œëª© ì•ì—ë§Œ êµ¬ë¶„ì„ ë‘ê³ , ë’¤ëŠ” ë°”ë¡œ ì´ì–´ì§€ë„ë¡ í•¨
            content = re.sub(title_pattern, r'\n\n\2\n', content)
            
            # A-1., A-2. ê°™ì€ ì„¹ì…˜ êµ¬ë¶„
            section_letter_pattern = r'(\n)([A-Z]-\d+\.)'
            content = re.sub(section_letter_pattern, r'\n\n\n\2', content)
            
            processed_doc = Document(
                page_content=content,
                metadata=doc.metadata.copy()
            )
            processed_docs.append(processed_doc)
        
        return processed_docs
    
    def _is_title_only_chunk(self, content: str) -> bool:
        """ì œëª© ì „ìš© ì²­í¬ì¸ì§€ í™•ì¸"""
        content = content.strip()
        
        # ì •í™•íˆ "[ë¯¼ì›ì ‘ìˆ˜ë°©ë²• ì•ˆë‚´]" ê°™ì€ íŒ¨í„´ë§Œ ìˆëŠ” ê²½ìš°
        if content.startswith('[') and content.endswith(']') and '\n' not in content:
            return True
        
        # /. ë¡œ ì‹œì‘í•˜ëŠ” ì œëª©ë§Œ ìˆëŠ” ê²½ìš°  
        if content.startswith('/.') and len(content) < 100:
            return True
            
        # ì¤„ë°”ê¿ˆì´ ì—†ê³  ì§§ì€ ê²½ìš° (ì œëª©ì¼ ê°€ëŠ¥ì„±)
        if '\n' not in content and len(content) < 50:
            return True
            
        # ë§¤ìš° ì§§ì€ ì²­í¬ (50ì ë¯¸ë§Œ)
        if len(content) < 50:
            return True
        
        # ì œëª©ìœ¼ë¡œë§Œ êµ¬ì„±ëœ ê²½ìš° (ì˜ˆ: "[ë¯¼ì›ì ‘ìˆ˜ë°©ë²• ì•ˆë‚´]"ë§Œ ìˆê³  ë‹¤ë¥¸ ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš°)
        lines = [line.strip() for line in content.split('\n') if line.strip()]
        if len(lines) == 1 and lines[0].startswith('[') and lines[0].endswith(']'):
            return True
            
        return False

class SemanticChunkingStrategy:
    """ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ ì „ëµ (ê°„ì†Œí™” ë²„ì „)"""
    
    def __init__(self, min_chunk_size: int = 500, max_chunk_size: int = 1500):
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        # ê°„ì†Œí™”ëœ ë²„ì „ - ì˜ì¡´ì„± ì œê±°
        self.sentence_model = None
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ ë¶„í• """
        if not self.sentence_model:
            # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì „ëµ ì‚¬ìš©
            basic_strategy = BasicChunkingStrategy()
            return basic_strategy.split_documents(documents)
        
        all_chunks = []
        
        for doc in documents:
            chunks = self._split_document_semantically(doc)
            all_chunks.extend(chunks)
        
        return all_chunks
    
    def _split_document_semantically(self, document: Document) -> List[Document]:
        """ë‹¨ì¼ ë¬¸ì„œë¥¼ ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œ ë¶„í•  (ê°„ì†Œí™”)"""
        # ê°„ì†Œí™”ëœ ë²„ì „ - ë¬¸ì¥ êµ¬ë¶„ì ê¸°ë°˜ ë¶„í• 
        text = document.page_content
        
        # ê°„ë‹¨í•œ ë¬¸ì¥ ë¶„í•  (ë§ˆì¹¨í‘œ ê¸°ì¤€)
        sentences = [s.strip() for s in text.split('.') if s.strip()]
        
        if len(sentences) <= 1:
            return [document]
        
        # í¬ê¸° ê¸°ë°˜ ê·¸ë£¨í•‘
        chunks = self._group_sentences_by_size(sentences, document.metadata)
        
        return chunks
    
    def _group_sentences_by_size(self, sentences: List[str], metadata: Dict) -> List[Document]:
        """í¬ê¸° ê¸°ë°˜ ë¬¸ì¥ ê·¸ë£¨í•‘ (ê°„ì†Œí™”)"""
        chunks = []
        current_chunk = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence)
            
            # í˜„ì¬ ì²­í¬ì— ì¶”ê°€í• ì§€ íŒë‹¨
            if current_size + sentence_size <= self.max_chunk_size:
                current_chunk.append(sentence + '.')  # ë§ˆì¹¨í‘œ ë³µì›
                current_size += sentence_size
            else:
                # í˜„ì¬ ì²­í¬ ì™„ì„±
                if current_chunk and current_size >= self.min_chunk_size:
                    chunk_text = " ".join(current_chunk)
                    chunks.append(Document(
                        page_content=chunk_text,
                        metadata=metadata.copy()
                    ))
                
                # ìƒˆ ì²­í¬ ì‹œì‘
                current_chunk = [sentence + '.']
                current_size = sentence_size
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk:
            chunk_text = " ".join(current_chunk)
            if len(chunk_text) >= self.min_chunk_size:
                chunks.append(Document(
                    page_content=chunk_text,
                    metadata=metadata.copy()
                ))
        
        return chunks if chunks else [Document(
            page_content=" ".join(s + '.' for s in sentences),
            metadata=metadata.copy()
        )]

class HybridChunkingStrategy:
    """í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹ ì „ëµ - ê¸°ë³¸ + ì˜ë¯¸ ê¸°ë°˜"""
    
    def __init__(self):
        self.basic_strategy = BasicChunkingStrategy(chunk_size=800, chunk_overlap=150)
        self.semantic_strategy = SemanticChunkingStrategy(min_chunk_size=400, max_chunk_size=1200)
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ë¬¸ì„œ ë¶„í• """
        # ë¨¼ì € ê¸°ë³¸ ì „ëµìœ¼ë¡œ ë¶„í• 
        basic_chunks = self.basic_strategy.split_documents(documents)
        
        # í° ì²­í¬ë“¤ì€ ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œ ì¬ë¶„í• 
        final_chunks = []
        
        for chunk in basic_chunks:
            if len(chunk.page_content) > 1200:
                # í° ì²­í¬ëŠ” ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œ ì¬ë¶„í• 
                semantic_chunks = self.semantic_strategy._split_document_semantically(chunk)
                final_chunks.extend(semantic_chunks)
            else:
                final_chunks.append(chunk)
        
        return final_chunks

class S3BasicChunkingStrategy:
    """ê¸°ë³¸ ì²­í‚¹ ì „ëµ (s3 í´ë” ê¸°ë°˜)"""
    
    def __init__(self):
        self.s3_docs_path = '/mnt/d/99_DEOTIS_QA_SYSTEM/03_DEOTIS_QA/s3/'
        self.basic_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,  # Reduced for LLM optimization
            chunk_overlap=150,  # Proportionally reduced overlap
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """ê¸°ë³¸ ì²­í‚¹ ìˆ˜í–‰"""
        basic_chunks = []
        
        for document in documents:
            # ê¸°ë³¸ ì²­í‚¹ ìˆ˜í–‰
            chunks = self.basic_splitter.split_documents([document])
            
            for chunk in chunks:
                chunk.metadata['chunking_strategy'] = 's3-basic'
                chunk.metadata['data_source'] = 's3'
                basic_chunks.append(chunk)
        
        return basic_chunks

class S3CustomChunkingStrategy:
    """ì»¤ìŠ¤í…€ ì²­í‚¹ ì „ëµ (s3-chunking í´ë” ê¸°ë°˜)"""
    
    def __init__(self):
        self.s3_chunking_path = '/mnt/d/99_DEOTIS_QA_SYSTEM/03_DEOTIS_QA/s3-chunking/'
        self.card_knowledge = self._load_card_knowledge()
        self.basic_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,  # Reduced for LLM optimization
            chunk_overlap=150,  # Proportionally reduced overlap
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
    
    def _load_card_knowledge(self) -> Dict[str, str]:
        """ì „ë© ì¹´ë“œ ë¬¸ì„œë“¤ì—ì„œ ì§€ì‹ ì¶”ì¶œ (s3-chunking í´ë”)"""
        knowledge = {}
        
        try:
            # BCì¹´ë“œ ì‹ ìš©ì¹´ë“œ ì—…ë¬´ì²˜ë¦¬ ì•ˆë‚´
            doc1_path = os.path.join(self.s3_chunking_path, 'BCì¹´ë“œ(ì‹ ìš©ì¹´ë“œ ì—…ë¬´ì²˜ë¦¬ ì•ˆë‚´).docx')
            if os.path.exists(doc1_path):
                doc1 = DocxDocument(doc1_path)
                content1 = '\n'.join([paragraph.text for paragraph in doc1.paragraphs if paragraph.text.strip()])
                knowledge['business_guide'] = content1
                print(f"[OK] s3-chunking ì—…ë¬´ì²˜ë¦¬ ì•ˆë‚´ ë¬¸ì„œ ë¡œë“œ: {len(content1)}ì")
            
            # BCì¹´ë“œ ì¹´ë“œì´ìš©ì•ˆë‚´
            doc2_path = os.path.join(self.s3_chunking_path, 'BCì¹´ë“œ(ì¹´ë“œì´ìš©ì•ˆë‚´).docx')
            if os.path.exists(doc2_path):
                doc2 = DocxDocument(doc2_path)
                content2 = '\n'.join([paragraph.text for paragraph in doc2.paragraphs if paragraph.text.strip()])
                knowledge['usage_guide'] = content2
                print(f"[OK] s3-chunking ì´ìš©ì•ˆë‚´ ë¬¸ì„œ ë¡œë“œ: {len(content2)}ì")
                
        except Exception as e:
            print(f"[WARNING] s3-chunking ì „ë© ë¬¸ì„œ ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        return knowledge
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """ì „ë© ì§€ì‹ì„ í™œìš©í•œ ì»¤ìŠ¤í…€ ì²­í‚¹"""
        enhanced_chunks = []
        
        for document in documents:
            # ê¸°ë³¸ ì²­í‚¹ ìˆ˜í–‰
            basic_chunks = self.basic_splitter.split_documents([document])
            
            # ê° ì²­í¬ë¥¼ ì „ë© ì§€ì‹ìœ¼ë¡œ í–¥ìƒ
            for chunk in basic_chunks:
                enhanced_chunk = self._enhance_chunk_with_s3_knowledge(chunk)
                enhanced_chunks.append(enhanced_chunk)
        
        return enhanced_chunks
    
    def _enhance_chunk_with_s3_knowledge(self, chunk: Document) -> Document:
        """ì²­í¬ë¥¼ ì „ë© ì§€ì‹ìœ¼ë¡œ í–¥ìƒ"""
        content = chunk.page_content
        metadata = chunk.metadata.copy()
        
        # ì¹´ë“œ ê´€ë ¨ í‚¤ì›Œë“œ ê°ì§€
        card_keywords = [
            'ì¹´ë“œ', 'ì‹ ìš©ì¹´ë“œ', 'ì²´í¬ì¹´ë“œ', 'ì „ìš©ì¹´ë“œ',
            'ë¹„ë°€ë²ˆí˜¸', 'ì´ìš©í•œë„', 'ìˆ˜ìˆ˜ë£Œ', 'ì´ì',
            'ì—°ì²´ë£Œ', 'ê²°ì œ', 'ìŠ¹ì¸', 'ì·¨ì†Œ'
        ]
        
        matched_keywords = [kw for kw in card_keywords if kw in content]
        
        if matched_keywords:
            # ì „ë© ë°°ê²½ ì§€ì‹ ì¶”ê°€
            enhanced_content = content
            
            # ì—…ë¬´ì²˜ë¦¬ ê´€ë ¨ ì§€ì‹ ì¶”ê°€
            if any(kw in content for kw in ['ìŠ¹ì¸', 'ì·¨ì†Œ', 'ë¹„ë°€ë²ˆí˜¸']):
                if 'business_guide' in self.card_knowledge:
                    relevant_info = self._extract_relevant_info(
                        content, self.card_knowledge['business_guide']
                    )
                    if relevant_info:
                        enhanced_content += f"\n\n[ì „ë¥ ì—…ë¬´ì²˜ë¦¬ ì§€ì‹]: {relevant_info[:500]}..."
            
            # ì´ìš©ì•ˆë‚´ ê´€ë ¨ ì§€ì‹ ì¶”ê°€
            if any(kw in content for kw in ['ì´ìš©í•œë„', 'ìˆ˜ìˆ˜ë£Œ', 'ì´ì']):
                if 'usage_guide' in self.card_knowledge:
                    relevant_info = self._extract_relevant_info(
                        content, self.card_knowledge['usage_guide']
                    )
                    if relevant_info:
                        enhanced_content += f"\n\n[ì „ë¥ ì´ìš©ì•ˆë‚´ ì§€ì‹]: {relevant_info[:500]}..."
            
            metadata['enhanced_with_s3'] = True
            metadata['matched_keywords'] = matched_keywords
            metadata['chunking_strategy'] = 's3-custom'
            metadata['data_source'] = 's3-chunking'
            
            return Document(
                page_content=enhanced_content,
                metadata=metadata
            )
        
        # í‚¤ì›Œë“œ ë§¤ì¹˜ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì²­í¬ ë°˜í™˜
        metadata['chunking_strategy'] = 's3-custom-basic'
        metadata['data_source'] = 's3-chunking'
        return Document(
            page_content=content,
            metadata=metadata
        )
    
    def _extract_relevant_info(self, query_content: str, knowledge_base: str) -> str:
        """ì¿¼ë¦¬ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì§€ì‹ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ì¶”ì¶œ
        query_words = re.findall(r'\w+', query_content.lower())
        knowledge_sentences = knowledge_base.split('\n')
        
        relevant_sentences = []
        for sentence in knowledge_sentences:
            if len(sentence.strip()) > 20:  # ì¶©ë¶„íˆ ê¸´ ë¬¸ì¥ë§Œ
                sentence_lower = sentence.lower()
                matches = sum(1 for word in query_words if word in sentence_lower)
                if matches >= 2:  # 2ê°œ ì´ìƒ ë‹¨ì–´ ë§¤ì¹­
                    relevant_sentences.append(sentence.strip())
        
        return '. '.join(relevant_sentences[:3])  # ìƒìœ„ 3ê°œ ë¬¸ì¥

# ì „ì—­ ë²¤ì¹˜ë§ˆì»¤ ì¸ìŠ¤í„´ìŠ¤
chunking_benchmarker = ChunkingBenchmarker()

class CustomDelimiterChunkingStrategy:
    """ì»¤ìŠ¤í…€ êµ¬ë¶„ì ì²­í‚¹ ì „ëµ - /$$/ êµ¬ë¶„ì ê¸°ë°˜"""
    
    def __init__(self, delimiter: str = "/$$/" , fallback_chunk_size: int = 1000, fallback_overlap: int = 200):
        self.delimiter = delimiter
        self.fallback_splitter = RecursiveCharacterTextSplitter(
            chunk_size=fallback_chunk_size,
            chunk_overlap=fallback_overlap,
            separators=["\n\n", "\n", " ", ""]
        )
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """ì»¤ìŠ¤í…€ êµ¬ë¶„ìë¡œ ë¬¸ì„œ ë¶„í• """
        all_chunks = []
        
        for doc in documents:
            chunks = self._split_document_by_delimiter(doc)
            all_chunks.extend(chunks)
        
        return all_chunks
    
    def _split_document_by_delimiter(self, document: Document) -> List[Document]:
        """ë‹¨ì¼ ë¬¸ì„œë¥¼ ì»¤ìŠ¤í…€ êµ¬ë¶„ìë¡œ ë¶„í• """
        text = document.page_content
        metadata = document.metadata.copy()
        
        # ì»¤ìŠ¤í…€ êµ¬ë¶„ìê°€ ìˆëŠ”ì§€ í™•ì¸
        if self.delimiter in text:
            # ì»¤ìŠ¤í…€ êµ¬ë¶„ìë¡œ ë¶„í• 
            parts = text.split(self.delimiter)
            chunks = []
            
            for i, part in enumerate(parts):
                part = part.strip()
                if part:  # ë¹ˆ ë¶€ë¶„ ì œì™¸
                    chunk_metadata = metadata.copy()
                    chunk_metadata['chunking_strategy'] = 'custom_delimiter'
                    chunk_metadata['chunk_index'] = i
                    chunk_metadata['total_chunks'] = len([p for p in parts if p.strip()])
                    chunk_metadata['delimiter_used'] = self.delimiter
                    
                    chunks.append(Document(
                        page_content=part,
                        metadata=chunk_metadata
                    ))
            
            return chunks if chunks else self._fallback_chunking(document)
        else:
            # êµ¬ë¶„ìê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì²­í‚¹ ì‚¬ìš©
            return self._fallback_chunking(document)
    
    def _fallback_chunking(self, document: Document) -> List[Document]:
        """í´ë°± ì²­í‚¹ (ê¸°ë³¸ ì „ëµ)"""
        chunks = self.fallback_splitter.split_documents([document])
        
        for chunk in chunks:
            chunk.metadata['chunking_strategy'] = 'basic_fallback'
            chunk.metadata['delimiter_used'] = 'none'
        
        return chunks

def get_chunking_strategy(strategy_name: str = 'basic'):
    """ì²­í‚¹ ì „ëµ íŒ©í† ë¦¬ í•¨ìˆ˜"""
    strategies = {
        'basic': BasicChunkingStrategy(),
        'semantic': SemanticChunkingStrategy(),
        'hybrid': HybridChunkingStrategy(),
        's3-basic': S3BasicChunkingStrategy(),
        's3-custom': S3CustomChunkingStrategy(),
        'custom_delimiter': CustomDelimiterChunkingStrategy()
    }
    
    return strategies.get(strategy_name, strategies['basic'])

def benchmark_chunking_strategies(documents: List[Document]) -> Dict:
    """ì²­í‚¹ ì „ëµ ë²¤ì¹˜ë§ˆí‚¹ ìˆ˜í–‰"""
    return chunking_benchmarker.compare_strategies(documents)