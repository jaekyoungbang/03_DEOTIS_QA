#!/usr/bin/env python3
"""
s3-chunking ì „ìš© MD íŒŒì¼ ë¡œë”
ìµœì í™”ëœ ì²­í‚¹ìœ¼ë¡œ MD íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ê³  ì´ë¯¸ì§€ ê²½ë¡œ ì •ë³´ë„ ë³´ì¡´
"""

import os
import sys
import json
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import hashlib

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models.embeddings import EmbeddingManager
from models.vectorstore import DualVectorStoreManager
from services.advanced_chunking_strategies import AdvancedChunkingStrategies
from langchain.schema import Document


class OptimizedMarkdownChunker:
    """ìµœì í™”ëœ ë§ˆí¬ë‹¤ìš´ ì²­í‚¹ í´ë˜ìŠ¤ - ê³ ê¸‰ ì²­í‚¹ ì „ëµ í¬í•¨"""
    
    def __init__(self, chunk_size_limit: int = 1500, chunk_overlap: int = 250, preserve_images: bool = True, use_advanced_chunking: bool = True):
        self.chunk_size_limit = chunk_size_limit
        self.chunk_overlap = chunk_overlap
        self.preserve_images = preserve_images
        self.use_advanced_chunking = use_advanced_chunking
        self.image_pattern = re.compile(r'!\[.*?\]\((.*?)\)')
        
        # ê³ ê¸‰ ì²­í‚¹ ì „ëµ ì´ˆê¸°í™”
        if use_advanced_chunking:
            self.advanced_chunker = AdvancedChunkingStrategies(
                base_chunk_size=chunk_size_limit,
                overlap_ratio=chunk_overlap / chunk_size_limit
            )
        
    def chunk_markdown_file(self, file_path: str, chunking_strategy: str = "hybrid") -> List[Document]:
        """MD íŒŒì¼ì„ ì²­í‚¹í•˜ì—¬ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            filename = os.path.basename(file_path)
            
            # ê³ ê¸‰ ì²­í‚¹ ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ì²˜ë¦¬ ë°©ì‹ ì„ íƒ
            if self.use_advanced_chunking and hasattr(self, 'advanced_chunker'):
                documents = self._apply_advanced_chunking(content, file_path, filename, chunking_strategy)
            else:
                # ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                sections = self._split_by_sections(content)
                documents = self._create_overlapped_documents(sections, file_path, filename)
            
            return documents
            
        except Exception as e:
            print(f"âŒ MD íŒŒì¼ ì²­í‚¹ ì‹¤íŒ¨ ({file_path}): {e}")
            return []
    
    def _apply_advanced_chunking(self, content: str, file_path: str, filename: str, chunking_strategy: str) -> List[Document]:
        """ê³ ê¸‰ ì²­í‚¹ ì „ëµ ì ìš©"""
        base_metadata = {
            'source': file_path,
            'filename': filename,
            'folder_type': 's3-chunking',
            'file_type': 'markdown',
            'processing_strategy': f'advanced_{chunking_strategy}_chunking'
        }
        
        # ì „ëµë³„ ì²­í‚¹ ì ìš©
        if chunking_strategy == "semantic":
            return self.advanced_chunker.semantic_chunking(content, base_metadata)
        elif chunking_strategy == "question_aware":
            return self.advanced_chunker.question_aware_chunking(content, base_metadata)
        elif chunking_strategy == "hierarchical":
            return self.advanced_chunker.hierarchical_chunking(content, base_metadata)
        elif chunking_strategy == "hybrid":
            return self.advanced_chunker.hybrid_chunking(content, base_metadata)
        else:
            # ê¸°ë³¸ì ìœ¼ë¡œ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ ì‚¬ìš©
            return self.advanced_chunker.hybrid_chunking(content, base_metadata)
    
    def _extract_images(self, content: str) -> List[Dict[str, str]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ"""
        images = []
        for match in self.image_pattern.finditer(content):
            image_path = match.group(1)
            # GIF í™•ì¥ìë¥¼ JPGë¡œ ì¹˜í™˜
            if image_path.endswith('.gif'):
                image_path = image_path.replace('.gif', '-0000.jpg')
            
            images.append({
                'path': image_path,
                'full_tag': match.group(0)
            })
        return images
    
    def _create_overlapped_documents(self, sections: List[Dict], file_path: str, filename: str) -> List[Document]:
        """ì¤‘ì²© ê¸°ëŠ¥ì„ í¬í•¨í•œ ë¬¸ì„œ ìƒì„±"""
        documents = []
        
        for idx, section in enumerate(sections):
            # í˜„ì¬ ì„¹ì…˜ì˜ ê¸°ë³¸ ë¬¸ì„œ
            current_content = section['content']
            images = self._extract_images(current_content)
            
            # ê¸°ë³¸ ë¬¸ì„œ ìƒì„±
            doc = Document(
                page_content=current_content,
                metadata={
                    'source': file_path,
                    'filename': filename,
                    'section': section['title'],
                    'chunk_id': str(idx),
                    'chunk_type': section['type'],
                    'images_count': len(images),
                    'has_images': len(images) > 0,
                    'folder_type': 's3-chunking',
                    'file_type': 'markdown',
                    'processing_strategy': 'optimized_md_chunking_with_overlap',
                    'is_overlap': False
                }
            )
            documents.append(doc)
            
            # ì¤‘ì²© ë¬¸ì„œ ìƒì„± (ë‹¤ìŒ ì„¹ì…˜ê³¼)
            if idx < len(sections) - 1 and len(current_content) > self.chunk_overlap:
                next_section = sections[idx + 1]
                
                # í˜„ì¬ ì„¹ì…˜ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ + ë‹¤ìŒ ì„¹ì…˜ì˜ ì‹œì‘ ë¶€ë¶„
                current_end = current_content[-self.chunk_overlap:]
                next_start = next_section['content'][:self.chunk_overlap]
                
                # ì„¹ì…˜ í—¤ë” ë³´ì¡´
                overlap_content = f"## {section['title']}\n...\n{current_end}\n\n## {next_section['title']}\n{next_start}"
                
                overlap_images = self._extract_images(overlap_content)
                
                overlap_doc = Document(
                    page_content=overlap_content,
                    metadata={
                        'source': file_path,
                        'filename': filename,
                        'section': f"{section['title']} â†’ {next_section['title']}",
                        'chunk_id': f"{idx}_overlap_{idx+1}",
                        'chunk_type': 'overlap',
                        'images_count': len(overlap_images),
                        'has_images': len(overlap_images) > 0,
                        'folder_type': 's3-chunking',
                        'file_type': 'markdown',
                        'processing_strategy': 'optimized_md_chunking_with_overlap',
                        'is_overlap': True
                    }
                )
                documents.append(overlap_doc)
        
        return documents
    
    def _split_by_sections(self, content: str) -> List[Dict]:
        """ë§ˆí¬ë‹¤ìš´ì„ ì„¹ì…˜ë³„ë¡œ ë¶„í• """
        sections = []
        lines = content.split('\n')
        
        current_section = {"title": "", "content": "", "type": "text"}
        in_table = False
        table_buffer = []
        
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # ì£¼ìš” ì„¹ì…˜ í—¤ë”© ê°ì§€
            if re.match(r'^#{1,3}\s+', line_stripped):
                # ì´ì „ ì„¹ì…˜ ì €ì¥
                if current_section['content'].strip():
                    sections.append(current_section)
                
                # ìƒˆ ì„¹ì…˜ ì‹œì‘
                current_section = {
                    "title": line_stripped.replace('#', '').strip(),
                    "content": line + '\n',
                    "type": "section"
                }
            
            # í…Œì´ë¸” ê°ì§€ ë° ì²˜ë¦¬
            elif '|' in line and line.count('|') >= 2:
                if not in_table:
                    # í…Œì´ë¸” ì‹œì‘ - í˜„ì¬ ì„¹ì…˜ ì €ì¥
                    if current_section['content'].strip() and not re.match(r'^#{1,3}\s+', current_section['content'].strip()):
                        sections.append(current_section)
                    
                    # í…Œì´ë¸” ë²„í¼ ì‹œì‘
                    table_buffer = [line]
                    in_table = True
                else:
                    table_buffer.append(line)
            
            # í…Œì´ë¸” ì¢…ë£Œ ê°ì§€
            elif in_table and (line_stripped == "" or not '|' in line):
                if table_buffer:
                    # í…Œì´ë¸”ì„ ë³„ë„ ì„¹ì…˜ìœ¼ë¡œ ì €ì¥
                    table_content = '\n'.join(table_buffer)
                    table_section = {
                        "title": current_section.get("title", "í…Œì´ë¸”"),
                        "content": table_content,
                        "type": "table"
                    }
                    sections.append(table_section)
                    table_buffer = []
                
                in_table = False
                
                # ìƒˆ ì„¹ì…˜ ì‹œì‘
                if line_stripped:
                    current_section = {
                        "title": current_section.get("title", ""),
                        "content": line + '\n',
                        "type": "text"
                    }
            
            else:
                if not in_table:
                    current_section['content'] += line + '\n'
                    
                    # ì²­í¬ í¬ê¸° ì œí•œ í™•ì¸
                    if len(current_section['content']) > self.chunk_size_limit:
                        sections.append(current_section)
                        current_section = {
                            "title": current_section.get("title", ""),
                            "content": "",
                            "type": "text"
                        }
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì²˜ë¦¬
        if in_table and table_buffer:
            table_content = '\n'.join(table_buffer)
            sections.append({
                "title": current_section.get("title", "í…Œì´ë¸”"),
                "content": table_content,
                "type": "table"
            })
        
        if current_section['content'].strip():
            sections.append(current_section)
        
        return sections


class S3ChunkingMDLoader:
    """s3-chunking í´ë”ì˜ MD íŒŒì¼ ì „ìš© ë¡œë” - ê³ ê¸‰ ì²­í‚¹ ì „ëµ í¬í•¨"""
    
    def __init__(self, use_advanced_chunking: bool = True, default_chunking_strategy: str = "hybrid"):
        # ê³ ê¸‰ ì²­í‚¹ ì „ëµì„ í¬í•¨í•œ ì²­í‚¹ ì„¤ì •
        self.chunker = OptimizedMarkdownChunker(
            chunk_size_limit=1500, 
            chunk_overlap=250,
            use_advanced_chunking=use_advanced_chunking
        )
        self.default_chunking_strategy = default_chunking_strategy
        self.embedding_manager = EmbeddingManager()
        self.vectorstore_manager = DualVectorStoreManager(self.embedding_manager.get_embeddings())
        
    def load_s3_chunking_md_files(self, clear_before_load: bool = False, chunking_strategy: str = None):
        """s3-chunking í´ë”ì˜ MD íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ê³  ê³ ê¸‰ ì²­í‚¹ ì „ëµ ì ìš©"""
        
        strategy = chunking_strategy or self.default_chunking_strategy
        
        print("ğŸš€ s3-chunking MD íŒŒì¼ ë¡œë”© ì‹œì‘...")
        print(f"ğŸ“‹ ì²­í‚¹ ì „ëµ: {strategy}")
        print("=" * 60)
        
        # í´ë” ê²½ë¡œ ì„¤ì •
        import platform
        if platform.system() == "Windows" or os.name == "nt":
            s3_chunking_path = r"D:\99_DEOTIS_QA_SYSTEM\03_DEOTIS_QA\s3-chunking"
        else:
            s3_chunking_path = "/mnt/d/99_DEOTIS_QA_SYSTEM/03_DEOTIS_QA/s3-chunking"
        
        print(f"ğŸ“‚ ëŒ€ìƒ í´ë”: {s3_chunking_path}")
        
        # ê¸°ì¡´ custom ì»¬ë ‰ì…˜ ë°ì´í„° ì‚­ì œ (ì˜µì…˜)
        if clear_before_load:
            print("ğŸ—‘ï¸ ê¸°ì¡´ custom ì»¬ë ‰ì…˜ ë°ì´í„° ì‚­ì œ ì¤‘...")
            try:
                # custom ì»¬ë ‰ì…˜ë§Œ ì‚­ì œ
                self.vectorstore_manager.clear_collection("custom")
                print("âœ… custom ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # MD íŒŒì¼ ì°¾ê¸° - ëª¨ë“  MD íŒŒì¼ í¬í•¨
        md_files = []
        if os.path.exists(s3_chunking_path):
            for file in os.listdir(s3_chunking_path):
                if file.endswith('.md'):
                    md_files.append(os.path.join(s3_chunking_path, file))
        
        if not md_files:
            print("âš ï¸ ì²˜ë¦¬í•  MD íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ“‹ ë°œê²¬ëœ MD íŒŒì¼: {len(md_files)}ê°œ")
        for md_file in md_files:
            print(f"   - {os.path.basename(md_file)}")
        
        # ì „ì²´ ì²­í¬ ì €ì¥ìš©
        all_documents = []
        total_chunks = 0
        files_processed = 0
        
        # ê° MD íŒŒì¼ ì²˜ë¦¬
        for md_file in md_files:
            print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {os.path.basename(md_file)}")
            
            try:
                # MD íŒŒì¼ ì²­í‚¹ (ì„ íƒëœ ì „ëµ ì ìš©)
                documents = self.chunker.chunk_markdown_file(md_file, strategy)
                
                if not documents:
                    print("   âš ï¸ ë¬¸ì„œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    continue
                
                # í†µê³„ ì •ë³´ ìˆ˜ì§‘
                chunk_types = {}
                chunk_strategies = {}
                image_chunks = 0
                table_chunks = 0
                quality_scores = []
                importance_scores = []
                
                for doc in documents:
                    chunk_type = doc.metadata.get('chunk_type', 'text')
                    chunk_strategy = doc.metadata.get('chunking_strategy', 'legacy')
                    
                    chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
                    chunk_strategies[chunk_strategy] = chunk_strategies.get(chunk_strategy, 0) + 1
                    
                    if doc.metadata.get('has_images', False):
                        image_chunks += 1
                    if chunk_type == 'table':
                        table_chunks += 1
                    
                    # ê³ ê¸‰ ì²­í‚¹ ë©”íŠ¸ë¦­
                    if 'chunk_quality' in doc.metadata:
                        quality_scores.append(doc.metadata['chunk_quality'])
                    if 'importance_score' in doc.metadata:
                        importance_scores.append(doc.metadata['importance_score'])
                
                print(f"   âœ… {len(documents)}ê°œ ì²­í¬ ìƒì„±")
                print(f"      - ì²­í‚¹ ì „ëµ: {chunk_strategies}")
                print(f"      - ì²­í¬ íƒ€ì…: {chunk_types}")
                print(f"      - ì´ë¯¸ì§€ í¬í•¨: {image_chunks}ê°œ")
                print(f"      - í…Œì´ë¸”: {table_chunks}ê°œ")
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­ í‘œì‹œ
                if quality_scores:
                    avg_quality = sum(quality_scores) / len(quality_scores)
                    print(f"      - í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality:.3f}")
                if importance_scores:
                    avg_importance = sum(importance_scores) / len(importance_scores)
                    print(f"      - í‰ê·  ì¤‘ìš”ë„ ì ìˆ˜: {avg_importance:.3f}")
                
                # ì¤‘ìš” í…Œì´ë¸” í™•ì¸
                for doc in documents:
                    if "ê²°ì œì¼ë³„ ì‹ ìš©ê³µì—¬ê¸°ê°„" in doc.page_content:
                        print(f"      ğŸŒŸ ê²°ì œì¼ë³„ ì‹ ìš©ê³µì—¬ê¸°ê°„ í…Œì´ë¸” ë°œê²¬!")
                    if "ì¥ì• ìœ í˜•ë³„ ë³¸ì¸í™•ì¸" in doc.page_content:
                        print(f"      ğŸŒŸ ì¥ì• ìœ í˜•ë³„ ë³¸ì¸í™•ì¸ í…Œì´ë¸” ë°œê²¬!")
                
                all_documents.extend(documents)
                files_processed += 1
                total_chunks += len(documents)
                
            except Exception as e:
                print(f"   âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
        
        # ë²¡í„° DBì— ì €ì¥
        if all_documents:
            print(f"\nğŸ’¾ ë²¡í„° DB ì €ì¥ ì‹œì‘...")
            print(f"   - ì´ ë¬¸ì„œ ìˆ˜: {files_processed}ê°œ")
            print(f"   - ì´ ì²­í¬ ìˆ˜: {total_chunks}ê°œ")
            
            try:
                # custom ì»¬ë ‰ì…˜ì— ì €ì¥
                self.vectorstore_manager.add_documents(all_documents, "custom")
                
                print(f"\nâœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ!")
                print(f"   - ì»¬ë ‰ì…˜: custom")
                print(f"   - ì €ì¥ëœ ì²­í¬: {len(all_documents)}ê°œ")
                
                # ì €ì¥ ê²°ê³¼ ê²€ì¦
                self._verify_storage()
                
            except Exception as e:
                print(f"âŒ ë²¡í„° DB ì €ì¥ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
        
        print("\nğŸ‰ s3-chunking MD íŒŒì¼ ë¡œë”© ì™„ë£Œ!")
    
    def _verify_storage(self):
        """ì €ì¥ëœ ë°ì´í„° ê²€ì¦"""
        try:
            print("\nğŸ” ì €ì¥ ë°ì´í„° ê²€ì¦...")
            
            # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
            test_queries = [
                "ê²°ì œì¼ë³„ ì‹ ìš©ê³µì—¬ê¸°ê°„",
                "1ì¼ ê²°ì œì¼",
                "ì¥ì• ìœ í˜•ë³„ ë³¸ì¸í™•ì¸",
                "í• ë¶€ìˆ˜ìˆ˜ë£Œìœ¨"
            ]
            
            for query in test_queries:
                results = self.vectorstore_manager.similarity_search_with_score(
                    query, "custom", k=3
                )
                
                if results:
                    print(f"\n   âœ… '{query}' ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
                    for doc, score in results[:1]:  # ìƒìœ„ 1ê°œë§Œ í‘œì‹œ
                        print(f"      - Score: {score:.4f}")
                        print(f"      - Content: {doc.page_content[:100]}...")
                        if doc.metadata.get('has_images'):
                            print(f"      - ì´ë¯¸ì§€ í¬í•¨: {doc.metadata.get('images_count', 0)}ê°œ")
                else:
                    print(f"   âš ï¸ '{query}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                    
        except Exception as e:
            print(f"âš ï¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ê³ ê¸‰ ì²­í‚¹ ì „ëµ ì„ íƒ:")
    print("1. semantic - ì˜ë¯¸ë¡ ì  ì²­í‚¹ (ë¬¸ë§¥ ì¤‘ì‹¬)")
    print("2. question_aware - ì§ˆë¬¸ ìœ í˜•ë³„ ë§ì¶¤ ì²­í‚¹")
    print("3. hierarchical - ê³„ì¸µì  ë¬¸ì„œ êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹")
    print("4. hybrid - í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹ (ì¶”ì²œ)")
    print("5. legacy - ê¸°ì¡´ ì²­í‚¹ ë°©ì‹")
    
    strategy_choice = input("\nì „ëµì„ ì„ íƒí•˜ì„¸ìš” (1-5, ê¸°ë³¸ê°’: 4): ").strip()
    
    strategy_map = {
        "1": "semantic",
        "2": "question_aware", 
        "3": "hierarchical",
        "4": "hybrid",
        "5": "legacy"
    }
    
    selected_strategy = strategy_map.get(strategy_choice, "hybrid")
    use_advanced = selected_strategy != "legacy"
    
    print(f"\nâœ… ì„ íƒëœ ì „ëµ: {selected_strategy}")
    print("=" * 60)
    
    # ì„ íƒëœ ì „ëµìœ¼ë¡œ ë¡œë” ì´ˆê¸°í™”
    loader = S3ChunkingMDLoader(
        use_advanced_chunking=use_advanced,
        default_chunking_strategy=selected_strategy
    )
    
    # clear_before_load=Trueë¡œ ê¸°ì¡´ custom ì»¬ë ‰ì…˜ ë°ì´í„° ì‚­ì œ í›„ ë¡œë“œ
    loader.load_s3_chunking_md_files(
        clear_before_load=True,
        chunking_strategy=selected_strategy
    )
    
    print("\nğŸ’¡ ì‚¬ìš© ë°©ë²•:")
    print("   - ëª¨ë“œ 2: ì‚¬ë‚´ì„œë²„ vLLM + s3-chunking")
    print("   - ëª¨ë“œ 4: ChatGPT + s3-chunking")
    print(f"   â¡ï¸ {selected_strategy} ì²­í‚¹ìœ¼ë¡œ ë” ì •í™•í•œ ê²€ìƒ‰ ê°€ëŠ¥")
    print("   â¡ï¸ ì´ë¯¸ì§€ ê²½ë¡œ ì •ë³´ë„ í•¨ê»˜ ì „ë‹¬")
    
    if selected_strategy != "legacy":
        print(f"\nğŸ” {selected_strategy} ì²­í‚¹ ì „ëµì˜ íŠ¹ì§•:")
        strategy_descriptions = {
            "semantic": "ë¬¸ë§¥ê³¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì •ë³´ì˜ ì™„ì „ì„± ë³´ì¥",
            "question_aware": "ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ íŒ¨í„´ì— ìµœì í™”ëœ ì²­í‚¹",
            "hierarchical": "ë¬¸ì„œ ê³„ì¸µ êµ¬ì¡°ë¥¼ í™œìš©í•œ ë‹¤ì¸µ ì²­í‚¹",
            "hybrid": "ëª¨ë“  ì „ëµì„ ì¡°í•©í•œ ìµœì ì˜ ì²­í‚¹ (ì¤‘ë³µ ì œê±° í¬í•¨)"
        }
        print(f"   ğŸ“‹ {strategy_descriptions.get(selected_strategy, '')}")


if __name__ == "__main__":
    main()