#!/usr/bin/env python3
"""
s3-chunking ì „ìš© MD íŒŒì¼ ë¡œë”
ìµœì í™”ëœ ì²­í‚¹ìœ¼ë¡œ MD íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ê³  ì´ë¯¸ì§€ ê²½ë¡œ ì •ë³´ë„ ë³´ì¡´
"""

import os
import sys
import json
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import hashlib

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models.embeddings import EmbeddingManager
from models.vectorstore import DualVectorStoreManager
from langchain.schema import Document


class OptimizedMarkdownChunker:
    """ìµœì í™”ëœ ë§ˆí¬ë‹¤ìš´ ì²­í‚¹ í´ë˜ìŠ¤"""
    
    def __init__(self, chunk_size_limit: int = 1500, chunk_overlap: int = 250, preserve_images: bool = True):
        self.chunk_size_limit = chunk_size_limit
        self.chunk_overlap = chunk_overlap
        self.preserve_images = preserve_images
        self.image_pattern = re.compile(r'!\[.*?\]\((.*?)\)')
        
    def chunk_markdown_file(self, file_path: str) -> List[Document]:
        """MD íŒŒì¼ì„ ì²­í‚¹í•˜ì—¬ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            filename = os.path.basename(file_path)
            sections = self._split_by_sections(content)
            
            # ì¤‘ì²© ì²˜ë¦¬ëœ ë¬¸ì„œ ìƒì„±
            documents = self._create_overlapped_documents(sections, file_path, filename)
            
            return documents
            
        except Exception as e:
            print(f"âŒ MD íŒŒì¼ ì²­í‚¹ ì‹¤íŒ¨ ({file_path}): {e}")
            return []
    
    def _extract_images(self, content: str) -> List[Dict[str, str]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ"""
        images = []
        for match in self.image_pattern.finditer(content):
            image_path = match.group(1)
            images.append({
                'path': image_path,
                'full_tag': match.group(0)
            })
        return images
    
    def _create_overlapped_documents(self, sections: List[Dict], file_path: str, filename: str) -> List[Document]:
        """ì¤‘ì²© ê¸°ëŠ¥ì„ í¬í•¨í•œ ë¬¸ì„œ ìƒì„±"""
        documents = []
        
        for idx, section in enumerate(sections):
            # í˜„ì¬ ì„¹ì…˜ì˜ ê¸°ë³¸ ë¬¸ì„œ
            current_content = section['content']
            images = self._extract_images(current_content)
            
            # ê¸°ë³¸ ë¬¸ì„œ ìƒì„±
            doc = Document(
                page_content=current_content,
                metadata={
                    'source': file_path,
                    'filename': filename,
                    'section': section['title'],
                    'chunk_id': idx,
                    'chunk_type': section['type'],
                    'images': images,
                    'has_images': len(images) > 0,
                    'folder_type': 's3-chunking',
                    'file_type': 'markdown',
                    'processing_strategy': 'optimized_md_chunking_with_overlap',
                    'is_overlap': False
                }
            )
            documents.append(doc)
            
            # ì¤‘ì²© ë¬¸ì„œ ìƒì„± (ë‹¤ìŒ ì„¹ì…˜ê³¼)
            if idx < len(sections) - 1 and len(current_content) > self.chunk_overlap:
                next_section = sections[idx + 1]
                
                # í˜„ì¬ ì„¹ì…˜ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ + ë‹¤ìŒ ì„¹ì…˜ì˜ ì‹œì‘ ë¶€ë¶„
                current_end = current_content[-self.chunk_overlap:]
                next_start = next_section['content'][:self.chunk_overlap]
                
                # ì„¹ì…˜ í—¤ë” ë³´ì¡´
                overlap_content = f"## {section['title']}\n...\n{current_end}\n\n## {next_section['title']}\n{next_start}"
                
                overlap_images = self._extract_images(overlap_content)
                
                overlap_doc = Document(
                    page_content=overlap_content,
                    metadata={
                        'source': file_path,
                        'filename': filename,
                        'section': f"{section['title']} â†’ {next_section['title']}",
                        'chunk_id': f"{idx}_overlap_{idx+1}",
                        'chunk_type': 'overlap',
                        'images': overlap_images,
                        'has_images': len(overlap_images) > 0,
                        'folder_type': 's3-chunking',
                        'file_type': 'markdown',
                        'processing_strategy': 'optimized_md_chunking_with_overlap',
                        'is_overlap': True,
                        'overlap_sections': [section['title'], next_section['title']]
                    }
                )
                documents.append(overlap_doc)
        
        return documents
    
    def _split_by_sections(self, content: str) -> List[Dict]:
        """ë§ˆí¬ë‹¤ìš´ì„ ì„¹ì…˜ë³„ë¡œ ë¶„í• """
        sections = []
        lines = content.split('\n')
        
        current_section = {"title": "", "content": "", "type": "text"}
        in_table = False
        table_buffer = []
        
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # ì£¼ìš” ì„¹ì…˜ í—¤ë”© ê°ì§€
            if re.match(r'^#{1,3}\s+', line_stripped):
                # ì´ì „ ì„¹ì…˜ ì €ì¥
                if current_section['content'].strip():
                    sections.append(current_section)
                
                # ìƒˆ ì„¹ì…˜ ì‹œì‘
                current_section = {
                    "title": line_stripped.replace('#', '').strip(),
                    "content": line + '\n',
                    "type": "section"
                }
            
            # í…Œì´ë¸” ê°ì§€ ë° ì²˜ë¦¬
            elif '|' in line and line.count('|') >= 2:
                if not in_table:
                    # í…Œì´ë¸” ì‹œì‘ - í˜„ì¬ ì„¹ì…˜ ì €ì¥
                    if current_section['content'].strip() and not re.match(r'^#{1,3}\s+', current_section['content'].strip()):
                        sections.append(current_section)
                    
                    # í…Œì´ë¸” ë²„í¼ ì‹œì‘
                    table_buffer = [line]
                    in_table = True
                else:
                    table_buffer.append(line)
            
            # í…Œì´ë¸” ì¢…ë£Œ ê°ì§€
            elif in_table and (line_stripped == "" or not '|' in line):
                if table_buffer:
                    # í…Œì´ë¸”ì„ ë³„ë„ ì„¹ì…˜ìœ¼ë¡œ ì €ì¥
                    table_content = '\n'.join(table_buffer)
                    table_section = {
                        "title": current_section.get("title", "í…Œì´ë¸”"),
                        "content": table_content,
                        "type": "table"
                    }
                    sections.append(table_section)
                    table_buffer = []
                
                in_table = False
                
                # ìƒˆ ì„¹ì…˜ ì‹œì‘
                if line_stripped:
                    current_section = {
                        "title": current_section.get("title", ""),
                        "content": line + '\n',
                        "type": "text"
                    }
            
            else:
                if not in_table:
                    current_section['content'] += line + '\n'
                    
                    # ì²­í¬ í¬ê¸° ì œí•œ í™•ì¸
                    if len(current_section['content']) > self.chunk_size_limit:
                        sections.append(current_section)
                        current_section = {
                            "title": current_section.get("title", ""),
                            "content": "",
                            "type": "text"
                        }
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì²˜ë¦¬
        if in_table and table_buffer:
            table_content = '\n'.join(table_buffer)
            sections.append({
                "title": current_section.get("title", "í…Œì´ë¸”"),
                "content": table_content,
                "type": "table"
            })
        
        if current_section['content'].strip():
            sections.append(current_section)
        
        return sections


class S3ChunkingMDLoader:
    """s3-chunking í´ë”ì˜ MD íŒŒì¼ ì „ìš© ë¡œë”"""
    
    def __init__(self):
        # ì¤‘ì²© ê¸°ëŠ¥ì„ í¬í•¨í•œ ì²­í‚¹ ì„¤ì •: 1500ì ì œí•œ, 250ì ì¤‘ì²©
        self.chunker = OptimizedMarkdownChunker(chunk_size_limit=1500, chunk_overlap=250)
        self.embedding_manager = EmbeddingManager()
        self.vectorstore_manager = DualVectorStoreManager(self.embedding_manager.get_embeddings())
        
    def load_s3_chunking_md_files(self, clear_before_load: bool = False):
        """s3-chunking í´ë”ì˜ MD íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ê³  ìµœì í™”ëœ ì²­í‚¹ ì ìš©"""
        
        print("ğŸš€ s3-chunking MD íŒŒì¼ ë¡œë”© ì‹œì‘...")
        print("=" * 60)
        
        # í´ë” ê²½ë¡œ ì„¤ì •
        import platform
        if platform.system() == "Windows" or os.name == "nt":
            s3_chunking_path = r"D:\99_DEOTIS_QA_SYSTEM\03_DEOTIS_QA\s3-chunking"
        else:
            s3_chunking_path = "/mnt/d/99_DEOTIS_QA_SYSTEM/03_DEOTIS_QA/s3-chunking"
        
        print(f"ğŸ“‚ ëŒ€ìƒ í´ë”: {s3_chunking_path}")
        
        # ê¸°ì¡´ custom ì»¬ë ‰ì…˜ ë°ì´í„° ì‚­ì œ (ì˜µì…˜)
        if clear_before_load:
            print("ğŸ—‘ï¸ ê¸°ì¡´ custom ì»¬ë ‰ì…˜ ë°ì´í„° ì‚­ì œ ì¤‘...")
            try:
                # custom ì»¬ë ‰ì…˜ë§Œ ì‚­ì œ
                self.vectorstore_manager.clear_collection("custom")
                print("âœ… custom ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # MD íŒŒì¼ ì°¾ê¸°
        md_files = []
        if os.path.exists(s3_chunking_path):
            for file in os.listdir(s3_chunking_path):
                if file.endswith('.md') and ('ì™„ì „íŒ' in file or 'ìµœì í™”' in file):
                    md_files.append(os.path.join(s3_chunking_path, file))
        
        if not md_files:
            print("âš ï¸ ì²˜ë¦¬í•  MD íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ“‹ ë°œê²¬ëœ MD íŒŒì¼: {len(md_files)}ê°œ")
        for md_file in md_files:
            print(f"   - {os.path.basename(md_file)}")
        
        # ì „ì²´ ì²­í¬ ì €ì¥ìš©
        all_documents = []
        total_chunks = 0
        files_processed = 0
        
        # ê° MD íŒŒì¼ ì²˜ë¦¬
        for md_file in md_files:
            print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {os.path.basename(md_file)}")
            
            try:
                # MD íŒŒì¼ ì²­í‚¹
                documents = self.chunker.chunk_markdown_file(md_file)
                
                if not documents:
                    print("   âš ï¸ ë¬¸ì„œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    continue
                
                # í†µê³„ ì •ë³´ ìˆ˜ì§‘
                chunk_types = {}
                image_chunks = 0
                table_chunks = 0
                
                for doc in documents:
                    chunk_type = doc.metadata.get('chunk_type', 'text')
                    chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
                    
                    if doc.metadata.get('has_images', False):
                        image_chunks += 1
                    if chunk_type == 'table':
                        table_chunks += 1
                
                print(f"   âœ… {len(documents)}ê°œ ì²­í¬ ìƒì„±")
                print(f"      - ì²­í¬ íƒ€ì…: {chunk_types}")
                print(f"      - ì´ë¯¸ì§€ í¬í•¨: {image_chunks}ê°œ")
                print(f"      - í…Œì´ë¸”: {table_chunks}ê°œ")
                
                # ì¤‘ìš” í…Œì´ë¸” í™•ì¸
                for doc in documents:
                    if "ê²°ì œì¼ë³„ ì‹ ìš©ê³µì—¬ê¸°ê°„" in doc.page_content:
                        print(f"      ğŸŒŸ ê²°ì œì¼ë³„ ì‹ ìš©ê³µì—¬ê¸°ê°„ í…Œì´ë¸” ë°œê²¬!")
                    if "ì¥ì• ìœ í˜•ë³„ ë³¸ì¸í™•ì¸" in doc.page_content:
                        print(f"      ğŸŒŸ ì¥ì• ìœ í˜•ë³„ ë³¸ì¸í™•ì¸ í…Œì´ë¸” ë°œê²¬!")
                
                all_documents.extend(documents)
                files_processed += 1
                total_chunks += len(documents)
                
            except Exception as e:
                print(f"   âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
        
        # ë²¡í„° DBì— ì €ì¥
        if all_documents:
            print(f"\nğŸ’¾ ë²¡í„° DB ì €ì¥ ì‹œì‘...")
            print(f"   - ì´ ë¬¸ì„œ ìˆ˜: {files_processed}ê°œ")
            print(f"   - ì´ ì²­í¬ ìˆ˜: {total_chunks}ê°œ")
            
            try:
                # custom ì»¬ë ‰ì…˜ì— ì €ì¥
                self.vectorstore_manager.add_documents(all_documents, "custom")
                
                print(f"\nâœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ!")
                print(f"   - ì»¬ë ‰ì…˜: custom")
                print(f"   - ì €ì¥ëœ ì²­í¬: {len(all_documents)}ê°œ")
                
                # ì €ì¥ ê²°ê³¼ ê²€ì¦
                self._verify_storage()
                
            except Exception as e:
                print(f"âŒ ë²¡í„° DB ì €ì¥ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
        
        print("\nğŸ‰ s3-chunking MD íŒŒì¼ ë¡œë”© ì™„ë£Œ!")
    
    def _verify_storage(self):
        """ì €ì¥ëœ ë°ì´í„° ê²€ì¦"""
        try:
            print("\nğŸ” ì €ì¥ ë°ì´í„° ê²€ì¦...")
            
            # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
            test_queries = [
                "ê²°ì œì¼ë³„ ì‹ ìš©ê³µì—¬ê¸°ê°„",
                "1ì¼ ê²°ì œì¼",
                "ì¥ì• ìœ í˜•ë³„ ë³¸ì¸í™•ì¸",
                "í• ë¶€ìˆ˜ìˆ˜ë£Œìœ¨"
            ]
            
            for query in test_queries:
                results = self.vectorstore_manager.similarity_search_with_score(
                    query, "custom", k=3
                )
                
                if results:
                    print(f"\n   âœ… '{query}' ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
                    for doc, score in results[:1]:  # ìƒìœ„ 1ê°œë§Œ í‘œì‹œ
                        print(f"      - Score: {score:.4f}")
                        print(f"      - Content: {doc.page_content[:100]}...")
                        if doc.metadata.get('has_images'):
                            print(f"      - ì´ë¯¸ì§€ í¬í•¨: {len(doc.metadata.get('images', []))}ê°œ")
                else:
                    print(f"   âš ï¸ '{query}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                    
        except Exception as e:
            print(f"âš ï¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    loader = S3ChunkingMDLoader()
    
    # clear_before_load=Trueë¡œ ê¸°ì¡´ custom ì»¬ë ‰ì…˜ ë°ì´í„° ì‚­ì œ í›„ ë¡œë“œ
    loader.load_s3_chunking_md_files(clear_before_load=True)
    
    print("\nğŸ’¡ ì‚¬ìš© ë°©ë²•:")
    print("   - ëª¨ë“œ 2: ì‚¬ë‚´ì„œë²„ vLLM + s3-chunking")
    print("   - ëª¨ë“œ 4: ChatGPT + s3-chunking")
    print("   â¡ï¸ ìµœì í™”ëœ MD ì²­í‚¹ìœ¼ë¡œ ë” ì •í™•í•œ ê²€ìƒ‰ ê°€ëŠ¥")
    print("   â¡ï¸ ì´ë¯¸ì§€ ê²½ë¡œ ì •ë³´ë„ í•¨ê»˜ ì „ë‹¬")


if __name__ == "__main__":
    main()