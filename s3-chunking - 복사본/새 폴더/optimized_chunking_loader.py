#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìµœì í™”ëœ ì²­í‚¹ ë¡œë” - s3-chunking ì „ìš©
MD íŒŒì¼ì„ ì„¸ë¶„í™”ëœ ì²­í‚¹ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë²¡í„° DBì— ì €ì¥
"""

import os
import sys
import json
import re
from datetime import datetime
from pathlib import Path

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ Python íŒ¨ìŠ¤ì— ì¶”ê°€
current_dir = Path(__file__).parent
parent_dir = current_dir.parent / "rag-qa-system"
sys.path.append(str(parent_dir))

from models.vectorstore import DualVectorStoreManager
from models.embeddings import EmbeddingManager

class OptimizedChunker:
    """ìµœì í™”ëœ ì²­í‚¹ ì „ëµ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.chunk_patterns = [
            # ì£¼ìš” ì„¹ì…˜ êµ¬ë¶„ì
            r'^#{1,3}\s+.*$',  # # ## ### í—¤ë”©
            r'^---+$',         # ê°€ë¡œì„  êµ¬ë¶„ì
            r'^\|.*\|$',       # í…Œì´ë¸” í–‰
            r'^\*\*.*\*\*$',   # ë³¼ë“œ ì œëª©
            r'^\d+[.)]',       # ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸
            r'^\s*[-*]\s+',    # ë¶ˆë › ë¦¬ìŠ¤íŠ¸
        ]
    
    def chunk_markdown_content(self, content: str, source_file: str = "optimized.md") -> list:
        """MD ë‚´ìš©ì„ ìµœì í™”ëœ ì²­í‚¹ìœ¼ë¡œ ë¶„í• """
        chunks = []
        lines = content.split('\n')
        
        current_chunk = []
        current_section = ""
        chunk_id = 0
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            # ì£¼ìš” ì„¹ì…˜ í—¤ë”© ê°ì§€ (# ## ###)
            if re.match(r'^#{1,3}\s+', line):
                # ì´ì „ ì²­í¬ ì €ì¥
                if current_chunk:
                    chunk_text = '\n'.join(current_chunk).strip()
                    if chunk_text:
                        chunks.append({
                            'content': chunk_text,
                            'metadata': {
                                'chunk_id': chunk_id,
                                'section': current_section,
                                'source_file': source_file,
                                'chunk_type': 'section',
                                'length': len(chunk_text)
                            }
                        })
                        chunk_id += 1
                
                # ìƒˆë¡œìš´ ì„¹ì…˜ ì‹œì‘
                current_section = line.replace('#', '').strip()
                current_chunk = [line]
            
            # í…Œì´ë¸” ê°ì§€ ë° ì²˜ë¦¬
            elif re.match(r'^\|.*\|', line):
                table_chunk = self._extract_table(lines, i)
                if table_chunk['content']:
                    table_chunk['metadata'].update({
                        'chunk_id': chunk_id,
                        'section': current_section,
                        'source_file': source_file,
                        'chunk_type': 'table'
                    })
                    chunks.append(table_chunk)
                    chunk_id += 1
                    
                # í…Œì´ë¸” ëê¹Œì§€ ì¸ë±ìŠ¤ ì´ë™
                i = table_chunk['end_index']
                continue
            
            # ê°€ë¡œì„  êµ¬ë¶„ì (---)
            elif re.match(r'^---+$', line):
                # ì´ì „ ì²­í¬ ì €ì¥
                if current_chunk and len(current_chunk) > 1:  # í—¤ë”©ë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
                    chunk_text = '\n'.join(current_chunk).strip()
                    if chunk_text:
                        chunks.append({
                            'content': chunk_text,
                            'metadata': {
                                'chunk_id': chunk_id,
                                'section': current_section,
                                'source_file': source_file,
                                'chunk_type': 'section',
                                'length': len(chunk_text)
                            }
                        })
                        chunk_id += 1
                current_chunk = []
            
            # ì¼ë°˜ í…ìŠ¤íŠ¸
            else:
                current_chunk.append(line)
                
                # ì²­í¬ í¬ê¸° ì œí•œ (2000ì)
                if len('\n'.join(current_chunk)) > 2000:
                    chunk_text = '\n'.join(current_chunk).strip()
                    if chunk_text:
                        chunks.append({
                            'content': chunk_text,
                            'metadata': {
                                'chunk_id': chunk_id,
                                'section': current_section,
                                'source_file': source_file,
                                'chunk_type': 'content',
                                'length': len(chunk_text)
                            }
                        })
                        chunk_id += 1
                    current_chunk = []
            
            i += 1
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk:
            chunk_text = '\n'.join(current_chunk).strip()
            if chunk_text:
                chunks.append({
                    'content': chunk_text,
                    'metadata': {
                        'chunk_id': chunk_id,
                        'section': current_section,
                        'source_file': source_file,
                        'chunk_type': 'content',
                        'length': len(chunk_text)
                    }
                })
        
        return chunks
    
    def _extract_table(self, lines: list, start_index: int) -> dict:
        """í…Œì´ë¸” ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ì¶”ì¶œ"""
        table_lines = []
        i = start_index
        
        # í…Œì´ë¸” í—¤ë” ë° êµ¬ë¶„ì ì°¾ê¸°
        while i < len(lines) and (re.match(r'^\|.*\|', lines[i]) or re.match(r'^[-:|]+$', lines[i])):
            table_lines.append(lines[i])
            i += 1
        
        # í…Œì´ë¸” ë°ì´í„° ê³„ì† ì½ê¸°
        while i < len(lines) and re.match(r'^\|.*\|', lines[i]):
            table_lines.append(lines[i])
            i += 1
        
        table_content = '\n'.join(table_lines).strip()
        
        return {
            'content': table_content,
            'metadata': {
                'chunk_type': 'table',
                'length': len(table_content),
                'table_rows': len([line for line in table_lines if '|' in line])
            },
            'end_index': i - 1
        }

class OptimizedChunkingLoader:
    """ìµœì í™”ëœ ì²­í‚¹ ë¡œë”"""
    
    def __init__(self):
        self.chunker = OptimizedChunker()
        self.embedding_manager = EmbeddingManager()
        self.vectorstore_manager = DualVectorStoreManager()
        
        print("ğŸš€ ìµœì í™”ëœ ì²­í‚¹ ë¡œë” ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - ì„ë² ë”©: {self.embedding_manager.model_name}")
        print(f"   - ë²¡í„° DB: DualVectorStoreManager")
    
    def load_markdown_file(self, md_file_path: str) -> dict:
        """MD íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì²­í‚¹ ì²˜ë¦¬"""
        try:
            print(f"\nğŸ“– MD íŒŒì¼ ë¡œë“œ ì¤‘: {md_file_path}")
            
            with open(md_file_path, 'r', encoding='utf-8') as file:
                content = file.read()
            
            filename = os.path.basename(md_file_path)
            chunks = self.chunker.chunk_markdown_content(content, filename)
            
            print(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            
            # ì²­í¬ ì •ë³´ ì¶œë ¥
            for i, chunk in enumerate(chunks):
                metadata = chunk['metadata']
                print(f"   ì²­í¬ {i}: [{metadata['chunk_type']}] {metadata['section'][:30]}... ({metadata['length']}ì)")
            
            return {
                'source_file': filename,
                'total_chunks': len(chunks),
                'chunks': chunks,
                'processing_time': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"âŒ MD íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def store_to_vectordb(self, chunk_data: dict) -> bool:
        """ì²­í¬ ë°ì´í„°ë¥¼ ë²¡í„° DBì— ì €ì¥"""
        try:
            print(f"\nğŸ’¾ ë²¡í„° DB ì €ì¥ ì‹œì‘: {chunk_data['source_file']}")
            
            documents = []
            metadatas = []
            
            for chunk in chunk_data['chunks']:
                documents.append(chunk['content'])
                
                metadata = chunk['metadata'].copy()
                metadata.update({
                    'chunking_strategy': 'optimized_markdown',
                    'processing_time': chunk_data['processing_time'],
                    'file_type': 'markdown'
                })
                metadatas.append(metadata)
            
            # custom ì»¬ë ‰ì…˜ì— ì €ì¥ (s3-chunkingìš©)
            collection_name = "custom"
            
            # ê¸°ì¡´ ë™ì¼ íŒŒì¼ ì‚­ì œ
            self._clear_existing_file_chunks(chunk_data['source_file'], collection_name)
            
            # ë²¡í„° ì„ë² ë”© ìƒì„±
            print("ğŸ”„ ì„ë² ë”© ë²¡í„° ìƒì„± ì¤‘...")
            embeddings = self.embedding_manager.embed_documents(documents)
            
            # ë²¡í„° DBì— ì €ì¥
            print("ğŸ“ ë²¡í„° DB ì €ì¥ ì¤‘...")
            success = self.vectorstore_manager.add_documents_to_collection(
                documents=documents,
                metadatas=metadatas,
                embeddings=embeddings,
                collection_name=collection_name
            )
            
            if success:
                print(f"âœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ: {len(documents)}ê°œ ì²­í¬")
                return True
            else:
                print("âŒ ë²¡í„° DB ì €ì¥ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"âŒ ë²¡í„° DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False
    
    def _clear_existing_file_chunks(self, filename: str, collection_name: str):
        """ê¸°ì¡´ íŒŒì¼ì˜ ì²­í¬ë“¤ ì‚­ì œ"""
        try:
            print(f"ğŸ—‘ï¸ ê¸°ì¡´ íŒŒì¼ ì²­í¬ ì‚­ì œ ì¤‘: {filename}")
            # ì—¬ê¸°ì— ê¸°ì¡´ íŒŒì¼ ì‚­ì œ ë¡œì§ êµ¬í˜„ (ì„ íƒì‚¬í•­)
            # self.vectorstore_manager.delete_by_metadata({"source_file": filename}, collection_name)
            pass
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ íŒŒì¼ ì²­í¬ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def save_chunk_analysis(self, chunk_data: dict, output_path: str):
        """ì²­í‚¹ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        try:
            analysis = {
                'source_file': chunk_data['source_file'],
                'total_chunks': chunk_data['total_chunks'],
                'processing_time': chunk_data['processing_time'],
                'chunk_summary': [],
                'chunk_statistics': {}
            }
            
            # ì²­í¬ë³„ ìš”ì•½
            chunk_types = {}
            total_length = 0
            
            for chunk in chunk_data['chunks']:
                metadata = chunk['metadata']
                chunk_type = metadata['chunk_type']
                chunk_length = metadata['length']
                
                analysis['chunk_summary'].append({
                    'chunk_id': metadata['chunk_id'],
                    'section': metadata['section'],
                    'type': chunk_type,
                    'length': chunk_length,
                    'preview': chunk['content'][:100] + '...' if len(chunk['content']) > 100 else chunk['content']
                })
                
                # í†µê³„
                chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
                total_length += chunk_length
            
            analysis['chunk_statistics'] = {
                'chunk_types': chunk_types,
                'average_length': total_length // len(chunk_data['chunks']) if chunk_data['chunks'] else 0,
                'total_length': total_length
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(analysis, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“Š ì²­í‚¹ ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_path}")
            
        except Exception as e:
            print(f"âŒ ì²­í‚¹ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ìµœì í™”ëœ ì²­í‚¹ ë¡œë” ì‹œì‘")
    print("=" * 60)
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    current_dir = Path(__file__).parent
    md_file = current_dir / "BCì¹´ë“œ_ì¹´ë“œì´ìš©ì•ˆë‚´_ìµœì í™”.md"
    output_dir = current_dir / "optimized_chunks"
    output_dir.mkdir(exist_ok=True)
    
    if not md_file.exists():
        print(f"âŒ MD íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {md_file}")
        return
    
    try:
        # ë¡œë” ì´ˆê¸°í™”
        loader = OptimizedChunkingLoader()
        
        # MD íŒŒì¼ ë¡œë“œ ë° ì²­í‚¹
        chunk_data = loader.load_markdown_file(str(md_file))
        
        # ì²­í‚¹ ë¶„ì„ ê²°ê³¼ ì €ì¥
        analysis_file = output_dir / "chunk_analysis.json"
        loader.save_chunk_analysis(chunk_data, str(analysis_file))
        
        # ë²¡í„° DBì— ì €ì¥
        success = loader.store_to_vectordb(chunk_data)
        
        if success:
            print(f"\nğŸ‰ ìµœì í™”ëœ ì²­í‚¹ ì²˜ë¦¬ ì™„ë£Œ!")
            print(f"   - ì´ ì²­í¬ ìˆ˜: {chunk_data['total_chunks']}ê°œ")
            print(f"   - ë¶„ì„ ê²°ê³¼: {analysis_file}")
            print(f"   - ë²¡í„° DB: custom ì»¬ë ‰ì…˜ì— ì €ì¥ë¨")
        else:
            print(f"\nâŒ ë²¡í„° DB ì €ì¥ ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"\nğŸ’¥ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()