#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© MD ì²­í‚¹ ë¡œë” - s3-chunking ì „ìš©
BCì¹´ë“œ 2ê°œ MD íŒŒì¼ì„ ìµœì í™”ëœ ì²­í‚¹ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë²¡í„° DBì— ì €ì¥
"""

import os
import sys
import json
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ Python íŒ¨ìŠ¤ì— ì¶”ê°€
current_dir = Path(__file__).parent
rag_system_dir = current_dir.parent / "rag-qa-system"
sys.path.append(str(rag_system_dir))

try:
    from models.vectorstore import DualVectorStoreManager
    from models.embeddings import EmbeddingManager
except ImportError:
    print("âš ï¸ Import ì‹¤íŒ¨: rag-qa-system ëª¨ë“ˆë“¤")
    print("   í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰í•´ì£¼ì„¸ìš”: cd /mnt/d/99_DEOTIS_QA_SYSTEM/03_DEOTIS_QA/s3-chunking")

class OptimizedMarkdownChunker:
    """ìµœì í™”ëœ ë§ˆí¬ë‹¤ìš´ ì²­í‚¹ í´ë˜ìŠ¤"""
    
    def __init__(self, chunk_size_limit: int = 2000):
        self.chunk_size_limit = chunk_size_limit
        
    def chunk_markdown_file(self, md_file_path: str) -> Dict:
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì½ì–´ì„œ ìµœì í™”ëœ ì²­í‚¹ ìˆ˜í–‰"""
        try:
            with open(md_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            filename = os.path.basename(md_file_path)
            chunks = self._chunk_by_sections(content, filename)
            
            return {
                'source_file': filename,
                'file_path': md_file_path,
                'total_chunks': len(chunks),
                'chunks': chunks,
                'processing_time': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"âŒ MD íŒŒì¼ ì²­í‚¹ ì‹¤íŒ¨ ({md_file_path}): {e}")
            return None
    
    def _chunk_by_sections(self, content: str, filename: str) -> List[Dict]:
        """ì„¹ì…˜ë³„ ì²­í‚¹ ìˆ˜í–‰"""
        chunks = []
        lines = content.split('\n')
        
        current_chunk_lines = []
        current_section = ""
        chunk_id = 0
        in_table = False
        table_buffer = []
        
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # ì£¼ìš” ì„¹ì…˜ í—¤ë”© ê°ì§€ (##, ###)
            if re.match(r'^#{2,3}\s+', line_stripped):
                # ì´ì „ ì²­í¬ ì™„ë£Œ ì²˜ë¦¬
                if current_chunk_lines:
                    chunk = self._create_chunk(current_chunk_lines, current_section, filename, chunk_id)
                    if chunk:
                        chunks.append(chunk)
                        chunk_id += 1
                
                # ìƒˆ ì„¹ì…˜ ì‹œì‘
                current_section = line_stripped.replace('#', '').strip()
                current_chunk_lines = [line]
                in_table = False
            
            # í…Œì´ë¸” ê°ì§€
            elif re.match(r'^[\|\s]*\|.*\|[\|\s]*$', line_stripped):
                if not in_table:
                    # í…Œì´ë¸” ì‹œì‘ - ì´ì „ ë‚´ìš© ì²­í¬ë¡œ ì €ì¥
                    if current_chunk_lines and len(current_chunk_lines) > 1:
                        chunk = self._create_chunk(current_chunk_lines, current_section, filename, chunk_id, chunk_type='section')
                        if chunk:
                            chunks.append(chunk)
                            chunk_id += 1
                    
                    # í…Œì´ë¸” ì „ìš© ì²­í¬ ì‹œì‘
                    table_buffer = [line]
                    in_table = True
                else:
                    table_buffer.append(line)
            
            # êµ¬ë¶„ì„  (---)
            elif re.match(r'^---+$', line_stripped):
                if in_table and table_buffer:
                    # í…Œì´ë¸” ì²­í¬ ì™„ë£Œ
                    table_chunk = self._create_table_chunk(table_buffer, current_section, filename, chunk_id)
                    if table_chunk:
                        chunks.append(table_chunk)
                        chunk_id += 1
                    table_buffer = []
                    in_table = False
                
                # êµ¬ë¶„ì„ ìœ¼ë¡œ ì„¹ì…˜ êµ¬ë¶„
                if current_chunk_lines and len(current_chunk_lines) > 1:
                    chunk = self._create_chunk(current_chunk_lines, current_section, filename, chunk_id)
                    if chunk:
                        chunks.append(chunk)
                        chunk_id += 1
                
                current_chunk_lines = []
            
            else:
                if in_table:
                    # í…Œì´ë¸”ì´ ëë‚¬ì„ ê°€ëŠ¥ì„±
                    if line_stripped == "":
                        continue  # ë¹ˆ ì¤„ì€ ë¬´ì‹œ
                    else:
                        # í…Œì´ë¸” ì¢…ë£Œ
                        if table_buffer:
                            table_chunk = self._create_table_chunk(table_buffer, current_section, filename, chunk_id)
                            if table_chunk:
                                chunks.append(table_chunk)
                                chunk_id += 1
                            table_buffer = []
                        in_table = False
                        current_chunk_lines = [line]
                else:
                    current_chunk_lines.append(line)
                    
                    # ì²­í¬ í¬ê¸° ì œí•œ ì²´í¬
                    current_content = '\n'.join(current_chunk_lines)
                    if len(current_content) > self.chunk_size_limit:
                        chunk = self._create_chunk(current_chunk_lines, current_section, filename, chunk_id)
                        if chunk:
                            chunks.append(chunk)
                            chunk_id += 1
                        current_chunk_lines = []
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if in_table and table_buffer:
            table_chunk = self._create_table_chunk(table_buffer, current_section, filename, chunk_id)
            if table_chunk:
                chunks.append(table_chunk)
                chunk_id += 1
        
        if current_chunk_lines:
            chunk = self._create_chunk(current_chunk_lines, current_section, filename, chunk_id)
            if chunk:
                chunks.append(chunk)
        
        return chunks
    
    def _create_chunk(self, lines: List[str], section: str, filename: str, chunk_id: int, chunk_type: str = 'content') -> Optional[Dict]:
        """ì¼ë°˜ ì²­í¬ ìƒì„±"""
        content = '\n'.join(lines).strip()
        if not content or len(content) < 50:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸
            return None
        
        return {
            'content': content,
            'metadata': {
                'chunk_id': chunk_id,
                'section': section,
                'source_file': filename,
                'chunk_type': chunk_type,
                'length': len(content)
            }
        }
    
    def _create_table_chunk(self, table_lines: List[str], section: str, filename: str, chunk_id: int) -> Optional[Dict]:
        """í…Œì´ë¸” ì „ìš© ì²­í¬ ìƒì„±"""
        content = '\n'.join(table_lines).strip()
        if not content:
            return None
        
        # í…Œì´ë¸” í–‰ ìˆ˜ ê³„ì‚°
        table_rows = len([line for line in table_lines if '|' in line and not re.match(r'^[\s\-\|:]+$', line.strip())])
        
        return {
            'content': content,
            'metadata': {
                'chunk_id': chunk_id,
                'section': section,
                'source_file': filename,
                'chunk_type': 'table',
                'length': len(content),
                'table_rows': table_rows,
                'is_structured_data': True
            }
        }

class IntegratedMDChunkingLoader:
    """í†µí•© MD ì²­í‚¹ ë¡œë”"""
    
    def __init__(self):
        self.chunker = OptimizedMarkdownChunker()
        self.chunk_results = []
        
        # ë²¡í„°ìŠ¤í† ì–´ ë° ì„ë² ë”© ë§¤ë‹ˆì € ì´ˆê¸°í™”
        try:
            self.embedding_manager = EmbeddingManager()
            self.vectorstore_manager = DualVectorStoreManager()
            print("âœ… ë²¡í„° DB ë° ì„ë² ë”© ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ë²¡í„° DB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.embedding_manager = None
            self.vectorstore_manager = None
    
    def load_all_md_files(self, md_dir: str = None) -> List[Dict]:
        """s3-chunking í´ë”ì˜ ëª¨ë“  ì™„ì „íŒ MD íŒŒì¼ ë¡œë“œ"""
        if md_dir is None:
            md_dir = str(current_dir)
        
        md_files = [
            os.path.join(md_dir, "BCì¹´ë“œ_ì¹´ë“œì´ìš©ì•ˆë‚´_ì™„ì „íŒ.md"),
            os.path.join(md_dir, "BCì¹´ë“œ_ì‹ ìš©ì¹´ë“œì—…ë¬´ì²˜ë¦¬ì•ˆë‚´_ì™„ì „íŒ.md")
        ]
        
        print(f"ğŸ” MD íŒŒì¼ ê²€ìƒ‰ ì¤‘: {md_dir}")
        
        results = []
        for md_file in md_files:
            if os.path.exists(md_file):
                print(f"ğŸ“– ì²˜ë¦¬ ì¤‘: {os.path.basename(md_file)}")
                result = self.chunker.chunk_markdown_file(md_file)
                if result:
                    results.append(result)
                    print(f"   âœ… {result['total_chunks']}ê°œ ì²­í¬ ìƒì„±")
            else:
                print(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {os.path.basename(md_file)}")
        
        self.chunk_results = results
        return results
    
    def store_to_vectordb(self, chunk_results: List[Dict] = None) -> bool:
        """ëª¨ë“  ì²­í¬ ë°ì´í„°ë¥¼ ë²¡í„° DBì— ì €ì¥"""
        if not self.vectorstore_manager or not self.embedding_manager:
            print("âŒ ë²¡í„° DB ë˜ëŠ” ì„ë² ë”© ë§¤ë‹ˆì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        if chunk_results is None:
            chunk_results = self.chunk_results
        
        if not chunk_results:
            print("âŒ ì €ì¥í•  ì²­í¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            print("\nğŸ’¾ ë²¡í„° DB ì €ì¥ ì‹œì‘")
            print("=" * 50)
            
            collection_name = "custom"  # s3-chunking ì „ìš© ì»¬ë ‰ì…˜
            
            # ëª¨ë“  ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            all_documents = []
            all_metadatas = []
            
            for chunk_data in chunk_results:
                print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {chunk_data['source_file']}")
                
                for chunk in chunk_data['chunks']:
                    all_documents.append(chunk['content'])
                    
                    # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
                    metadata = chunk['metadata'].copy()
                    metadata.update({
                        'chunking_strategy': 'integrated_markdown_s3',
                        'processing_time': chunk_data['processing_time'],
                        'file_type': 'markdown_complete',
                        'collection_type': 's3_chunking'
                    })
                    all_metadatas.append(metadata)
            
            print(f"ğŸ“Š ì´ ì²­í¬ ìˆ˜: {len(all_documents)}ê°œ")
            
            # ê¸°ì¡´ s3-chunking ë°ì´í„° ì‚­ì œ (ì„ íƒì‚¬í•­)
            # self._clear_s3_chunking_collection(collection_name)
            
            # ë²¡í„° ì„ë² ë”© ìƒì„±
            print("ğŸ”„ ì„ë² ë”© ë²¡í„° ìƒì„± ì¤‘...")
            embeddings = self.embedding_manager.embed_documents(all_documents)
            print(f"   âœ… {len(embeddings)}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
            
            # ë²¡í„° DBì— ì¼ê´„ ì €ì¥
            print("ğŸ“ ë²¡í„° DB ì €ì¥ ì¤‘...")
            success = self.vectorstore_manager.add_documents_to_collection(
                documents=all_documents,
                metadatas=all_metadatas,
                embeddings=embeddings,
                collection_name=collection_name
            )
            
            if success:
                print("\nğŸ‰ ë²¡í„° DB ì €ì¥ ì™„ë£Œ!")
                self._print_storage_summary(chunk_results)
                return True
            else:
                print("\nâŒ ë²¡í„° DB ì €ì¥ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"\nğŸ’¥ ë²¡í„° DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _print_storage_summary(self, chunk_results: List[Dict]):
        """ì €ì¥ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\nğŸ“Š ì €ì¥ ê²°ê³¼ ìš”ì•½:")
        print("-" * 40)
        
        total_chunks = 0
        for chunk_data in chunk_results:
            total_chunks += chunk_data['total_chunks']
            
            # ì²­í¬ íƒ€ì…ë³„ í†µê³„
            chunk_types = {}
            for chunk in chunk_data['chunks']:
                chunk_type = chunk['metadata']['chunk_type']
                chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
            
            print(f"ğŸ“„ {chunk_data['source_file']}: {chunk_data['total_chunks']}ê°œ ì²­í¬")
            for chunk_type, count in chunk_types.items():
                print(f"   - {chunk_type}: {count}ê°œ")
        
        print(f"\nâœ¨ ì „ì²´: {total_chunks}ê°œ ì²­í¬ê°€ 'custom' ì»¬ë ‰ì…˜ì— ì €ì¥ë¨")
        print(f"ğŸ¯ s3-chunking ëª¨ë“œì—ì„œ ìµœì í™”ëœ ê²€ìƒ‰ ê°€ëŠ¥")
    
    def save_processing_log(self, output_file: str = None):
        """ì²˜ë¦¬ ê²°ê³¼ ë¡œê·¸ ì €ì¥"""
        if output_file is None:
            output_file = os.path.join(current_dir, "integrated_chunking_log.json")
        
        try:
            log_data = {
                'processing_time': datetime.now().isoformat(),
                'total_files': len(self.chunk_results),
                'chunk_results': self.chunk_results,
                'summary': {
                    'total_chunks': sum(r['total_chunks'] for r in self.chunk_results),
                    'files_processed': [r['source_file'] for r in self.chunk_results],
                    'chunking_strategy': 'integrated_markdown_s3',
                    'target_collection': 'custom'
                }
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“„ ì²˜ë¦¬ ë¡œê·¸ ì €ì¥: {output_file}")
            
        except Exception as e:
            print(f"âš ï¸ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ í†µí•© MD ì²­í‚¹ ë¡œë” ì‹œì‘")
    print("=" * 60)
    print("ğŸ“‹ BCì¹´ë“œ ì™„ì „íŒ MD íŒŒì¼ë“¤ì„ s3-chunking ìµœì í™” ì²˜ë¦¬")
    print()
    
    try:
        # ë¡œë” ì´ˆê¸°í™”
        loader = IntegratedMDChunkingLoader()
        
        # ëª¨ë“  MD íŒŒì¼ ë¡œë“œ ë° ì²­í‚¹
        print("1ï¸âƒ£ MD íŒŒì¼ ë¡œë“œ ë° ì²­í‚¹ ì²˜ë¦¬...")
        chunk_results = loader.load_all_md_files()
        
        if not chunk_results:
            print("âŒ ì²˜ë¦¬í•  MD íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë²¡í„° DBì— ì €ì¥
        print("\n2ï¸âƒ£ ë²¡í„° DB ì €ì¥...")
        success = loader.store_to_vectordb()
        
        # ì²˜ë¦¬ ë¡œê·¸ ì €ì¥
        print("\n3ï¸âƒ£ ì²˜ë¦¬ ë¡œê·¸ ì €ì¥...")
        loader.save_processing_log()
        
        if success:
            print("\nğŸŠ í†µí•© MD ì²­í‚¹ ì²˜ë¦¬ ì™„ë£Œ!")
            print("\nğŸ’¡ ì‚¬ìš©ë²•:")
            print("   - ëª¨ë“œ 2: ì‚¬ë‚´ì„œë²„ vLLM + s3-chunking")
            print("   - ëª¨ë“œ 4: ChatGPT + s3-chunking")
            print("   â¡ï¸ ìµœì í™”ëœ ì²­í‚¹ìœ¼ë¡œ ë” ì •í™•í•œ ê²€ìƒ‰ ê°€ëŠ¥")
        else:
            print("\nğŸ’¥ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\n\nâ¸ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nğŸ’¥ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()